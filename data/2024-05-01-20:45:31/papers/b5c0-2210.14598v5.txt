  1. 1 Introduction
  2. 2 Variational inference
  3. 3 Elements of manifold optimization
    1. 3.1 Two manifolds
  4. 4 Related work
  5. 5 Manifold Gaussian variational Bayes on the precision matrix
    1. 5.1 Caveats with the MGVB update
    2. 5.2 Updating the precision matrix
    3. 5.3 Gradient computations
    4. 5.4 Retraction and vector transport
    5. 5.5 Isotropic prior
    6. 5.6 Mean-field variant
    7. 5.7 Computational aspects
  6. 6 Implementation aspects
    1. 6.1 Classification vs. regression
    2. 6.2 Variance reduction
    3. 6.3 Constraints on model parameters
    4. 6.4 LB smoothing and stopping criterion
    5. 6.5 Gradient clipping
    6. 6.6 Adaptive learning rate
  7. 7 Experiments
    1. 7.1 Classification tasks
    2. 7.2 Regression tasks
    3. 7.3 Non-differentiable model
  8. 8 Conclusion
  9. A Block-diagonal implementation
  10. B Experiments
    1. B.1 Additional results for the labour data
    2. B.2 Volatility models
    3. B.3 Istanbul dataset: block-diagonal covariance
    4. B.4 Datasets and hyperparameters
  11. C Proofs
    1. C.1 Preliminaries: the Gaussian FIM
    2. C.2 Preliminaries: a useful relation
    3. C.3 Proof of Proposition 5.1
    4. C.4 Derivation of the MGVBP update
    5. C.5 General form of the MGVBP update

# Manifold Gaussian Variational Bayes on the Precision Matrix

Martin Magris, Mostafa Shabani & Alexandros Iosifidis  
Department of Electrical and Computer Engineering  
Aarhus University  
Finlandsgade 22, 8200 Aarhus N, Denmark  
{magris,mshabani,ai}@ece.au.dk  

Martin Magris, Mostafa Shabani, Alexandros Iosifidis  
Department of Electrical and Computer Engineering, Aarhus University,
Finlandsgade 22, Aarhus 8200, Denmark  

Keywords: Variational inference, Manifold optimization, Bayesian learning,
Black-box optimization

Abstract

We propose an optimization algorithm for Variational Inference (VI) in complex
models. Our approach relies on natural gradient updates where the variational
space is a Riemann manifold. We develop an efficient algorithm for Gaussian
Variational Inference whose updates satisfy the positive definite constraint
on the variational covariance matrix. Our Manifold Gaussian Variational Bayes
on the Precision matrix (MGVBP) solution provides simple update rules, is
straightforward to implement, and the use of the precision matrix
parametrization has a significant computational advantage. Due to its black-
box nature, MGVBP stands as a ready-to-use solution for VI in complex models.
Over five datasets, we empirically validate our feasible approach on different
statistical and econometric models, discussing its performance with respect to
baseline methods.

##  Introduction

Although Bayesian principles are not new to Machine Learning (ML) (Mackay,
1992; 1995; Lampinen and Vehtari, 2001), it is only with the recent
methodological developments that we are witnessing a growing use of Bayesian
techniques in the field are (Zhang etÂ al., 2018; Trusheim etÂ al., 2018;
Osawa etÂ al., 2019; Khan etÂ al., 2018; Khan and Nielsen, 2018). In typical
ML settings, the applicability of sampling methods for the challenging
computation of the posterior is prohibitive; however, approximate methods such
as Variational Inference (VI) have been proved suitable and successful (Saul
etÂ al., 1996; Wainwright and Jordan, 2008; Hoffman etÂ al., 2013; Blei etÂ
al., 2017). VI is generally performed with Stochastic Gradient Descent (SGD)
methods (Robbins and Monro, 1951; Hoffman etÂ al., 2013; Salimans and Knowles,
2014), boosted by the use of natural gradients (Hoffman etÂ al., 2013;
Wierstra etÂ al., 2014; Khan etÂ al., 2018), and the updates often take a
simple form (Khan and Nielsen, 2018; Osawa etÂ al., 2019; Magris etÂ al.,
2022).

Most VI algorithms rely on the extensive use of modelsâ€™ gradients, and the
form of the variational posterior implies additional model-specific
derivations that are not easy to adapt to a general, plug-and-play optimizer.
Black box methods (Ranganath etÂ al., 2014) are straightforward to implement
and versatile as they avoid model-specific derivations by relying on
stochastic sampling (Salimans and Knowles, 2014; Paisley etÂ al., 2012; Kingma
and Welling, 2013). The increased variance in the gradient estimates as
opposed to, e.g., methods relying on the reparametrization trick (Blundell etÂ
al., 2015; Xu etÂ al., 2019) can be alleviated with variance reduction
techniques.

Furthermore, most existing algorithms do not directly address parameter
constraints. Under the typical Gaussian variational assumption, granting
positive-definiteness of the covariance matrix is an acknowledged problem
(Tran etÂ al., 2021a; Khan etÂ al., 2018; Lin etÂ al., 2020). Only a few
algorithms directly tackle the problem (Osawa etÂ al., 2019; Lin etÂ al.,
2020), see Section 4. A recent approximate approach based on manifold
optimization is found in (Tran etÂ al., 2021a). For a review of the various
algorithms for performing VI, see (Magris and Iosifidis, 2023a).

On the results of Tran etÂ al. (2021a) and on their of Manifold Gaussian
Variational Bayes (MGVB) method, we develop a variational inference algorithm
that explicitly tackles the positive-definiteness constraint for the
variational covariance matrix, resembles the readily-applicable natural-
gradient black-box framework of (Magris etÂ al., 2022), and that has
computational advantages. We bridge a theoretical issue for the use of
symmetric and positive-definite manifold retraction and parallel transport for
Gaussian VI, leading to our Manifold Gaussian variational Bayes on the
Precision matrix (MGVBP) algorithm. Our solution, based on the precision
matrix parametrization of the variational Gaussian distribution, has
furthermore a computational advantage over the implementation of the usual
canonical parameterization on the covariance matrix, as the form of the
relevant gradients in our update rule is greatly simplified. We distinguish
and apply two forms of the stochastic gradient estimator that are applicable
in a wider context and show how to exploit certain forms of the
prior/posterior further to reduce the variance of the stochastic gradient
estimators. We show that MGVBP is straightforward to implement, discuss
recommendations and practicalities in this regard, and demonstrate its
feasibility in extensive experiments over five datasets, 14 models, three
competing VI optimizers, and a Markov Chain Monte Carlo baseline.

In Section 2, we review the basis of VI, in Section 4, we review the Manifold
Gaussian Variational Bayes approach and other related works, Section 5
describes the proposed approach. Section 6 discusses implementation aspects,
results are reported in Section 7, and Section 8 concludes this paper.
Appendices expand the experiments and provide proofs.

##  Variational inference

Variational Inference (VI) is a convenient and feasible approximate method for Bayesian inference. Let ğ’šğ’š{\bm{y}}bold_italic_y denote the data, pâ¢(ğ’š|ğœ½)ğ‘conditionalğ’šğœ½p\left({\bm{y}}|{\bm{\theta}}\right)italic_p ( bold_italic_y | bold_italic_Î¸ ) the likelihood of the data based on some model whose dğ‘‘ditalic_d-dimensional parameter is ğœ½ğœ½{\bm{\theta}}bold_italic_Î¸. Let pâ¢(ğœ½)ğ‘ğœ½p\left({\bm{\theta}}\right)italic_p ( bold_italic_Î¸ ) be the prior distribution on ğœ½ğœ½{\bm{\theta}}bold_italic_Î¸. In standard Bayesian inference, the posterior is retrieved via the Bayes theorem as pâ¢(ğœ½|ğ’š)=pâ¢(ğœ½)â¢pâ¢(ğ’š|ğœ½)/pâ¢(ğ’š)ğ‘conditionalğœ½ğ’šğ‘ğœ½ğ‘conditionalğ’šğœ½ğ‘ğ’šp\left({\bm{\theta}}|{\bm{y}}\right)=p\left({\bm{\theta}}\right)p\left({\bm{y}% }|{\bm{\theta}}\right)/p\left({\bm{y}}\right)italic_p ( bold_italic_Î¸ | bold_italic_y ) = italic_p ( bold_italic_Î¸ ) italic_p ( bold_italic_y | bold_italic_Î¸ ) / italic_p ( bold_italic_y ). As the marginal likelihood pâ¢(ğ’š)ğ‘ğ’šp\left({\bm{y}}\right)italic_p ( bold_italic_y ) is generally intractable, Bayesian inference is often difficult for complex models. Though sampling techniques can tackle the problem, non-parametric and asymptotically exact Monte Carlo methods may be slow, especially in high-dimensional applications (Salimans etÂ al., 2015).

Fixed-form VI approximates the true unknown posterior with a probability density qğ‘qitalic_q chosen within a tractable class of distributions ğ’¬ğ’¬\mathcal{Q}caligraphic_Q, such as the exponential family. VI turns the Bayesian inference problem into that of finding the best variational distribution qâ‹†âˆˆğ’¬superscriptğ‘â‹†ğ’¬q^{\star}\in\mathcal{Q}italic_q start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆˆ caligraphic_Q minimizing the Kullback-Leibler (KL) divergence from qğ‘qitalic_q to pâ¢(ğœ½|ğ’š)ğ‘conditionalğœ½ğ’šp\left({\bm{\theta}}|{\bm{y}}\right)italic_p ( bold_italic_Î¸ | bold_italic_y ): qâ‹†=argâ¢minqâˆˆğ’¬DKL(q||p(ğœ½|ğ’š))q^{\star}=\operatorname*{arg\,min}_{q\in\mathcal{Q}}D_{\mathrm{KL}}\left(q||p% \left({\bm{\theta}}|{\bm{y}}\right)\right)italic_q start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_q âˆˆ caligraphic_Q end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT ( italic_q | | italic_p ( bold_italic_Î¸ | bold_italic_y ) ). It can be shown that the KL minimization problem is equivalent to the maximization of the so-called Lower Bound (LB) on logâ¡pâ¢(ğ’š)ğ‘ğ’š\log p\left({\bm{y}}\right)roman_log italic_p ( bold_italic_y ), e.g., (Tran etÂ al., 2021b). The optimization problem accounts for finding the optimal variational parameter ğœ»ğœ»{\bm{\zeta}}bold_italic_Î¶ parametrizing qâ‰¡qğœ»ğ‘subscriptğ‘ğœ»q\equiv q_{{\bm{\zeta}}}italic_q â‰¡ italic_q start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT that maximizes the Lower Bound (LB) (â„’â„’\mathcal{L}caligraphic_L), that is ğœ»â‹†=argâ¢maxğœ»âˆˆğ’µâ¡â„’â¢(ğœ»)superscriptğœ»â‹†subscriptargmaxğœ»ğ’µâ„’ğœ»{\bm{\zeta}}^{\star}=\operatorname*{arg\,max}_{{\bm{\zeta}}\in\mathcal{Z}}% \mathcal{L}\left({\bm{\zeta}}\right)bold_italic_Î¶ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_Î¶ âˆˆ caligraphic_Z end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ ), with

| â„’â¢(ğœ»)â„’ğœ»\displaystyle\mathcal{L}\left({\bm{\zeta}}\right)caligraphic_L ( bold_italic_Î¶ ) | â‰”âˆ«qğœ»â¢(ğœ½)â¢logâ¡pâ¢(ğœ½)â¢pâ¢(y|ğœ½)qğœ»â¢(ğœ½)â¢dâ¢ğœ½=ğ”¼qğœ»â¢[logâ¡pâ¢(ğœ½)â¢pâ¢(ğ’š|ğœ½)qğœ»â¢(ğœ½)]=ğ”¼qğœ»â¢[hğœ»â¢(ğœ½)]â¢,â‰”absentsubscriptğ‘ğœ»ğœ½ğ‘ğœ½ğ‘conditionalğ‘¦ğœ½subscriptğ‘ğœ»ğœ½dğœ½subscriptğ”¼subscriptğ‘ğœ»delimited-[]ğ‘ğœ½ğ‘conditionalğ’šğœ½subscriptğ‘ğœ»ğœ½subscriptğ”¼subscriptğ‘ğœ»delimited-[]subscriptâ„ğœ»ğœ½,\displaystyle\coloneqq\int q_{\bm{\zeta}}\left({\bm{\theta}}\right)\log\frac{p% \left({\bm{\theta}}\right)p\left(y|{\bm{\theta}}\right)}{q_{\bm{\zeta}}\left({% \bm{\theta}}\right)}\mathrm{d}{\bm{\theta}}=\mathbb{E}_{q_{\bm{\zeta}}}\left[% \log\frac{p\left({\bm{\theta}}\right)p\left({\bm{y}}|{\bm{\theta}}\right)}{q_{% \bm{\zeta}}\left({\bm{\theta}}\right)}\right]=\mathbb{E}_{q_{\bm{\zeta}}}\left% [h_{\bm{\zeta}}\left({\bm{\theta}}\right)\right]\text{,}â‰” âˆ« italic_q start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( bold_italic_Î¸ ) roman_log divide start_ARG italic_p ( bold_italic_Î¸ ) italic_p ( italic_y | bold_italic_Î¸ ) end_ARG start_ARG italic_q start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( bold_italic_Î¸ ) end_ARG roman_d bold_italic_Î¸ = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_p ( bold_italic_Î¸ ) italic_p ( bold_italic_y | bold_italic_Î¸ ) end_ARG start_ARG italic_q start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( bold_italic_Î¸ ) end_ARG ] = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] , |   
---|---|---|---  
  
where ğ”¼qğœ»subscriptğ”¼subscriptğ‘ğœ»\mathbb{E}_{q_{\bm{\zeta}}}blackboard_E
start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶
end_POSTSUBSCRIPT end_POSTSUBSCRIPT means that the expectation is taken with
respect to the distribution qğœ»subscriptğ‘ğœ»q_{\bm{\zeta}}italic_q
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT, and
ğ’µğ’µ\mathcal{Z}caligraphic_Z is the parameter space for
ğœ»ğœ»{\bm{\zeta}}bold_italic_Î¶.

The maximization of the LB is generally addressed with a gradient-descent
method such as SGD (Robbins and Monro, 1951), ADAM (Kingma and Ba, 2014). The
learning of the parameter ğœ»ğœ»{\bm{\zeta}}bold_italic_Î¶ based on standard
gradient descent is, however, problematic as it ignores the information
geometry of the distribution qğœ»subscriptğ‘ğœ»q_{{\bm{\zeta}}}italic_q
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT, is not scale invariant,
unstable, and very susceptible to the initial values (Wierstra etÂ al., 2014).
SGD implicitly relies on the Euclidean norm for capturing the dissimilarity
between two distributions, which can be a poor and misleading measure of
discrepancy (Khan and Nielsen, 2018). By using the KL divergence in place of
the Euclidean norm, the SGD update results in the following natural gradient
update:

| ğœ»t+1=ğœ»t+Î²tâ¢[âˆ‡~ğœ»â¢â„’â¢(ğœ»)]|ğœ»=ğœ»tâ¢,subscriptğœ»ğ‘¡1subscriptğœ»ğ‘¡evaluated-atsubscriptğ›½ğ‘¡delimited-[]subscript~âˆ‡ğœ»â„’ğœ»ğœ»subscriptğœ»ğ‘¡,{\bm{\zeta}}_{t+1}={\bm{\zeta}}_{t}+\beta_{t}\left.\left[\tilde{\nabla}_{\bm{% \zeta}}\mathcal{L}\left({\bm{\zeta}}\right)\right]\right|_{{\bm{\zeta}}={\bm{% \zeta}}_{t}}\text{,}bold_italic_Î¶ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_italic_Î¶ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [ over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ ) ] | start_POSTSUBSCRIPT bold_italic_Î¶ = bold_italic_Î¶ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , |  | (1)  
---|---|---|---  
  
where Î²tsubscriptğ›½ğ‘¡\beta_{t}italic_Î² start_POSTSUBSCRIPT italic_t
end_POSTSUBSCRIPT is a possibly adaptive learning rate, and tğ‘¡titalic_t
denotes the iteration. The above update results in improved steps towards the
maximum of the LB when optimizing it for the variational parameter
ğœ»ğœ»{\bm{\zeta}}bold_italic_Î¶. The natural gradient
âˆ‡~ğœ»â¢â„’â¢(ğœ»)subscript~âˆ‡ğœ»â„’ğœ»\tilde{\nabla}_{\bm{\zeta}}\mathcal{L}\left({\bm{\zeta}}\right)over~
start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT
caligraphic_L ( bold_italic_Î¶ ) is obtained by rescaling the Euclidean
gradient
âˆ‡ğœ»â„’â¢(ğœ»)subscriptâˆ‡ğœ»â„’ğœ»\nabla_{\bm{\zeta}}\mathcal{L}\left({\bm{\zeta}}\right)âˆ‡
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT caligraphic_L (
bold_italic_Î¶ ) by the inverse of the Fisher Information Matrix (FIM), i.e.,

| âˆ‡~ğœ»â¢â„’â¢(ğœ»)=â„ğœ»âˆ’1â¢âˆ‡ğœ»â„’â¢(ğœ»)â¢,subscript~âˆ‡ğœ»â„’ğœ»subscriptsuperscriptâ„1ğœ»subscriptâˆ‡ğœ»â„’ğœ»,\tilde{\nabla}_{\bm{\zeta}}\mathcal{L}\left({\bm{\zeta}}\right)=\mathcal{I}^{-% 1}_{\bm{\zeta}}\nabla_{\bm{\zeta}}\mathcal{L}\left({\bm{\zeta}}\right)\text{,}over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ ) = caligraphic_I start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ ) , |   
---|---|---  
  
where â„ğœ»subscriptâ„ğœ»\mathcal{I}_{\bm{\zeta}}caligraphic_I
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT denotes the FIM. A
significant issue in following this approach is that
ğœ»ğœ»{\bm{\zeta}}bold_italic_Î¶ is unconstrained. Think of a Gaussian
variational posterior: in the above setting, there is no guarantee that the
covariance matrix updates onto a symmetric and positive definite matrix. As
discussed in the introduction, manifold optimization is an attractive
possibility.

##  Elements of manifold optimization

We wish to optimize the function â„’â„’\mathcal{L}caligraphic_L of the
variational parameter ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ with an update like (1),
where the variational parameter ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ lies in a
manifold. The usual approach for unconstrained optimization reduces to (i)
finding the descent direction and (ii) performing a step in that direction to
obtain function decrease. The notion of gradient is extended to manifolds
through the tangent space. At a point ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ on the
manifold, the tangent space Tğœ»subscriptğ‘‡ğœ»T_{\bm{\zeta}}italic_T
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT is the approximating
vector space, thus given a descent direction
ğƒğœ»âˆˆTğœ»subscriptğƒğœ»subscriptğ‘‡ğœ»{\bm{\xi}}_{\bm{\zeta}}\in
T_{\bm{\zeta}}bold_italic_Î¾ start_POSTSUBSCRIPT bold_italic_Î¶
end_POSTSUBSCRIPT âˆˆ italic_T start_POSTSUBSCRIPT bold_italic_Î¶
end_POSTSUBSCRIPT, a step is performed along the smooth curve on the manifold
in this direction.

A Riemannian manifold is a real, smooth manifold equipped with a positive-
definite inner product
gğœ»â¢(â‹…,â‹…)subscriptğ‘”ğœ»â‹…â‹…g_{\bm{\zeta}}\left(\cdot,\cdot\right)italic_g
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( â‹… , â‹… ) on the
tangent space at each point ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ (see (Absil etÂ
al., 2008) for a rigorous definition). A Riemann manifold, hereafter simply
called manifold, is thus a pair (ğ’®,g)ğ’®ğ‘”\left(\mathcal{S},g\right)(
caligraphic_S , italic_g ), where ğ’®ğ’®\mathcal{S}caligraphic_S is a certain
set, e.g., of certain matrices. For Riemannian manifolds, the Riemann gradient
denoted by gradâ¢fâ¢(ğœ»)gradğ‘“ğœ»\text{grad}f\left(\bm{\zeta}\right)grad
italic_f ( bold_italic_Î¶ ) is defined as a direction on the tangent space,
where the inner product of the Riemann gradient and any direction in the
tangent space gives the directional derivative of the function,

| <gradâ¢fâ¢(ğœ»),Î¾ğœ»>=Dâ¢fâ¢(ğœ»)â¢[Î·]â¢,formulae-sequenceabsentgradğ‘“ğœ»subscriptğœ‰ğœ»Dğ‘“ğœ»delimited-[]ğœ‚,<\text{grad}f\left(\bm{\zeta}\right),{\xi_{\bm{\zeta}}}>=\text{D}f\left(\bm{% \zeta}\right)\left[{\eta}\right]\text{,}< grad italic_f ( bold_italic_Î¶ ) , italic_Î¾ start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT > = D italic_f ( bold_italic_Î¶ ) [ italic_Î· ] , |   
---|---|---  
  
where
Dâ¢fâ¢(ğœ»)â¢[Î·]Dğ‘“ğœ»delimited-[]ğœ‚\text{D}f\left(\bm{\zeta}\right)\left[\eta\right]D
italic_f ( bold_italic_Î¶ ) [ italic_Î· ] denotes the directional derivative
of fğ‘“fitalic_f at ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ in the direction
Î·ğœ‚\etaitalic_Î·. The gradient has the property that the direction of
gradâ¢fâ¢(ğœ»)gradğ‘“ğœ»\text{grad}f\left(\bm{\zeta}\right)grad italic_f (
bold_italic_Î¶ ) is the steepest-ascent direction of fğ‘“fitalic_f at
ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ (Absil etÂ al., 2008), that important for the
scope of optimization.

For a descent direction on the tangent space, the map that gives the
corresponding point on the manifold is called the exponential map. The
exponential map
Expğœ»â¢(ğƒğœ»)subscriptExpğœ»subscriptğƒğœ»\text{Exp}_{\bm{\zeta}}\left({\bm{\xi}}_{\bm{\zeta}}\right)Exp
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( bold_italic_Î¾
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ) thus projects a tangent
vector Î¾ğœ»âˆˆTğœ»subscriptğœ‰ğœ»subscriptğ‘‡ğœ»\xi_{\bm{\zeta}}\in
T_{\bm{\zeta}}italic_Î¾ start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT
âˆˆ italic_T start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT back to the
manifold, generalizing the usual concept
ğœ»+ğƒğœ»ğœ»subscriptğƒğœ»\bm{\zeta}+{\bm{\xi}}_{\bm{\zeta}}bold_italic_Î¶ +
bold_italic_Î¾ start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT in
Euclidean spaces. In fact,
Expğœ»â¢(ğƒğœ»)subscriptExpğœ»subscriptğƒğœ»\text{Exp}_{\bm{\zeta}}\left({\bm{\xi}}_{\bm{\zeta}}\right)Exp
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( bold_italic_Î¾
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ) can be thought as the
point on the manifold reached by leaving from ğœ»ğœ»\bm{\zeta}bold_italic_Î¶
and moving in the direction
ğƒğœ»subscriptğƒğœ»{\bm{\xi}}_{\bm{\zeta}}bold_italic_Î¾ start_POSTSUBSCRIPT
bold_italic_Î¶ end_POSTSUBSCRIPT while remaining on the manifold. Therefore,
in analogy with the usual gradient descent approach
Î¶â†Î¶+Î²â¢âˆ‡fâ¢(ğœ»)â†ğœğœğ›½âˆ‡ğ‘“ğœ»\zeta\leftarrow\zeta+\beta\nabla
f\left(\bm{\zeta}\right)italic_Î¶ â† italic_Î¶ + italic_Î² âˆ‡ italic_f (
bold_italic_Î¶ ) with Î²ğ›½\betaitalic_Î² being the learning rate, on
manifolds, the update is performed through retraction following the steepest
direction provided by the Riemann gradient as
Expğœ»â¢(Î²â¢gradâ¢fâ¢(ğœ»))subscriptExpğœ»ğ›½gradğ‘“ğœ»\text{Exp}_{\bm{\zeta}}\left(\beta\,\text{grad}f\left(\bm{\zeta}\right)\right)Exp
start_POSTSUBSCRIPT bold_italic_Î¶ end_POSTSUBSCRIPT ( italic_Î² grad italic_f
( bold_italic_Î¶ ) ).

In practice, exponential maps are cumbersome to compute; retractions are used
as first-order approximations. A Riemannian manifold also has a natural way of
transporting vectors. Parallel transport moves tangent vectors from one
tangent space to another while preserving the original length and direction,
extending the use of momentum gradients to manifolds. As for the exponential
map, a parallel transport is in practice approximated by the so-called vector
transport. Note that the forms of retraction and vector transport, as much as
that of the Riemann gradient, depend on the specific metric adopted in the
tangent space.

Thinking of ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ as the parameter of a Gaussian
distribution, ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ involves elements related to
ğğ\bm{\mu}bold_italic_Î¼, unconstrained over
â„dsuperscriptâ„ğ‘‘\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d
end_POSTSUPERSCRIPT, and elements related to the covariance matrix,
constrained to define a valid covariance matrix: the product space of
Riemannian manifolds is itself a Riemannian manifold. The exponential map,
gradient, and parallel transport are defined as the Cartesian product of the
individual ones, while the inner product is defined as the sum of the inner
product of the components in their respective manifolds (Hosseini and Sra,
2015).

![Refer to caption](x1.png)

![Refer to caption](x2.png)

![Refer to caption](x3.png)

Figure 1: Manifold illustration. Left: manifold (black), tangent space (light
blue), and Riemann gradient at the point in black. Middle: exponential map
(dotted gray) and the corresponding point on the manifold (green point).
Right: Parallel transform between vectors on two tangent planes.

###  Two manifolds

Be ğ’®={Pâˆˆâ„dÃ—d:P=PâŠ¤,Pâ‰»0}ğ’®conditional-
setğ‘ƒsuperscriptâ„ğ‘‘ğ‘‘formulae-
sequenceğ‘ƒsuperscriptğ‘ƒtopsucceedsğ‘ƒ0\mathcal{S}=\left\\{P\in\mathbb{R}^{d\times
d}:P=P^{\top},P\succ 0\right\\}caligraphic_S = { italic_P âˆˆ blackboard_R
start_POSTSUPERSCRIPT italic_d Ã— italic_d end_POSTSUPERSCRIPT : italic_P =
italic_P start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT , italic_P â‰» 0 } the
set of Symmetric and Positive Definite (SPD) dÃ—dğ‘‘ğ‘‘d\times ditalic_d Ã—
italic_d matrices, we denote by
â„³=(ğ’®,gP)â„³ğ’®subscriptğ‘”ğ‘ƒ\mathcal{M}=\left(\mathcal{S},g_{P}\right)caligraphic_M
= ( caligraphic_S , italic_g start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT )
the corresponding manifold, where with the metric
gPsubscriptğ‘”ğ‘ƒg_{P}italic_g start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT
between two vectors ğœ»ğœ»\bm{\zeta}bold_italic_Î¶ and
ğƒğƒ{\bm{\xi}}bold_italic_Î¾ at Pğ‘ƒPitalic_P defined as
gP=Trâ¢(Pâˆ’1â¢ğœ»â¢Pâˆ’1â¢ğƒ)subscriptğ‘”ğ‘ƒTrsuperscriptğ‘ƒ1ğœ»superscriptğ‘ƒ1ğƒg_{P}=\text{Tr}\left(P^{-1}\bm{\zeta}P^{-1}{\bm{\xi}}\right)italic_g
start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT = Tr ( italic_P
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Î¶ italic_P
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Î¾ ). In the
remainder of the paper, as the metric gPsubscriptğ‘”ğ‘ƒg_{P}italic_g
start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT is derived from the Frobenius
norm, i.e., the Euclidean norm of a matrix, we refer to this metric between
tangent vectors as Euclidean metric (see, e.g., (Bhatia etÂ al., 2019; Han etÂ
al., 2021)). By denoting with fğ‘“fitalic_f a generic smooth function of
Pğ‘ƒPitalic_P, from, e.g., (Hosseini and Sra, 2015; Pennec, 2020; Boumal etÂ
al., 2014), the relationship between the usual Euclidean gradient
âˆ‡fâ¢(P)âˆ‡ğ‘“ğ‘ƒ\nabla f\left(P\right)âˆ‡ italic_f ( italic_P ) and the
Riemann gradient is

| gradâ¢fâ¢(P)=Pâ¢âˆ‡fâ¢(P)â¢Pâ¢,gradğ‘“ğ‘ƒğ‘ƒâˆ‡ğ‘“ğ‘ƒğ‘ƒ,\text{grad}\,f\left(P\right)=P\nabla f\left(P\right)P\text{,}grad italic_f ( italic_P ) = italic_P âˆ‡ italic_f ( italic_P ) italic_P , |  | (2)  
---|---|---|---  
  
assuming âˆ‡fâ¢(P)âˆ‡ğ‘“ğ‘ƒ\nabla f\left(P\right)âˆ‡ italic_f ( italic_P ) is
a symmetric matrix. For the manifold â„³â„³\mathcal{M}caligraphic_M, the
retraction at point Pğ‘ƒPitalic_P in the direction
ğƒğƒ{\bm{\xi}}bold_italic_Î¾ is computed as

| RPâ¢(ğƒ)=P+ğƒ+12â¢ğƒâ¢Pâˆ’1â¢ğƒ,ğƒâˆˆTPâ¢â„³â¢,formulae-sequencesubscriptğ‘…ğ‘ƒğƒğ‘ƒğƒ12ğƒsuperscriptğ‘ƒ1ğƒğƒsubscriptğ‘‡ğ‘ƒâ„³,R_{P}\left({\bm{\xi}}\right)=P+{\bm{\xi}}+\frac{1}{2}{\bm{\xi}}P^{-1}{\bm{\xi}% },\qquad{\bm{\xi}}\in T_{P}\mathcal{M}\text{,}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( bold_italic_Î¾ ) = italic_P + bold_italic_Î¾ + divide start_ARG 1 end_ARG start_ARG 2 end_ARG bold_italic_Î¾ italic_P start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Î¾ , bold_italic_Î¾ âˆˆ italic_T start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT caligraphic_M , |  | (3)  
---|---|---|---  
  
while the vector transport of the tangent vector from
P1subscriptğ‘ƒ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to
P2subscriptğ‘ƒ2P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is given
by:

| ğ’¯P1â†’P2â¢(ğƒ)=Eâ¢ğƒâ¢EâŠ¤â¢,E=(P2â¢P1âˆ’1)12â¢.formulae-sequencesubscriptğ’¯â†’subscriptğ‘ƒ1subscriptğ‘ƒ2ğƒğ¸ğƒsuperscriptğ¸top,ğ¸superscriptsubscriptğ‘ƒ2superscriptsubscriptğ‘ƒ1112.\mathcal{T}_{P_{1}\rightarrow P_{2}}\left({\bm{\xi}}\right)=E{\bm{\xi}}E^{\top% }\text{,}\qquad E=\left(P_{2}P_{1}^{-1}\right)^{\frac{1}{2}}\text{.}caligraphic_T start_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â†’ italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¾ ) = italic_E bold_italic_Î¾ italic_E start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT , italic_E = ( italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT . |  | (4)  
---|---|---|---  
  
In alternative to the manifold â„³â„³\mathcal{M}caligraphic_M, as for the
discussion in Section 2, another metric that gained popularity is the Fisher-
Rao metric. The Fisher-Rao metric depends on the Fisher Information Matrix
(FIM) â„Psubscriptâ„ğ‘ƒ\mathcal{I}_{P}caligraphic_I start_POSTSUBSCRIPT
italic_P end_POSTSUBSCRIPT, and for two vectors
ğœ»,ğƒğœ»ğƒ\bm{\zeta},\bm{\xi}bold_italic_Î¶ , bold_italic_Î¾ at a point
Pğ‘ƒPitalic_P in the tangent space TPsubscriptğ‘‡ğ‘ƒT_{P}italic_T
start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT, it defines the inner product
as <ğœ»,ğƒ>=ğœ»âŠ¤â¢â„Pâ¢ğƒformulae-
sequenceabsentğœ»ğƒsuperscriptğœ»topsubscriptâ„ğ‘ƒğƒ<\bm{\zeta},\bm{\xi}>=\bm{\zeta}^{\top}\mathcal{I}_{P}\bm{\xi}<
bold_italic_Î¶ , bold_italic_Î¾ > = bold_italic_Î¶ start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT caligraphic_I start_POSTSUBSCRIPT italic_P
end_POSTSUBSCRIPT bold_italic_Î¾. We refer to the manifold of the SPD matrices
equipped with the Fisher-Rao metric as â„±â„±\mathcal{F}caligraphic_F.
Importantly, the manifold â„±â„±\mathcal{F}caligraphic_F is the one adopted by
the work of (Tran etÂ al., 2021a), at the basis of this paper. (Tran etÂ al.,
2021a) furthermore show that the Riemann gradient in
â„±â„±\mathcal{F}caligraphic_F corresponds to the natural gradient, i.e.,

| gradâ¢fâ¢(P)=âˆ‡~â¢fâ¢(P)â¢.gradğ‘“ğ‘ƒ~âˆ‡ğ‘“ğ‘ƒ.\text{grad}f\left(P\right)=\tilde{\nabla}f\left(P\right)\text{.}grad italic_f ( italic_P ) = over~ start_ARG âˆ‡ end_ARG italic_f ( italic_P ) . |   
---|---|---  
  
From the form of the Gaussian FIM,
âˆ‡~â¢fâ¢(P)=2â¢Pâ¢âˆ‡fâ¢(P)â¢P~âˆ‡ğ‘“ğ‘ƒ2ğ‘ƒâˆ‡ğ‘“ğ‘ƒğ‘ƒ\tilde{\nabla}f\left(P\right)=2P\nabla
f\left(P\right)Pover~ start_ARG âˆ‡ end_ARG italic_f ( italic_P ) = 2 italic_P
âˆ‡ italic_f ( italic_P ) italic_P (see Appendix C.3), note that

| Pâ¢âˆ‡fâ¢(P)â¢P=12â¢âˆ‡~â¢fâ¢(P)â¢.ğ‘ƒâˆ‡ğ‘“ğ‘ƒğ‘ƒ12~âˆ‡ğ‘“ğ‘ƒ.P\nabla f\left(P\right)P=\frac{1}{2}\tilde{\nabla}f\left(P\right)\text{.}italic_P âˆ‡ italic_f ( italic_P ) italic_P = divide start_ARG 1 end_ARG start_ARG 2 end_ARG over~ start_ARG âˆ‡ end_ARG italic_f ( italic_P ) . |  | (5)  
---|---|---|---  
  
I.e., the Riemann gradient in â„³â„³\mathcal{M}caligraphic_M is one-half of
the natural gradient, which conversely is the Riemann gradient in
â„±â„±\mathcal{F}caligraphic_F. Note that the above also applies to
Pâˆ’1superscriptğ‘ƒ1P^{-1}italic_P start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT since the inverse of an SPD matrix is as well SPD.

##  Related work

(Tran etÂ al., 2021a) adopts a fixed-form dğ‘‘ditalic_d-dimensional Gaussian
distribution with mean ğğ{\bm{\mu}}bold_italic_Î¼ and covariance matrix
Î£Î£\Sigmaroman_Î£ for the variational approximation
qğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„q_{\bm{\zeta^{c}}}italic_q
start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, where the variational parameter
ğœ»ğ’„superscriptğœ»ğ’„{\bm{\zeta^{c}}}bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_c end_POSTSUPERSCRIPT collects all the elements of
ğğ{\bm{\mu}}bold_italic_Î¼ and Î£Î£\Sigmaroman_Î£, i.e.
ğœ»ğ’„=(ğâŠ¤,vecâ¢(Î£)âŠ¤)âŠ¤superscriptğœ»ğ’„superscriptsuperscriptğtopvecsuperscriptÎ£toptop{\bm{\zeta^{c}}}=\left({\bm{\mu}}^{\top},\text{vec}\left(\Sigma\right)^{\top}%
\right)^{\top}bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT = ( bold_italic_Î¼ start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT , vec ( roman_Î£ ) start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT (canonical
parametrization). There are no restrictions on ğğ{\bm{\mu}}bold_italic_Î¼, yet
the covariance matrix Î£Î£\Sigmaroman_Î£ is constrained to the manifold
â„±â„±\mathcal{F}caligraphic_F of SPD matrices equipped with the Fisher-Rao
metric.

For a multivariate Gaussian
qğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„q_{\bm{\zeta^{c}}}italic_q
start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, the exact form of the corresponding FIM
reads (e.g., Mardia and Marshall, 1984)

| â„ğœ»ğ’„=(Î£âˆ’100â„ğœ»ğ’„â¢(Î£))â¢,subscriptâ„superscriptğœ»ğ’„matrixsuperscriptÎ£100subscriptâ„superscriptğœ»ğ’„Î£,\mathcal{I}_{\bm{\zeta^{c}}}=\begin{pmatrix}\Sigma^{-1}&0\\\ 0&\mathcal{I}_{\bm{\zeta^{c}}}\left(\Sigma\right)\end{pmatrix}\text{,}caligraphic_I start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = ( start_ARG start_ROW start_CELL roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL caligraphic_I start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_Î£ ) end_CELL end_ROW end_ARG ) , |  | (6)  
---|---|---|---  
  
with
â„ğœ»ğ’„â¢(Î£)Ïƒiâ¢j,Ïƒkâ¢l=12â¢Trâ¢(Î£âˆ’1â¢âˆ‚Î£âˆ‚Ïƒiâ¢jâ¢Î£âˆ’1â¢âˆ‚Î£âˆ‚Ïƒkâ¢l)subscriptâ„superscriptğœ»ğ’„subscriptÎ£subscriptğœğ‘–ğ‘—subscriptğœğ‘˜ğ‘™12TrsuperscriptÎ£1Î£subscriptğœğ‘–ğ‘—superscriptÎ£1Î£subscriptğœğ‘˜ğ‘™\mathcal{I}_{\bm{\zeta^{c}}}\left(\Sigma\right)_{\sigma_{ij},\sigma_{kl}}=%
\frac{1}{2}\text{Tr}\left(\Sigma^{-1}\frac{\partial\Sigma}{\partial\sigma_{ij}%
}\Sigma^{-1}\frac{\partial\Sigma}{\partial\sigma_{kl}}\right)caligraphic_I
start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_Î£ ) start_POSTSUBSCRIPT
italic_Ïƒ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT , italic_Ïƒ
start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT end_POSTSUBSCRIPT =
divide start_ARG 1 end_ARG start_ARG 2 end_ARG Tr ( roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT divide start_ARG âˆ‚ roman_Î£
end_ARG start_ARG âˆ‚ italic_Ïƒ start_POSTSUBSCRIPT italic_i italic_j
end_POSTSUBSCRIPT end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT divide start_ARG âˆ‚ roman_Î£ end_ARG start_ARG âˆ‚
italic_Ïƒ start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT end_ARG )
being the generic element of the
d2Ã—d2superscriptğ‘‘2superscriptğ‘‘2d^{2}\times d^{2}italic_d
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã— italic_d start_POSTSUPERSCRIPT
2 end_POSTSUPERSCRIPT matrix
â„ğœ»ğ’„â¢(Î£)subscriptâ„superscriptğœ»ğ’„Î£\mathcal{I}_{\bm{\zeta^{c}}}\left(\Sigma\right)caligraphic_I
start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_Î£ ). The Manifold Gaussian
Variational Bayes (MGVB) method (Tran etÂ al., 2021a) relies on the
approximation
â„ğœ»ğ’„â¢(Î£)â‰ˆÎ£âˆ’1âŠ—Î£âˆ’1subscriptâ„superscriptğœ»ğ’„Î£tensor-
productsuperscriptÎ£1superscriptÎ£1\mathcal{I}_{\bm{\zeta^{c}}}\left(\Sigma\right)\approx\Sigma^{-1}\otimes\Sigma%
^{-1}caligraphic_I start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_Î£ ) â‰ˆ roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âŠ— roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, where âŠ—tensor-
product\otimesâŠ— denotes the Kronecker product. Accordingly, (Tran etÂ al.,
2021a) compute the convenient approximate form of the natural gradients of the
LB with respect to ğğ{\bm{\mu}}bold_italic_Î¼ and Î£Î£\Sigmaroman_Î£,
respectively as111 We follow the presentation of (Tran etÂ al., 2021a), where
âˆ‡~Î£â¢â„’â¢(ğœ»ğ’„)subscript~âˆ‡Î£â„’superscriptğœ»ğ’„\tilde{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)over~
start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT
caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT ) is approximated up to a scaling constant as
Î£â¢âˆ‡Î£Î£Î£subscriptâˆ‡Î£Î£\Sigma\nabla_{\Sigma}\Sigmaroman_Î£ âˆ‡
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT roman_Î£. Indeed (Lin etÂ al.,
2020) clarify that exact form of
â„ğœ»ğ’„â¢(Î£)subscriptâ„superscriptğœ»ğ’„Î£\mathcal{I}_{\bm{\zeta^{c}}}\left(\Sigma\right)caligraphic_I
start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_Î£ ) should read
2â¢(Î£âˆ’1âŠ—Î£âˆ’1)2tensor-
productsuperscriptÎ£1superscriptÎ£12(\Sigma^{-1}\otimes\Sigma^{-1})2 (
roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âŠ— roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ), see e.g., (Barfoot, 2020),
leading to the exact natural gradient
âˆ‡~Î£â¢â„’â¢(ğœ»ğ’„)=2â¢Î£â¢âˆ‡Î£â„’â¢(ğœ»ğ’„)â¢Î£subscript~âˆ‡Î£â„’superscriptğœ»ğ’„2Î£subscriptâˆ‡Î£â„’superscriptğœ»ğ’„Î£\tilde{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)=2\Sigma\nabla_%
{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\Sigmaover~ start_ARG âˆ‡
end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L (
bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = 2
roman_Î£ âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L (
bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT )
roman_Î£.,

| âˆ‡~ğâ¢â„’â¢(ğœ»ğ’„)=Î£â¢âˆ‡ğâ„’â¢(ğœ»ğ’„)andâˆ‡~Î£â¢â„’â¢(ğœ»ğ’„)â‰ˆÎ£â¢âˆ‡Î£â„’â¢(ğœ»ğ’„)â¢Î£â¢.formulae-sequencesubscript~âˆ‡ğâ„’superscriptğœ»ğ’„Î£subscriptâˆ‡ğâ„’superscriptğœ»ğ’„andsubscript~âˆ‡Î£â„’superscriptğœ»ğ’„Î£subscriptâˆ‡Î£â„’superscriptğœ»ğ’„Î£.\displaystyle\tilde{\nabla}_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)% =\Sigma\nabla_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\quad\text{and% }\quad\tilde{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\approx% \Sigma\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\Sigma\text{.}over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = roman_Î£ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) and over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) â‰ˆ roman_Î£ âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) roman_Î£ . |  | (7)  
---|---|---|---  
  
In virtue of the natural gradient definition, the first natural gradient is
exact, while the second is approximate. Thus, (Tran etÂ al., 2021a) adopts the
following updates on the variational parameter:

| ğâ†ğ+Î²â¢âˆ‡~ğâ¢â„’â¢(ğœ»ğ’„)andÎ£â†RÎ£â¢(Î²â¢âˆ‡~Î£â¢â„’â¢(ğœ»ğ’„))â¢,formulae-sequenceâ†ğğğ›½subscript~âˆ‡ğâ„’superscriptğœ»ğ’„andâ†Î£subscriptğ‘…Î£ğ›½subscript~âˆ‡Î£â„’superscriptğœ»ğ’„,{\bm{\mu}}\leftarrow{\bm{\mu}}+\beta\tilde{\nabla}_{\bm{\mu}}\mathcal{L}\left(% {\bm{\zeta^{c}}}\right)\quad\textrm{and}\quad\Sigma\leftarrow R_{\Sigma}(\beta% \tilde{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right))\text{,}bold_italic_Î¼ â† bold_italic_Î¼ + italic_Î² over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) and roman_Î£ â† italic_R start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT ( italic_Î² over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) ) , |  | (8)  
---|---|---|---  
  
where RÎ£â¢(â‹…)subscriptğ‘…Î£â‹…R_{\Sigma}\left(\cdot\right)italic_R
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT ( â‹… ) denotes the retraction
form (3). Retractions are of central importance in manifold optimization and
for the scope of the paper: they enable projecting vectors from the so-called
tangent space back to the manifold. Momentum gradients can be used in place of
plain natural ones. In particular, the momentum gradient for the update of
Î£Î£\Sigmaroman_Î£ relies on a vector transport, granting that, at each
iteration, the weighted gradient remains in the tangent space of the manifold,
see Section 3.

A method of handling the positivity constraint in diagonal covariance matrices
is the Variational Online Gauss-Newton (VOGN) optimizer (Khan etÂ al., 2018;
Osawa etÂ al., 2019). VOGN relates to the VON (Khan and Lin, 2017) update as
it indirectly updates ğğ{\bm{\mu}}bold_italic_Î¼ and Î£Î£\Sigmaroman_Î£ from
Gaussian natural parameters updates. Following a non-Black-Box approach, VOGN
uses some theoretical results on the Gaussian distribution to recover an
update for Î£Î£\Sigmaroman_Î£ that involves the Hessian of the likelihood.
Such Hessian is estimated as the samplesâ€™ mean squared gradient, granting
the non-negativity of the diagonal covariance update. (Osawa etÂ al., 2019)
devise the computation of the approximate Hessian in a block-diagonal fashion
within the layers of a Deep-learning model.

(Lin etÂ al., 2020) extend the above to handle the positive definiteness
constraint by adding an additional term to the update rule for
Î£Î£\Sigmaroman_Î£, applicable to certain partitioned structures of the FIM.
The retraction map in (Lin etÂ al., 2020) is more general than (Tran etÂ al.,
2021a) and obtained through a different Riemann metric, from which MGVB is
retrieved as a special case. As opposed to MGVBP, the use of the
reparametrization trick in (Lin etÂ al., 2020) requires model-specific
computation or auto-differentiation. See (Lin etÂ al., 2021) for an extension
on stochastic, non-convex problems.

Alternative methods that rely on unconstrained transformations (e.g., Cholesky
factor, (Tan, 2021)), or on the adaptive adjustment of the learning rate
(e.g., (Khan and Lin, 2017)) lie outside the manifold context discussed here.
Among the methods that do not control for the positive definiteness
constraint, the QBVI update (Magris etÂ al., 2022) provides a comparable
black-bock method that, despite other black-box VI algorithms, uses exact
natural gradients updates obtained without the computation of the FIM.

##  Manifold Gaussian variational Bayes on the precision matrix

###  Caveats with the MGVB update

We identify three criticalities concerning the MGVB update as formulated and
implemented by (Tran etÂ al., 2021a), providing the motivation for this paper
and the basis for developing our update.

  * (i)

(Tran etÂ al., 2021a) adopts the manifold of symmetric and positive definite
matrices equipped with the Fisher-Rao metric. We denote such manifold by
â„±â„±\mathcal{F}caligraphic_F. As of Section 3, in manifold optimization, the
update is carried out by employing the retraction function on the Riemann
gradient, mapping elements of the tangent space onto the manifold. Indeed, the
update (Tran etÂ al., 2021a) suggests on the variational covariance matrix
Î£Î£\Sigmaroman_Î£ for optimizing the lower bound objective
â„’â„’\mathcal{L}caligraphic_L, involves
RÎ£â¢(Î²â¢gradâ¢â„’)subscriptğ‘…Î£ğ›½gradâ„’R_{\Sigma}\left(\beta\text{grad}\mathcal{\mathcal{L}}\right)italic_R
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT ( italic_Î² grad caligraphic_L
). Interestingly, they show that for the manifold
â„±â„±\mathcal{F}caligraphic_F, the Riemann Gradient corresponds to the
natural gradient (see their Lemma 2.1). However, the form of the retraction
they adopt is that of the manifold â„³â„³\mathcal{M}caligraphic_M of the SPD
matrices equipped with the Euclidean metric and not the retraction form for
the manifold Î£Î£\Sigmaroman_Î£ induced by the Fisher-Rao metric. What is the
retraction form for the manifold ğ’®ğ’®\mathcal{S}caligraphic_S is an open
question. In practice, (Tran etÂ al., 2021a) mixes elements of the two
manifolds as it applies the retraction derived from
â„³â„³\mathcal{M}caligraphic_M on a Riemann gradient of the manifold
â„±â„±\mathcal{F}caligraphic_F. This very same point is also raised in (Lin
etÂ al., 2020), which, in fact, underlines that in (Tran etÂ al., 2021a), the
chosen form of the retraction is not well-justified as it is specific for the
SPD matrix manifold, whereas the natural gradient is computed within a
different manifold.

(Tran etÂ al., 2021a) establish the equivalence between the Riemann gradient
and the natural gradient in â„±â„±\mathcal{F}caligraphic_F. However, in
equations (5.2) and (5.4) of (Tan, 2021), and the implementation accompanying
the paper, the natural gradient reads
Î£â¢âˆ‡Î£â„’â¢Î£Î£subscriptâˆ‡Î£â„’Î£\Sigma\nabla_{\Sigma}\mathcal{L}\Sigmaroman_Î£
âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£ in
place of
2â¢Î£â¢âˆ‡Î£â„’â¢Î£2Î£subscriptâˆ‡Î£â„’Î£2\Sigma\nabla_{\Sigma}\mathcal{L}\Sigma2
roman_Î£ âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L
roman_Î£ (see Appendix C.1 and the earlier footnote). This leads to the
following two observations:

  * (ii)

The halved natural gradient
Î£â¢âˆ‡Î£â„’â¢Î£Î£subscriptâˆ‡Î£â„’Î£\Sigma\nabla_{\Sigma}\mathcal{L}\Sigmaroman_Î£
âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£ is a
Riemann gradient for the manifold â„³â„³\mathcal{M}caligraphic_M, see eq.(2).
Therefore, the form of retraction (3) is coherent and applicable; thus, the
update (8) on
Î£â¢âˆ‡Î£â„’â¢Î£Î£subscriptâˆ‡Î£â„’Î£\Sigma\nabla_{\Sigma}\mathcal{L}\Sigmaroman_Î£
âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£ is
formally correct. However, this is a fully consistent procedure for the
manifold â„³â„³\mathcal{M}caligraphic_M, and not for the manifold
â„±â„±\mathcal{F}caligraphic_F adopted in (Tran etÂ al., 2021a).

  * (iii)

By adopting
Î£â¢âˆ‡Î£â„’â¢Î£Î£subscriptâˆ‡Î£â„’Î£\Sigma\nabla_{\Sigma}\mathcal{L}\Sigmaroman_Î£
âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£ in
place of
2â¢Î£â¢âˆ‡Î£â„’â¢Î£2Î£subscriptâˆ‡Î£â„’Î£2\Sigma\nabla_{\Sigma}\mathcal{L}\Sigma2
roman_Î£ âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L
roman_Î£, in the above light, we recognize that the overall update in (Tran
etÂ al., 2021a), reads as a hybrid update: whereas ğğ{\bm{\mu}}bold_italic_Î¼
is updated with a natural ingredient update, Î£Î£\Sigmaroman_Î£ is updated
with the gradient
Î£â¢âˆ‡Î£â„’â¢Î£Î£subscriptâˆ‡Î£â„’Î£\Sigma\nabla_{\Sigma}\mathcal{L}\Sigmaroman_Î£
âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£,
Riemannian in â„±â„±\mathcal{F}caligraphic_F, but not natural.

Regarding point (iii), as for (5)
Î£â¢âˆ‡Î£â„’â¢Î£=12â¢âˆ‡~Î£â¢â„’Î£subscriptâˆ‡Î£â„’Î£12subscript~âˆ‡Î£â„’\Sigma\nabla_{\Sigma}\mathcal{L}\Sigma=\frac{1}{2}\tilde{\nabla}_{\Sigma}%
\mathcal{L}roman_Î£ âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT
caligraphic_L roman_Î£ = divide start_ARG 1 end_ARG start_ARG 2 end_ARG over~
start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT
caligraphic_L. Thus, the Riemann gradient
gradâ¢â„’gradâ„’\text{grad}\mathcal{L}grad caligraphic_L shares the same
direction in both â„±â„±\mathcal{F}caligraphic_F and
â„³â„³\mathcal{M}caligraphic_M. Therefore, in both manifolds, the respective
Riemann gradients point in the direction of the natural gradient. So the
update is, in practice, a natural gradient update. Equation (5), which does
not appear in (Tran etÂ al., 2021a) and in the earlier VI literature,
therefore explains why MGVB actually works despite the above issues.

![Refer to caption]() Figure 2: Variational inference for the simple linear
regression model, with 100 observations simulated according to
Y=2â¢X+ğœºğ‘Œ2ğ‘‹ğœºY=2X+\bm{\varepsilon}italic_Y = 2 italic_X +
bold_italic_Îµ, Îµiâˆ¼ğ’©â¢(0,1)similar-
tosubscriptğœ€ğ‘–ğ’©01\varepsilon_{i}\sim\mathcal{N}\left(0,1\right)italic_Îµ
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ¼ caligraphic_N ( 0 , 1 ),
X=[0,0.05,0.1,â€¦,5]ğ‘‹00.050.1â€¦5X=[0,0.05,0.1,\dots,5]italic_X = [ 0 , 0.05
, 0.1 , â€¦ , 5 ], for the MGVB update and the revisited version (MGVB rev.
label) applying retraction on the Riemannian gradient in
â„³â„³\mathcal{M}caligraphic_M. The red circle denotes the true posterior
parameter computed with standard results in Bayesian linear regression.

A remedy for the major points (i) and (ii) requires revising the MGVB update
for Î£Î£\Sigmaroman_Î£. Specifically, to adopt retraction for
â„³â„³\mathcal{M}caligraphic_M applied to an actual Riemann gradient for
â„³â„³\mathcal{M}caligraphic_M, i.e.,
RÎ£â¢(Î²2â¢âˆ‡~Î£â¢â„’)subscriptğ‘…Î£ğ›½2subscript~âˆ‡Î£â„’R_{\Sigma}(\frac{\beta}{2}\tilde{\nabla}_{\Sigma}\mathcal{L})italic_R
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT ( divide start_ARG italic_Î²
end_ARG start_ARG 2 end_ARG over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT
roman_Î£ end_POSTSUBSCRIPT caligraphic_L ). Figure 2 depicts the difference
between the MGVB update and such a revisited version. The three panels show a
simple example of applying retraction (3) on the actual Riemann gradient on
â„³â„³\mathcal{M}caligraphic_M, highlighting a non-irrelevant impact on the
steps of line-search and the convergence of the algorithm towards the optimum.

The computation of the Riemann gradient
12â¢âˆ‡~Î£â¢â„’=Î£â¢âˆ‡Î£â„’â¢Î£12subscript~âˆ‡Î£â„’Î£subscriptâˆ‡Î£â„’Î£\frac{1}{2}\tilde{\nabla}_{\Sigma}\mathcal{L}=\Sigma\nabla_{\Sigma}\mathcal{L}\Sigmadivide
start_ARG 1 end_ARG start_ARG 2 end_ARG over~ start_ARG âˆ‡ end_ARG
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L = roman_Î£ âˆ‡
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£, which
involves two matrix multiplications of
ğ’ªâ¢(d3)ğ’ªsuperscriptğ‘‘3\mathcal{O}\left(d^{3}\right)caligraphic_O (
italic_d start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) operations, can,
however, be simplified considering a parametrization of the variational
distribution in terms of its covariance matrix
Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT. This leads to our suggested update in the next section.

###  Updating the precision matrix

Consider a variational Gaussian distribution with mean
ğğ{\bm{\mu}}bold_italic_Î¼ and positive-definite covariance matrix222This is
the case of practical relevance, ruling out singular Gaussian distributions
for which the discussion is here out of scope. Positive-definiteness is not
restrictive and aligned with (Tran etÂ al., 2021a). Î£Î£\Sigmaroman_Î£. The
corresponding precision matrix Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT is thus well-identified, and the
following proposition establishes a central relationship in this regard.

######  Proposition 5.1.

For a dğ‘‘ditalic_d-dimensional Gaussian variational posterior whose mean is
denoted by ğ›ğ›{\bm{\mu}}bold_italic_Î¼ and covariance matrix by
Î£Î£\Sigmaroman_Î£, consider the following two parameterizations: the
canonical parameterization
ğ›‡ğœ=(ğ›âŠ¤,vecâ¢(Î£)âŠ¤)âŠ¤superscriptğ›‡ğœsuperscriptsuperscriptğ›topvecsuperscriptÎ£toptop{\bm{\zeta^{c}}}=\left({\bm{\mu}}^{\top},\text{vec}\left(\Sigma\right)^{\top}%
\right)^{\top}bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT = ( bold_italic_Î¼ start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT , vec ( roman_Î£ ) start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT and the
inverse parametrization
ğ›‡ğ¢=(ğ›âŠ¤,vecâ¢(Î£âˆ’1)âŠ¤)âŠ¤superscriptğ›‡ğ¢superscriptsuperscriptğ›topvecsuperscriptsuperscriptÎ£1toptop{\bm{\zeta^{i}}}=\left({\bm{\mu}}^{\top},\text{vec}\left(\Sigma^{-1}\right)^{%
\top}\right)^{\top}bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i
end_POSTSUPERSCRIPT = ( bold_italic_Î¼ start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT , vec ( roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT )
start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT. It holds that,

| âˆ‡~ğâ¢â„’â¢(ğœ»ğ’Š)subscript~âˆ‡ğâ„’superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{{\bm{\mu}}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | =Î£â¢âˆ‡ğâ„’â¢(ğœ»ğ’„)absentÎ£subscriptâˆ‡ğâ„’superscriptğœ»ğ’„\displaystyle=\Sigma\nabla_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)= roman_Î£ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) |  | (9)  
---|---|---|---|---  
| âˆ‡~Î£âˆ’1â¢â„’â¢(ğœ»ğ’Š)subscript~âˆ‡superscriptÎ£1â„’superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | =âˆ’2â¢âˆ‡Î£â„’â¢(ğœ»ğ’„)â¢.absent2subscriptâˆ‡Î£â„’superscriptğœ»ğ’„.\displaystyle=-2\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\text{.}= - 2 âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) . |  | (10)  
  
Equation (10) establishes an algebraic link between the natural gradient of
the lower bound in the parametrization
ğœ»ğ’Šsuperscriptğœ»ğ’Š{\bm{\zeta^{i}}}bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_i end_POSTSUPERSCRIPT with respect to the precision matrix
Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT â€“ â€œinverseâ€ (covariance) parametrization â€“ and the
Euclidean gradient of the lower bound with respect to respect to the
covariance matrix Î£Î£\Sigmaroman_Î£. In particular, Proposition 5.1 suggests
that under the parametrization
ğœ»ğ’Šsuperscriptğœ»ğ’Š{\bm{\zeta^{i}}}bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_i end_POSTSUPERSCRIPT, one could update Î¼ğœ‡\muitalic_Î¼ and
Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT, for which natural gradients are of closed-form solution
and correspond to the standard Euclidean gradients of
â„’â¢(ğœ»ğ’„)â„’superscriptğœ»ğ’„\mathcal{L}\left({\bm{\zeta^{c}}}\right)caligraphic_L
( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT )
under the canonical parametrization. Compared to the canonical parametrization
adopted by (Tran etÂ al., 2021a) and the discussion in Section 5.1, the
advantage of using the parametrization
ğœ»ğ’Šsuperscriptğœ»ğ’Š{\bm{\zeta^{i}}}bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_i end_POSTSUPERSCRIPT is clear:
âˆ‡~Î£âˆ’1â¢â„’â¢(ğœ»ğ’Š)subscript~âˆ‡superscriptÎ£1â„’superscriptğœ»ğ’Š\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)over~
start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) requires only the
computation of
âˆ‡Î£â„’â¢(ğœ»ğ’„)subscriptâˆ‡Î£â„’superscriptğœ»ğ’„\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)âˆ‡
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ), whereas
âˆ‡~Î£âˆ’1â¢â„’â¢(ğœ»ğ’Š)=Î£â¢âˆ‡Î£â„’â¢Î£subscript~âˆ‡superscriptÎ£1â„’superscriptğœ»ğ’ŠÎ£subscriptâˆ‡Î£â„’Î£\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)=\Sigma%
\nabla_{\Sigma}\mathcal{L}\Sigmaover~ start_ARG âˆ‡ end_ARG
start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT
end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_i end_POSTSUPERSCRIPT ) = roman_Î£ âˆ‡ start_POSTSUBSCRIPT
roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£ additionally involves
additional matrix multiplications, that significantly increase the time-
complexity of the lower bound optimization. Section 5.3 shows furthermore,
such Euclidean gradients are straightforward to compute and that their
variance can be conveniently controlled.

The matrix Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT is symmetric and positive definite, thus lies in the
manifold â„³â„³\mathcal{M}caligraphic_M, and can be effectively updated with
the retraction algorithm for Î£Î£\Sigmaroman_Î£ in (8):

| Î£âˆ’1â†RÎ£âˆ’1â¢(Î²â¢Î£âˆ’1â¢âˆ‡Î£âˆ’1â„’â¢(ğœ»ğ’Š)â¢Î£âˆ’1)â†superscriptÎ£1subscriptğ‘…superscriptÎ£1ğ›½superscriptÎ£1subscriptâˆ‡superscriptÎ£1â„’superscriptğœ»ğ’ŠsuperscriptÎ£1\displaystyle\Sigma^{-1}\leftarrow R_{\Sigma^{-1}}\left(\beta\Sigma^{-1}\nabla% _{\Sigma^{-1}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)\Sigma^{-1}\right)roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT â† italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_Î² roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) | =RÎ£âˆ’1â¢(Î²2â¢âˆ‡~Î£âˆ’1â¢â„’â¢(ğœ»ğ’Š))absentsubscriptğ‘…superscriptÎ£1ğ›½2subscript~âˆ‡superscriptÎ£1â„’superscriptğœ»ğ’Š\displaystyle=R_{\Sigma^{-1}}\left(\frac{\beta}{2}\tilde{\nabla}_{\Sigma^{-1}}% \mathcal{L}\left({\bm{\zeta^{i}}}\right)\right)= italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( divide start_ARG italic_Î² end_ARG start_ARG 2 end_ARG over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) ) |   
---|---|---|---  
|  | =RÎ£âˆ’1â¢(âˆ’Î²â¢âˆ‡Î£â„’â¢(ğœ»ğ’„))â¢.absentsubscriptğ‘…superscriptÎ£1ğ›½subscriptâˆ‡Î£â„’superscriptğœ»ğ’„.\displaystyle=R_{\Sigma^{-1}}\left(-\beta\nabla_{\Sigma}\mathcal{L}\left({\bm{% \zeta^{c}}}\right)\right)\text{.}= italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( - italic_Î² âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) ) . |  | (11)  
  
As opposed to (8), updating Î£Î£\Sigmaroman_Î£ upon the approximation
â„ğœ»ğ’„â¢(Î£)â‰ˆÎ£âˆ’1âŠ—Î£âˆ’1subscriptâ„superscriptğœ»ğ’„Î£tensor-
productsuperscriptÎ£1superscriptÎ£1\mathcal{I}_{\bm{\zeta^{c}}}\left(\Sigma\right)\approx\Sigma^{-1}\otimes\Sigma%
^{-1}caligraphic_I start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_Î£ ) â‰ˆ roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âŠ— roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT for tacking the positive-
definite constraint, we update Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, for which the natural gradient
is available in an exact form, avoids the computation of the FIM, and reduces
to the simple and standard computation of the Euclidean gradient
âˆ‡Î£â„’â¢(ğœ»ğ’„)subscriptâˆ‡Î£â„’superscriptğœ»ğ’„\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)âˆ‡
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ). For updating
ğğ{\bm{\mu}}bold_italic_Î¼, it is reasonable to adopt a plain SGD-like step
driven by the natural parameter
âˆ‡~ğâ¢â„’â¢(ğœ»ğ’Š)=Î£â¢âˆ‡ğâ„’â¢(ğœ»ğ’„)subscript~âˆ‡ğâ„’superscriptğœ»ğ’ŠÎ£subscriptâˆ‡ğâ„’superscriptğœ»ğ’„\tilde{\nabla}_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)=\Sigma\nabla%
_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)over~ start_ARG âˆ‡ end_ARG
start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L (
bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) =
roman_Î£ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT
caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT ), as in (Tran etÂ al., 2021a). We refer to the following
update rules as Manifold Gaussian Variational Bayes on the Precision matrix
(MGVBP):

| ğt+1subscriptğğ‘¡1\displaystyle{\bm{\mu}}_{t+1}bold_italic_Î¼ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | =ğt+Î²â¢Î£â¢âˆ‡ğâ„’tâ¢(ğœ»ğ’„)â¢,absentsubscriptğğ‘¡ğ›½Î£subscriptâˆ‡ğsubscriptâ„’ğ‘¡superscriptğœ»ğ’„,\displaystyle={\bm{\mu}}_{t}+\beta\Sigma\nabla_{\bm{\mu}}\mathcal{L}_{t}\left(% {\bm{\zeta^{c}}}\right)\text{,}= bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² roman_Î£ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) , |  | (12)  
---|---|---|---|---  
| Î£t+1âˆ’1subscriptsuperscriptÎ£1ğ‘¡1\displaystyle\Sigma^{-1}_{t+1}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | =RÎ£tâˆ’1â¢(âˆ’Î²â¢âˆ‡Î£â„’tâ¢(ğœ»ğ’„))â¢,absentsubscriptğ‘…subscriptsuperscriptÎ£1ğ‘¡ğ›½subscriptâˆ‡Î£subscriptâ„’ğ‘¡superscriptğœ»ğ’„,\displaystyle=R_{\Sigma^{-1}_{t}}\left(-\beta\nabla_{\Sigma}\mathcal{L}_{t}% \left({\bm{\zeta^{c}}}\right)\right)\text{,}= italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( - italic_Î² âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) ) , |  | (13)  
  
where the gradients are evaluated at the current value of the parameters, e.g., âˆ‡ğâ„’tâ¢(ğœ»ğ’„)=âˆ‡ğâ„’â¢(ğœ»ğ’„)|ğœ»ğ’„=ğœ»ğ’„tsubscriptâˆ‡ğsubscriptâ„’ğ‘¡superscriptğœ»ğ’„evaluated-atsubscriptâˆ‡ğâ„’superscriptğœ»ğ’„superscriptğœ»ğ’„subscriptsuperscriptğœ»ğ’„ğ‘¡\nabla_{\bm{\mu}}\mathcal{L}_{t}\left({\bm{\zeta^{c}}}\right)=\nabla_{\bm{\mu}% }\mathcal{L}\left({\bm{\zeta^{c}}}\right)|_{{\bm{\zeta^{c}}}={\bm{\zeta^{c}}}_% {t}}âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) | start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT = bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT. Opposed to the MGVB update of (Tran etÂ al., 2021a), the update for Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT is consistent for the manifold â„³â„³\mathcal{M}caligraphic_M, as it employs the corresponding Riemann gradient, Î£âˆ’1â¢âˆ‡Î£âˆ’1â„’â¢(ğœ»ğ’Š)â¢Î£âˆ’1=âˆ‡Î£â„’â¢(ğœ»ğ’„)superscriptÎ£1subscriptâˆ‡superscriptÎ£1â„’superscriptğœ»ğ’ŠsuperscriptÎ£1subscriptâˆ‡Î£â„’superscriptğœ»ğ’„\Sigma^{-1}\nabla_{\Sigma^{-1}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)\Sigma^% {-1}=\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ), which is ğ’ªâ¢(d3)ğ’ªsuperscriptğ‘‘3\mathcal{O}\left(d^{3}\right)caligraphic_O ( italic_d start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) cheaper than computing âˆ‡~Î£â¢â„’â¢(ğœ»ğ’„)=Î£â¢âˆ‡Î£â„’â¢(ğœ»ğ’„)â¢Î£subscript~âˆ‡Î£â„’superscriptğœ»ğ’„Î£subscriptâˆ‡Î£â„’superscriptğœ»ğ’„Î£\tilde{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)=\Sigma\nabla_{% \Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\Sigmaover~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = roman_Î£ âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) roman_Î£.

###  Gradient computations

We elaborate on how to evaluate the gradients
âˆ‡Î£â„’â¢(ğœ»ğ’„)subscriptâˆ‡Î£â„’superscriptğœ»ğ’„\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)âˆ‡
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) and
âˆ‡ğâ„’â¢(ğœ»ğ’„)subscriptâˆ‡ğâ„’superscriptğœ»ğ’„\nabla_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)âˆ‡
start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L (
bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ). We
follow the Black-box approach (Ranganath etÂ al., 2014) under which such
gradients are approximated via Monte Carlo (MC) sampling and rely on function
queries only. Implementing MGVBP does not require the modelâ€™s gradient to be
specified nor to be computed numerically, e.g., with backpropagation. The so-
called log-derivative trick (see, e.g., (Ranganath etÂ al., 2014)) makes it
possible to evaluate the gradients of the LB as an expectation with respect to
the variational distribution. In particular,

| âˆ‡ğœ»ğ’„â„’â¢(ğœ»ğ’„)=ğ”¼qğœ»ğ’„â¢[âˆ‡ğœ»ğ’„[logâ¡qğœ»ğ’„â¢(ğœ½)]â¡hğœ»ğ’„â¢(ğœ½)]â¢,subscriptâˆ‡superscriptğœ»ğ’„â„’superscriptğœ»ğ’„subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]subscriptâˆ‡superscriptğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„ğœ½subscriptâ„superscriptğœ»ğ’„ğœ½,\nabla_{\bm{\zeta^{c}}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)=\mathbb{E}_{q_% {\bm{\zeta^{c}}}}\left[\nabla_{\bm{\zeta^{c}}}\left[\log q_{\bm{\zeta^{c}}}% \left({\bm{\theta}}\right)\right]\,h_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right% )\right]\text{,}âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] , |   
---|---|---  
  
where hğœ»ğ’„â¢(ğœ½)=logâ¡[pâ¢(ğœ½)â¢pâ¢(ğ’š|ğœ½)/qğœ»ğ’„â¢(ğœ½)]subscriptâ„superscriptğœ»ğ’„ğœ½ğ‘ğœ½ğ‘conditionalğ’šğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½h_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)=\log\left[p\left({\bm{\theta}}% \right)p\left({\bm{y}}|{\bm{\theta}}\right)/q_{{\bm{\zeta^{c}}}}\left({\bm{% \theta}}\right)\right]italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) = roman_log [ italic_p ( bold_italic_Î¸ ) italic_p ( bold_italic_y | bold_italic_Î¸ ) / italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ]. The gradient can be easily estimated using Sğ‘†Sitalic_S samples from the posterior through the unbiased estimator

| âˆ‡ğœ»ğ’„â„’â¢(ğœ»ğ’„)subscriptâˆ‡superscriptğœ»ğ’„â„’superscriptğœ»ğ’„\displaystyle\nabla_{\bm{\zeta^{c}}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) | â‰ˆ1Sâ¢âˆ‘s=1S[âˆ‡ğœ»ğ’„[logâ¡qğœ»ğ’„â¢(ğœ½s)]â¡hğœ»ğ’„â¢(ğœ½s)]â¢,absent1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptâˆ‡superscriptğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„subscriptğœ½ğ‘ subscriptâ„superscriptğœ»ğ’„subscriptğœ½ğ‘ ,\displaystyle\approx\frac{1}{S}\sum_{s=1}^{S}\left[\nabla_{\bm{\zeta^{c}}}% \left[\log q_{{\bm{\zeta^{c}}}}\left({\bm{\theta}}_{s}\right)\right]\,h_{\bm{% \zeta^{c}}}\left({\bm{\theta}}_{s}\right)\right]\text{,}â‰ˆ divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , |  | (14)  
---|---|---|---|---  
  
with ğœ½sâˆ¼qğœ»ğ’„similar-tosubscriptğœ½ğ‘
subscriptğ‘superscriptğœ»ğ’„{\bm{\theta}}_{s}\sim
q_{{\bm{\zeta^{c}}}}bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
end_POSTSUBSCRIPT âˆ¼ italic_q start_POSTSUBSCRIPT bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. For
the Gaussian variational case under consideration, it can be shown that (e.g.,
Wierstra etÂ al., 2014; Magris etÂ al., 2022):

| âˆ‡ğlogâ¡qğœ»ğ’„â¢(ğœ½)subscriptâˆ‡ğsubscriptğ‘superscriptğœ»ğ’„ğœ½\displaystyle\nabla_{\bm{\mu}}\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) | =Î£âˆ’1â¢(ğœ½âˆ’ğ)=ğ‚â¢,absentsuperscriptÎ£1ğœ½ğğ‚,\displaystyle=\Sigma^{-1}\left({\bm{\theta}}-{\bm{\mu}}\right)={\bm{\nu}}\text% {,}= roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Î¸ - bold_italic_Î¼ ) = bold_italic_Î½ , |  | (15)  
---|---|---|---|---  
| âˆ‡Î£logâ¡qğœ»ğ’„â¢(ğœ½)subscriptâˆ‡Î£subscriptğ‘superscriptğœ»ğ’„ğœ½\displaystyle\nabla_{\Sigma}\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) | =âˆ’12â¢(Î£âˆ’1âˆ’ğ‚â¢ğ‚âŠ¤)â¢.absent12superscriptÎ£1ğ‚superscriptğ‚top.\displaystyle=-\frac{1}{2}\left(\Sigma^{-1}-{\bm{\nu}}{\bm{\nu}}^{\top}\right)% \text{.}= - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - bold_italic_Î½ bold_italic_Î½ start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ) . |  | (16)  
  
Equations (15), (16) along with (14) and Proposition 5.1 immediately lead to
the feasible natural gradients estimators:

| âˆ‡~ğâ¢â„’â¢(ğœ»ğ’Š)subscript~âˆ‡ğâ„’superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{{\bm{\mu}}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆÎ£tâ¢âˆ‡^ğâ¢â„’â¢(ğœ»ğ’„)=1Sâ¢âˆ‘s=1S[(ğœ½sâˆ’ğ)â¢hğœ»ğ’„â¢(ğœ½s)]â¢,absentsubscriptÎ£ğ‘¡subscript^âˆ‡ğâ„’superscriptğœ»ğ’„1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptğœ½ğ‘ ğsubscriptâ„superscriptğœ»ğ’„subscriptğœ½ğ‘ ,\displaystyle\approx\Sigma_{t}\hat{\nabla}_{\bm{\mu}}\mathcal{L}\left({\bm{% \zeta^{c}}}\right)=\frac{1}{S}\sum_{s=1}^{S}\left[\left({\bm{\theta}}_{s}-{\bm% {\mu}}\right)h_{{\bm{\zeta^{c}}}}\left({\bm{\theta}}_{s}\right)\right]\text{,}â‰ˆ roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ ) italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , |   
---|---|---|---  
| âˆ‡~Î£âˆ’1â¢â„’â¢(ğœ»ğ’Š)subscript~âˆ‡superscriptÎ£1â„’superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆâˆ’âˆ‡^Î£â¢â„’â¢(ğœ»ğ’„)=12â¢Sâ¢âˆ‘s=1S[(Î£âˆ’1âˆ’ğ‚sâ¢ğ‚sâŠ¤)â¢hğœ»ğ’„â¢(ğœ½s)]â¢,absentsubscript^âˆ‡Î£â„’superscriptğœ»ğ’„12ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]superscriptÎ£1subscriptğ‚ğ‘ superscriptsubscriptğ‚ğ‘ topsubscriptâ„superscriptğœ»ğ’„subscriptğœ½ğ‘ ,\displaystyle\approx-\hat{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}% \right)=\frac{1}{2S}\sum_{s=1}^{S}\left[\left(\Sigma^{-1}-{\bm{\nu}}_{s}{{\bm{% \nu}}_{s}}^{\top}\right)h_{{\bm{\zeta^{c}}}}\left({\bm{\theta}}_{s}\right)% \right]\text{,}â‰ˆ - over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 2 italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - bold_italic_Î½ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_italic_Î½ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ) italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , |   
  
with ğ‚s=Î£âˆ’1â¢(ğœ½sâˆ’ğ)subscriptğ‚ğ‘ superscriptÎ£1subscriptğœ½ğ‘ ğ{\bm{\nu}}_{s}=\Sigma^{-1}\left({\bm{\theta}}_{s}-{\bm{\mu}}\right)bold_italic_Î½ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ ). As for the MGVB update, MGVBP applies exclusively to Gaussian variational posteriors, yet no constraints are imposed on the parametric form of the prior pâ¢(ğœ½)ğ‘ğœ½p\left({\bm{\theta}}\right)italic_p ( bold_italic_Î¸ ). When considering a Gaussian prior, the implementation of the MGVBP update can take advantage of some analytical results leading to MC estimators of reduced variance, namely implemented over the log-likelihood logâ¡pâ¢(ğ’š|ğœ½s)ğ‘conditionalğ’šsubscriptğœ½ğ‘ \log p\left({\bm{y}}|{\bm{\theta}}_{s}\right)roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) rather than the hâ„hitalic_h-function.

In Appendix C.5, we show that under a Gaussian prior specification, the above
updates can also be implemented in terms of the model likelihood rather than
the hâ„hitalic_h-function. At iteration tğ‘¡titalic_t, the general form of the
gradients evaluated at the current (epoch-tğ‘¡titalic_t) values of the
parameters read

| âˆ‡~ğâ¢â„’tâ¢(ğœ»ğ’Š)subscript~âˆ‡ğsubscriptâ„’ğ‘¡superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{{\bm{\mu}}}\mathcal{L}_{t}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆcğt+1Sâ¢âˆ‘s=1S[(ğœ½sâˆ’ğt)â¢logâ¡fâ¢(ğœ½s)]â¢,absentsubscriptğ‘subscriptğğ‘¡1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptğœ½ğ‘ subscriptğğ‘¡ğ‘“subscriptğœ½ğ‘ ,\displaystyle\approx c_{{\bm{\mu}}_{t}}+\frac{1}{S}\sum_{s=1}^{S}\left[\left({% \bm{\theta}}_{s}-{\bm{\mu}}_{t}\right)\log f\left({\bm{\theta}}_{s}\right)% \right]\text{,}â‰ˆ italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , |  | (17)  
---|---|---|---|---  
| âˆ‡~Î£âˆ’1â¢â„’tâ¢(ğœ»ğ’Š)subscript~âˆ‡superscriptÎ£1subscriptâ„’ğ‘¡superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}_{t}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆCÎ£t+12â¢Sâ¢âˆ‘s=1S[(Î£tâˆ’1âˆ’ğ‚t,sâ¢ğ‚t,sâŠ¤)â¢logâ¡fâ¢(ğœ½s)]â¢,absentsubscriptğ¶subscriptÎ£ğ‘¡12ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptsuperscriptÎ£1ğ‘¡subscriptğ‚ğ‘¡ğ‘ superscriptsubscriptğ‚ğ‘¡ğ‘ topğ‘“subscriptğœ½ğ‘ ,\displaystyle\approx C_{\Sigma_{t}}+\frac{1}{2S}\sum_{s=1}^{S}\left[\left(% \Sigma^{-1}_{t}-{\bm{\nu}}_{t,s}{{\bm{\nu}}_{t,s}}^{\top}\right)\log f\left({% \bm{\theta}}_{s}\right)\right]\text{,}â‰ˆ italic_C start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_Î½ start_POSTSUBSCRIPT italic_t , italic_s end_POSTSUBSCRIPT bold_italic_Î½ start_POSTSUBSCRIPT italic_t , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ) roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , |  | (18)  
  
with ğ‚s,t=Î£tâˆ’1â¢(ğœ½sâˆ’ğt)subscriptğ‚ğ‘
ğ‘¡subscriptsuperscriptÎ£1ğ‘¡subscriptğœ½ğ‘
subscriptğğ‘¡{\bm{\nu}}_{s,t}=\Sigma^{-1}_{t}\left({\bm{\theta}}_{s}-{\bm{\mu}}_{t}\right)bold_italic_Î½
start_POSTSUBSCRIPT italic_s , italic_t end_POSTSUBSCRIPT = roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t
end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_t
end_POSTSUBSCRIPT ) where in general (whether the prior is Gaussian or not)

| {CÎ£t=0â¢,cğt=ğŸâ¢,logâ¡fâ¢(ğœ½s)=hğœ»tcâ¢(ğœ½s)â¢,casessubscriptğ¶subscriptÎ£ğ‘¡0,otherwisesubscriptğ‘subscriptğğ‘¡0,otherwiseğ‘“subscriptğœ½ğ‘ subscriptâ„subscriptsuperscriptğœ»ğ‘ğ‘¡subscriptğœ½ğ‘ ,otherwise\begin{cases}C_{\Sigma_{t}}=0\text{,}\\\ c_{{\bm{\mu}}_{t}}=\bm{0}\text{,}\\\ \log f\left({\bm{\theta}}_{s}\right)=h_{\bm{\zeta}^{c}_{t}}\left({\bm{\theta}}% _{s}\right)\text{,}\end{cases}{ start_ROW start_CELL italic_C start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 0 , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = bold_0 , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) , end_CELL start_CELL end_CELL end_ROW |  | (19)  
---|---|---|---  
  
whereas for a Gaussian prior, one can adopt

| {CÎ£t=âˆ’12â¢Î£tâˆ’1+12â¢Î£0âˆ’1â¢,cğt=âˆ’Î£tâ¢Î£0âˆ’1â¢(ğtâˆ’ğ0)â¢,logâ¡fâ¢(ğœ½s)=logâ¡pâ¢(ğ’š|ğœ½s)â¢.casessubscriptğ¶subscriptÎ£ğ‘¡12subscriptsuperscriptÎ£1ğ‘¡12subscriptsuperscriptÎ£10,otherwisesubscriptğ‘subscriptğğ‘¡subscriptÎ£ğ‘¡subscriptsuperscriptÎ£10subscriptğğ‘¡subscriptğ0,otherwiseğ‘“subscriptğœ½ğ‘ ğ‘conditionalğ’šsubscriptğœ½ğ‘ .otherwise\begin{cases}C_{\Sigma_{t}}=-\frac{1}{2}\Sigma^{-1}_{t}+\frac{1}{2}\Sigma^{-1}% _{0}\text{,}\\\ c_{{\bm{\mu}}_{t}}=-\Sigma_{t}\Sigma^{-1}_{0}\left({\bm{\mu}}_{t}-{\bm{\mu}}_{% 0}\right)\text{,}\\\ \log f\left({\bm{\theta}}_{s}\right)=\log p\left({\bm{y}}|{\bm{\theta}}_{s}% \right)\text{.}\end{cases}{ start_ROW start_CELL italic_C start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = - divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = - roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) . end_CELL start_CELL end_CELL end_ROW |  | (20)  
---|---|---|---  
  
That is, (19) holds for a generic prior (including a Gaussian prior as a
special case), whereas (20) holds with Gaussian priors only. As shown in
Appendix C.5, under a Gaussian prior, certain components of the hâ„hitalic_h-
function involved in (19) have an algebraic solution, and the MC estimator
(20) based on the log-likelihood function is of reduced variance. Thus, under
a Gaussian prior, (20) is preferred to (19). This alternative estimator is
applicable in (Tran etÂ al., 2021a) and, generally, in other black-box
Gaussian VI contexts. Note that the log-likelihood case does not involve an
additional inversion for retrieving Î£Î£\Sigmaroman_Î£ in
cğsubscriptğ‘ğc_{\bm{\mu}}italic_c start_POSTSUBSCRIPT bold_italic_Î¼
end_POSTSUBSCRIPT, as Î£Î£\Sigmaroman_Î£ is anyway required in the second-
order retraction (for both MGVB and MGVBP). This aspect is further developed
in Section 5.7. For inverting Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT we suggest inverting the
Cholesky factor Lğ¿Litalic_L of Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT and compute Î£Î£\Sigmaroman_Î£
as Lâˆ’âŠ¤â¢Lâˆ’1superscriptğ¿absenttopsuperscriptğ¿1L^{-\top}L^{-1}italic_L
start_POSTSUPERSCRIPT - âŠ¤ end_POSTSUPERSCRIPT italic_L start_POSTSUPERSCRIPT
- 1 end_POSTSUPERSCRIPT. The triangular form of Lğ¿Litalic_L can be inverted
with back-substitution, requiring d3/3superscriptğ‘‘33d^{3}/3italic_d
start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT / 3 flops instead of
d3superscriptğ‘‘3d^{3}italic_d start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT.
Lâˆ’âŠ¤superscriptğ¿absenttopL^{-\top}italic_L start_POSTSUPERSCRIPT - âŠ¤
end_POSTSUPERSCRIPT is furthermore used for generating the draws
ğœ½ssubscriptğœ½ğ‘ {\bm{\theta}}_{s}bold_italic_Î¸ start_POSTSUBSCRIPT
italic_s end_POSTSUBSCRIPT as ğœ½s=ğ+Lâˆ’âŠ¤â¢ğœºsubscriptğœ½ğ‘
ğsuperscriptğ¿absenttopğœº{\bm{\theta}}_{s}={\bm{\mu}}+L^{-\top}\bm{\varepsilon}bold_italic_Î¸
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = bold_italic_Î¼ + italic_L
start_POSTSUPERSCRIPT - âŠ¤ end_POSTSUPERSCRIPT bold_italic_Îµ, with
ğœºâˆ¼ğ’©â¢(ğŸ,I)similar-
toğœºğ’©0ğ¼\bm{\varepsilon}\sim\mathcal{N}\left(\bm{0},I\right)bold_italic_Îµ
âˆ¼ caligraphic_N ( bold_0 , italic_I ). We suggest using control variates to
reduce the variance of the stochastic gradient estimators; see Section 6.2.

Though the lower bound is not directly involved in MGVBP updates, it can be
naively estimated at each iteration as

| â„’^tâ¢(ğœ»ğ’„)=1Sâ¢âˆ‘s=1S[logâ¡pâ¢(ğœ½s)+logâ¡pâ¢(ğ’š|ğœ½s)âˆ’logâ¡qğœ»tcâ¢(ğœ½s)]â¢,ğœ½âˆ¼qğœ»tcâ¢.formulae-sequencesubscript^â„’ğ‘¡superscriptğœ»ğ’„1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]ğ‘subscriptğœ½ğ‘ ğ‘conditionalğ’šsubscriptğœ½ğ‘ subscriptğ‘subscriptsuperscriptğœ»ğ‘ğ‘¡subscriptğœ½ğ‘ ,similar-toğœ½subscriptğ‘subscriptsuperscriptğœ»ğ‘ğ‘¡.\hat{\mathcal{L}}_{t}\left({\bm{\zeta^{c}}}\right)=\frac{1}{S}\sum_{s=1}^{S}% \left[\log p\left({\bm{\theta}}_{s}\right)+\log p\left({\bm{y}}|{\bm{\theta}}_% {s}\right)-\log q_{\bm{\zeta}^{c}_{t}}\left({\bm{\theta}}_{s}\right)\right]% \text{,}\quad{\bm{\theta}}\sim q_{\bm{\zeta}^{c}_{t}}\text{.}over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ roman_log italic_p ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) + roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , bold_italic_Î¸ âˆ¼ italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT . |  | (21)  
---|---|---|---  
  
â„’^tâ¢(ğœ»ğ’„)subscript^â„’ğ‘¡superscriptğœ»ğ’„\hat{\mathcal{L}}_{t}\left({\bm{\zeta^{c}}}\right)over^
start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT
( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) is
required for terminating the optimization routine (see Section 6.4), verifying
anomalies in the algorithm (the LB should increase across the iterations and
eventually and converge), and comparing MGVBP with MGVB, as in Section 7.

###  Retraction and vector transport

Aligned with (Tran etÂ al., 2021a), we adopt the retraction method advanced in
(Jeuris etÂ al., 2012) for the manifold â„³â„³\mathcal{M}caligraphic_M, but on
the actual Riemannian gradients for this manifold,

| RÎ£âˆ’1â¢(ğƒ)=Î£âˆ’1+ğƒ+12â¢ğƒâ¢Î£â¢ğƒâ¢,subscriptğ‘…superscriptÎ£1ğƒsuperscriptÎ£1ğƒ12ğƒÎ£ğƒ,R_{\Sigma^{-1}}\left({\bm{\xi}}\right)=\Sigma^{-1}+{\bm{\xi}}+\frac{1}{2}{\bm{% \xi}}\Sigma{\bm{\xi}}\text{,}italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¾ ) = roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + bold_italic_Î¾ + divide start_ARG 1 end_ARG start_ARG 2 end_ARG bold_italic_Î¾ roman_Î£ bold_italic_Î¾ , |  | (22)  
---|---|---|---  
  
with ğƒâˆˆTÎ£âˆ’1â¢â„³ğƒsubscriptğ‘‡superscriptÎ£1â„³{\bm{\xi}}\in
T_{\Sigma^{-1}}\mathcal{M}bold_italic_Î¾ âˆˆ italic_T start_POSTSUBSCRIPT
roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT
caligraphic_M being the rescaled natural gradient
Î²/2â¢âˆ‡~Î£âˆ’1â¢â„’â¢(ğœ»ğ’„)=âˆ’Î²â¢âˆ‡Î£â„’â¢(ğœ»ğ’„)ğ›½2subscript~âˆ‡superscriptÎ£1â„’superscriptğœ»ğ’„ğ›½subscriptâˆ‡Î£â„’superscriptğœ»ğ’„\beta/2\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)=-%
\beta\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)italic_Î² / 2
over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT
- 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) = - italic_Î² âˆ‡
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ). Vector transport is
easily implemented by

| ğ’¯Î£tâˆ’1â†’Î£t+1âˆ’1â¢(ğƒ)=Eâ¢ğƒâ¢EâŠ¤â¢,subscriptğ’¯â†’subscriptsuperscriptÎ£1ğ‘¡subscriptsuperscriptÎ£1ğ‘¡1ğƒğ¸ğƒsuperscriptğ¸top,\mathcal{T}_{\Sigma^{-1}_{t}\rightarrow\Sigma^{-1}_{t+1}}\left({\bm{\xi}}% \right)=E{\bm{\xi}}E^{\top}\text{,}caligraphic_T start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT â†’ roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¾ ) = italic_E bold_italic_Î¾ italic_E start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT , |  | (23)  
---|---|---|---  
  
with E=(Î£t+1âˆ’1â¢Î£t)12,ğƒâˆˆTÎ£âˆ’1â¢â„³formulae-
sequenceğ¸superscriptsubscriptsuperscriptÎ£1ğ‘¡1subscriptÎ£ğ‘¡12ğƒsubscriptğ‘‡superscriptÎ£1â„³E=\left(\Sigma^{-1}_{t+1}\Sigma_{t}\right)^{\frac{1}{2}},\,{\bm{\xi}}\in
T_{% \Sigma^{-1}}\mathcal{M}italic_E = ( roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT
roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )
start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG
end_POSTSUPERSCRIPT , bold_italic_Î¾ âˆˆ italic_T start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_M.
In implementations, for numerically granting the symmetric form of a matrix
Pğ‘ƒPitalic_P, we compute Pğ‘ƒPitalic_P as
1/2â¢(P+PâŠ¤)12ğ‘ƒsuperscriptğ‘ƒtop1/2(P+P^{\top})1 / 2 ( italic_P + italic_P
start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ). We refer to the Manopt
toolbox (Boumal etÂ al., 2014) for further practical details on implementing
the above two algorithms in a numerically stable fashion. The momentum
gradients immediately follow:

| âˆ‡~Î£âˆ’1mom.â¢â„’t+1â¢(ğœ»ğ’Š)=Ï‰superscriptsubscript~âˆ‡superscriptÎ£1mom.subscriptâ„’ğ‘¡1superscriptğœ»ğ’Šğœ”\displaystyle\tilde{\nabla}_{\Sigma^{-1}}^{\text{mom.}}\mathcal{L}_{t+1}\left(% {\bm{\zeta^{i}}}\right)=\omega\,over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT mom. end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) = italic_Ï‰ | ğ’¯Î£tâˆ’1â†’Î£t+1âˆ’1â¢(âˆ‡~Î£âˆ’1mom.â¢â„’tâ¢(ğœ»ğ’Š))+(1âˆ’Ï‰)â¢âˆ‡~Î£âˆ’1â¢â„’t+1â¢(ğœ»ğ’Š)â¢,subscriptğ’¯â†’subscriptsuperscriptÎ£1ğ‘¡subscriptsuperscriptÎ£1ğ‘¡1superscriptsubscript~âˆ‡superscriptÎ£1mom.subscriptâ„’ğ‘¡superscriptğœ»ğ’Š1ğœ”subscript~âˆ‡superscriptÎ£1subscriptâ„’ğ‘¡1superscriptğœ»ğ’Š,\displaystyle\mathcal{T}_{\Sigma^{-1}_{t}\rightarrow\Sigma^{-1}_{t+1}}\left(% \tilde{\nabla}_{\Sigma^{-1}}^{\text{mom.}}\mathcal{L}_{t}\left({\bm{\zeta^{i}}% }\right)\right)+\left(1-\omega\right)\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}_{% t+1}\left({\bm{\zeta^{i}}}\right)\text{,}caligraphic_T start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT â†’ roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT mom. end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) ) + ( 1 - italic_Ï‰ ) over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) , |  | (24)  
---|---|---|---|---  
| âˆ‡~ğmom.â¢â„’t+1â¢(ğœ»ğ’Š)=Ï‰superscriptsubscript~âˆ‡ğmom.subscriptâ„’ğ‘¡1superscriptğœ»ğ’Šğœ”\displaystyle\tilde{\nabla}_{{\bm{\mu}}}^{\text{mom.}}\mathcal{L}_{t+1}\left({% \bm{\zeta^{i}}}\right)=\omega\,over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT mom. end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) = italic_Ï‰ | âˆ‡~ğmom.â¢â„’tâ¢(ğœ»ğ’Š)+(1âˆ’Ï‰)â¢âˆ‡~ğâ¢â„’tâ¢(ğœ»ğ’Š)â¢,superscriptsubscript~âˆ‡ğmom.subscriptâ„’ğ‘¡superscriptğœ»ğ’Š1ğœ”subscript~âˆ‡ğsubscriptâ„’ğ‘¡superscriptğœ»ğ’Š,\displaystyle\tilde{\nabla}_{{\bm{\mu}}}^{\text{mom.}}\mathcal{L}_{t}\left({% \bm{\zeta^{i}}}\right)+\left(1-\omega\right)\tilde{\nabla}_{\bm{\mu}}\mathcal{% L}_{t}\left({\bm{\zeta^{i}}}\right)\text{,}over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT mom. end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) + ( 1 - italic_Ï‰ ) over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) , |  | (25)  
  
where the weight 0<Ï‰<10ğœ”10<\omega<10 < italic_Ï‰ < 1 is a hyper-parameter.
Algorithm 1 summarizes the MGVBP update for the Gaussian prior-variational
posterior case. Aspects of relevance in its implementation are discussed in
Section 6.

Algorithm 1 MGVBP implementation

Â Â Set hyper-parameters:â€‰0<Î²,Ï‰<1formulae-
sequence0ğ›½ğœ”10<\beta,\omega<10 < italic_Î² , italic_Ï‰ < 1, Sğ‘†Sitalic_S

Â Â Set the type of gradient estimator, i.e. function
logâ¡fâ¢(ğœ½s)ğ‘“subscriptğœ½ğ‘ \log f\left({\bm{\theta}}_{s}\right)roman_log
italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )

Â Â Set the initial values for ğğ{\bm{\mu}}bold_italic_Î¼, Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, specify pâ¢(ğœ½)ğ‘ğœ½p\left({\bm{\theta}}\right)italic_p ( bold_italic_Î¸ ) and pâ¢(ğ’š|ğœ½)ğ‘conditionalğ’šğœ½p\left({\bm{y}}|{\bm{\theta}}\right)italic_p ( bold_italic_y | bold_italic_Î¸ )

Â Â t=1ğ‘¡1t=1italic_t = 1, Stop=falseStopfalse\text{Stop}=\texttt{false}Stop
= false

Â Â Generate:â€‰ ğœ½sâˆ¼qğ,Î£similar-tosubscriptğœ½ğ‘
subscriptğ‘ğÎ£{\bm{\theta}}_{s}\sim q_{{\bm{\mu}},\Sigma}bold_italic_Î¸
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT âˆ¼ italic_q
start_POSTSUBSCRIPT bold_italic_Î¼ , roman_Î£ end_POSTSUBSCRIPT, s=1,â€¦,Sğ‘
1â€¦ğ‘†s=1,\dots,Sitalic_s = 1 , â€¦ , italic_S

Â Â Compute:â€‰
g^ğ=Î£â¢âˆ‡^ğâ¢â„’subscript^ğ‘”ğÎ£subscript^âˆ‡ğâ„’\hat{g}_{\bm{\mu}}=\Sigma\hat{\nabla}_{\bm{\mu}}\mathcal{L}over^
start_ARG italic_g end_ARG start_POSTSUBSCRIPT bold_italic_Î¼
end_POSTSUBSCRIPT = roman_Î£ over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT
bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L,
g^Î£âˆ’1=âˆ’âˆ‡^Î£â¢â„’subscript^ğ‘”superscriptÎ£1subscript^âˆ‡Î£â„’\hat{g}_{\Sigma^{-1}}=-\hat{\nabla}_{\Sigma}\mathcal{L}over^
start_ARG italic_g end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT
- 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = - over^ start_ARG âˆ‡ end_ARG
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L
{eqs.â€‰(17),â€‰(18)}

Â Â mğ=g^ğsubscriptğ‘šğsubscript^ğ‘”ğm_{\bm{\mu}}=\hat{g}_{\bm{\mu}}italic_m
start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT = over^ start_ARG
italic_g end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT,
mÎ£âˆ’1=g^Î£âˆ’1subscriptğ‘šsuperscriptÎ£1subscript^ğ‘”superscriptÎ£1m_{\Sigma^{-1}}=\hat{g}_{\Sigma^{-1}}italic_m
start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT
end_POSTSUBSCRIPT = over^ start_ARG italic_g end_ARG start_POSTSUBSCRIPT
roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT
{initialize momentum}

Â Â whileÂ Stop=falseStopfalse\text{Stop}=\texttt{false}Stop = falseÂ do

Â Â Â Â Â ğ=ğ+Î²â¢mğğğğ›½subscriptğ‘šğ{\bm{\mu}}={\bm{\mu}}+\beta
m_{\bm{\mu}}bold_italic_Î¼ = bold_italic_Î¼ + italic_Î² italic_m
start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT {MGVBP update for
ğğ{\bm{\mu}}bold_italic_Î¼}

Â Â Â Â Â
Î£oldâˆ’1=Î£âˆ’1subscriptsuperscriptÎ£1oldsuperscriptÎ£1\Sigma^{-1}_{\text{old}}=\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT old
end_POSTSUBSCRIPT = roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT

Â Â Â Â Â
Î£âˆ’1=RÎ£oldâˆ’1â¢(Î²â¢mÎ£âˆ’1)superscriptÎ£1subscriptğ‘…subscriptsuperscriptÎ£1oldğ›½subscriptğ‘šsuperscriptÎ£1\Sigma^{-1}=R_{\Sigma^{-1}_{\text{old}}}\left(\beta
m_{\Sigma^{-1}}\right)roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT =
italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT old end_POSTSUBSCRIPT
end_POSTSUBSCRIPT ( italic_Î² italic_m start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) {MGVBP
update for Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT}

Â Â Â Â Â Generate:â€‰ ğœ½sâˆ¼qğ,Î£similar-tosubscriptğœ½ğ‘
subscriptğ‘ğÎ£{\bm{\theta}}_{s}\sim q_{{\bm{\mu}},\Sigma}bold_italic_Î¸
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT âˆ¼ italic_q
start_POSTSUBSCRIPT bold_italic_Î¼ , roman_Î£ end_POSTSUBSCRIPT, s=1,â€¦,Sğ‘
1â€¦ğ‘†s=1,\dots,Sitalic_s = 1 , â€¦ , italic_S

Â Â Â Â Â Compute:â€‰ g^ğsubscript^ğ‘”ğ\hat{g}_{\bm{\mu}}over^ start_ARG
italic_g end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT,
g^Î£âˆ’1subscript^ğ‘”superscriptÎ£1\hat{g}_{\Sigma^{-1}}over^ start_ARG
italic_g end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT

Â Â Â Â Â
mğ=Ï‰â¢mğ+(1âˆ’Ï‰)â¢g^ğsubscriptğ‘šğğœ”subscriptğ‘šğ1ğœ”subscript^ğ‘”ğm_{{\bm{\mu}}}=\omega
m_{\bm{\mu}}+\left(1-\omega\right)\hat{g}_{\bm{\mu}}italic_m
start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT = italic_Ï‰ italic_m
start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT + ( 1 - italic_Ï‰ ) over^
start_ARG italic_g end_ARG start_POSTSUBSCRIPT bold_italic_Î¼
end_POSTSUBSCRIPT {eq.(24)}

Â Â Â Â Â
mÎ£âˆ’1=ğ’¯Î£oldâˆ’1â†’Î£âˆ’1â¢(mÎ£âˆ’1)+(1âˆ’Ï‰)â¢g^Î£âˆ’1subscriptğ‘šsuperscriptÎ£1subscriptğ’¯â†’subscriptsuperscriptÎ£1oldsuperscriptÎ£1subscriptğ‘šsuperscriptÎ£11ğœ”subscript^ğ‘”superscriptÎ£1m_{\Sigma^{-1}}=\mathcal{T}_{\Sigma^{-1}_{\text{old}}\rightarrow\Sigma^{-1}}%
\left(m_{\Sigma^{-1}}\right)+\left(1-\omega\right)\hat{g}_{\Sigma^{-1}}italic_m
start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT
end_POSTSUBSCRIPT = caligraphic_T start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT old
end_POSTSUBSCRIPT â†’ roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT
end_POSTSUBSCRIPT ( italic_m start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) + ( 1 -
italic_Ï‰ ) over^ start_ARG italic_g end_ARG start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT {eq.â€‰(25)}

Â Â Â Â Â Compute:â€‰ â„’^^â„’\hat{\mathcal{L}}over^ start_ARG caligraphic_L
end_ARG {eq.â€‰(21)}

Â Â Â Â Â t=t+1ğ‘¡ğ‘¡1t=t+1italic_t = italic_t + 1,
Stop=fexitâ¢(t,â€¦)Stopsubscriptğ‘“exitğ‘¡â€¦\text{Stop}=f_{\text{exit}}(t,\dots)Stop
= italic_f start_POSTSUBSCRIPT exit end_POSTSUBSCRIPT ( italic_t , â€¦ ) {see
Section 6}

Â Â endÂ while

###  Isotropic prior

For mid-sized to large-scale problems, the prior is commonly specified as an
isotropic Gaussian of mean ğ0subscriptğ0{\bm{\mu}}_{0}bold_italic_Î¼
start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, often
ğ0=ğŸsubscriptğ00{\bm{\mu}}_{0}=\bm{0}bold_italic_Î¼ start_POSTSUBSCRIPT 0
end_POSTSUBSCRIPT = bold_0, and covariance matrix
Ï„âˆ’1â¢Idsuperscriptğœ1subscriptğ¼ğ‘‘\tau^{-1}I_{d}italic_Ï„
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT
italic_d end_POSTSUBSCRIPT, with Ï„>0ğœ0\tau>0italic_Ï„ > 0 a scalar precision
parameter. The covariance matrix of the variational posterior can be either
diagonal or not. Whether a full covariance specification
(d2superscriptğ‘‘2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT
parameters) can provide additional degrees of freedom to gauge modelsâ€™
predictive ability, a diagonal posterior (dğ‘‘ditalic_d parameters) can be
practically and computationally convenient to adopt, e.g., in large-sized
problems. The diagonal-posterior assumption is broadly adopted in Bayesian
inference and VI, e.g., (Blundell etÂ al., 2015; Ganguly and Earp, 2021; Tran
etÂ al., 2021b) and Bayesian ML applications, e.g., (Kingma and Welling, 2013;
Graves, 2011; Khan etÂ al., 2018; Osawa etÂ al., 2019), in Appendix A we
provide a block-diagonal variant.

#### Isotropic prior and diagonal Gaussian posterior

Assume a dğ‘‘ditalic_d-variate diagonal Gaussian variational specification,
that is qâˆ¼ğ’©â¢(ğ,Î£)similar-
toğ‘ğ’©ğÎ£q\sim\mathcal{N}\left({\bm{\mu}},\Sigma\right)italic_q âˆ¼
caligraphic_N ( bold_italic_Î¼ , roman_Î£ ) with
diagâ¢(Î£)=ğˆ2diagÎ£superscriptğˆ2\text{diag}\left(\Sigma\right)=\bm{\sigma}^{2}diag
( roman_Î£ ) = bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT,
Î£iâ¢j=0subscriptÎ£ğ‘–ğ‘—0\Sigma_{ij}=0roman_Î£ start_POSTSUBSCRIPT italic_i
italic_j end_POSTSUBSCRIPT = 0, for i,j=1,â€¦,dformulae-
sequenceğ‘–ğ‘—1â€¦ğ‘‘i,j=1,\dots,ditalic_i , italic_j = 1 , â€¦ , italic_d and
iâ‰ jğ‘–ğ‘—i\neq jitalic_i â‰ italic_j. In this case,
Î£âˆ’1=diagâ¢(1/ğˆ2)superscriptÎ£1diag1superscriptğˆ2\Sigma^{-1}=\text{diag}\left(1/\bm{\sigma}^{2}\right)roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = diag ( 1 / bold_italic_Ïƒ
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), where the division is intended
element-wise, is now a dÃ—1ğ‘‘1d\times 1italic_d Ã— 1 vector. Be
ğœ»ğ’„=(ğâŠ¤,(ğˆ2)âŠ¤)âŠ¤superscriptğœ»ğ’„superscriptsuperscriptğtopsuperscriptsuperscriptğˆ2toptop{\bm{\zeta^{c}}}=\left({\bm{\mu}}^{\top},\left(\bm{\sigma}^{2}\right)^{\top}%
\right)^{\top}bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT = ( bold_italic_Î¼ start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT , ( bold_italic_Ïƒ start_POSTSUPERSCRIPT 2
end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT )
start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT and
ğœ»ğ’Š=(ğâŠ¤,(ğˆâˆ’2)âŠ¤)âŠ¤superscriptğœ»ğ’Šsuperscriptsuperscriptğtopsuperscriptsuperscriptğˆ2toptop{\bm{\zeta^{i}}}=\left({\bm{\mu}}^{\top},\left(\bm{\sigma}^{-2}\right)^{\top}%
\right)^{\top}bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i
end_POSTSUPERSCRIPT = ( bold_italic_Î¼ start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT , ( bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2
end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT )
start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT, thus
âˆ‡ğˆ2â„’â¢(ğœ»ğ’„)=diagâ¢(âˆ‡ğˆ2â„’â¢(ğœ»ğ’„))subscriptâˆ‡superscriptğˆ2â„’superscriptğœ»ğ’„diagsubscriptâˆ‡superscriptğˆ2â„’superscriptğœ»ğ’„\nabla_{\bm{\sigma}^{2}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)=\text{diag}%
\left(\nabla_{\bm{\sigma}^{2}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\right)âˆ‡
start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT
end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_c end_POSTSUPERSCRIPT ) = diag ( âˆ‡ start_POSTSUBSCRIPT
bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT
caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT ) ). Updating Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT amounts to updating
ğˆâˆ’2superscriptğˆ2\bm{\sigma}^{-2}bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2
end_POSTSUPERSCRIPT: the natural gradient retraction-based update for
ğˆâˆ’2superscriptğˆ2\bm{\sigma}^{-2}bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2
end_POSTSUPERSCRIPT is now based on the equality
âˆ‡~ğˆâˆ’2â¢â„’â¢(ğœ»ğ’Š)=âˆ’âˆ‡ğˆ2â„’â¢(ğœ»ğ’„)subscript~âˆ‡superscriptğˆ2â„’superscriptğœ»ğ’Šsubscriptâˆ‡superscriptğˆ2â„’superscriptğœ»ğ’„\tilde{\nabla}_{\bm{\sigma}^{-2}}\mathcal{L}\left({\bm{\zeta^{i}}}\right)=-%
\nabla_{\bm{\sigma}^{2}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)over~
start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT
- 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) = - âˆ‡
start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT
end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_c end_POSTSUPERSCRIPT ), so that the general-case MGVBP update
reads

| ğt+1subscriptğğ‘¡1\displaystyle{\bm{\mu}}_{t+1}bold_italic_Î¼ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | =ğt+ğˆt2âŠ™Î²â¢âˆ‡ğâ„’â¢(ğœ»ğ’„)â¢,absentsubscriptğğ‘¡direct-productsubscriptsuperscriptğˆ2ğ‘¡ğ›½subscriptâˆ‡ğâ„’superscriptğœ»ğ’„,\displaystyle={\bm{\mu}}_{t}+\bm{\sigma}^{2}_{t}\odot\beta\nabla_{\bm{\mu}}% \mathcal{L}\left({\bm{\zeta^{c}}}\right)\textrm{,}= bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âŠ™ italic_Î² âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) , |   
---|---|---|---  
| ğˆt+1âˆ’2subscriptsuperscriptğˆ2ğ‘¡1\displaystyle\bm{\sigma}^{-2}_{t+1}bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | =Rğˆt+1âˆ’2â¢(âˆ’Î²â¢âˆ‡ğˆ2â„’â¢(ğœ»ğ’„))â¢,absentsubscriptğ‘…subscriptsuperscriptğˆ2ğ‘¡1ğ›½subscriptâˆ‡superscriptğˆ2â„’superscriptğœ»ğ’„,\displaystyle=R_{\bm{\sigma}^{-2}_{t+1}}\left(-\beta\nabla_{\bm{\sigma}^{2}}% \mathcal{L}\left({\bm{\zeta^{c}}}\right)\right)\textrm{,}= italic_R start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( - italic_Î² âˆ‡ start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) ) , |   
  
where âŠ™direct-product\odotâŠ™ denotes the element-wise product, and the
retraction is adapted to

| Rğˆtâˆ’2â¢(ğƒ)=ğˆtâˆ’2+ğƒ+12â¢ğƒâŠ™ğˆtâˆ’2âŠ™ğƒâ¢,subscriptğ‘…subscriptsuperscriptğˆ2ğ‘¡ğƒsubscriptsuperscriptğˆ2ğ‘¡ğƒdirect-product12ğƒsubscriptsuperscriptğˆ2ğ‘¡ğƒ,R_{\bm{\sigma}^{-2}_{t}}\left({\bm{\xi}}\right)=\bm{\sigma}^{-2}_{t}+{\bm{\xi}% }+\frac{1}{2}{\bm{\xi}}\odot\bm{\sigma}^{-2}_{t}\odot{\bm{\xi}}\text{,}italic_R start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¾ ) = bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_italic_Î¾ + divide start_ARG 1 end_ARG start_ARG 2 end_ARG bold_italic_Î¾ âŠ™ bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âŠ™ bold_italic_Î¾ , |   
---|---|---  
  
where ğƒğƒ{\bm{\xi}}bold_italic_Î¾ is a dâˆ’limit-fromğ‘‘d-italic_d
-dimensional vector. The corresponding MC estimators for the gradients are

| âˆ‡~ğâ¢â„’tâ¢(ğœ»ğ’Š)subscript~âˆ‡ğsubscriptâ„’ğ‘¡superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{\bm{\mu}}\mathcal{L}_{t}\left({\bm{\zeta^{i}}}% \right)\,over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆğˆ2âŠ™âˆ‡^ğâ¢â„’tâ¢(ğœ»ğ’„)absentdirect-productsuperscriptğˆ2subscript^âˆ‡ğsubscriptâ„’ğ‘¡superscriptğœ»ğ’„\displaystyle\approx\bm{\sigma}^{2}\odot\hat{\nabla}_{\bm{\mu}}\mathcal{L}_{t}% \left({\bm{\zeta^{c}}}\right)â‰ˆ bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT âŠ™ over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) |   
---|---|---|---  
|  | =cğt+1Sâ¢âˆ‘s=1S[(ğœ½sâˆ’ğt)â¢logâ¡pâ¢(ğ’š|ğœ½s)]â¢,absentsubscriptğ‘subscriptğğ‘¡1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptğœ½ğ‘ subscriptğğ‘¡ğ‘conditionalğ’šsubscriptğœ½ğ‘ ,\displaystyle=c_{{\bm{\mu}}_{t}}+\frac{1}{S}\sum_{s=1}^{S}\left[\left({\bm{% \theta}}_{s}-{\bm{\mu}}_{t}\right)\log p\left({\bm{y}}|{\bm{\theta}}_{s}\right% )\right]\text{,}= italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , |   
| âˆ‡~ğˆâˆ’2â¢â„’tâ¢(ğœ»ğ’Š)subscript~âˆ‡superscriptğˆ2subscriptâ„’ğ‘¡superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{\bm{\sigma}^{-2}}\mathcal{L}_{t}\left({\bm{\zeta^% {i}}}\right)\,over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆâˆ’âˆ‡^ğˆ2â¢â„’tâ¢(ğœ»ğ’„)absentsubscript^âˆ‡superscriptğˆ2subscriptâ„’ğ‘¡superscriptğœ»ğ’„\displaystyle\approx-\hat{\nabla}_{\bm{\sigma}^{2}}\mathcal{L}_{t}\left({\bm{% \zeta^{c}}}\right)â‰ˆ - over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) |   
|  | =cğˆt2+ğˆtâˆ’2âŠ™12â¢Sâ¢âˆ‘s=1S[(ğŸdâˆ’(ğœ½sâˆ’ğt)2âŠ™ğˆtâˆ’2)â¢logâ¡pâ¢(ğ’š|ğœ½s)]â¢,absentsubscriptğ‘subscriptsuperscriptğˆ2ğ‘¡direct-productsubscriptsuperscriptğˆ2ğ‘¡12ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscript1ğ‘‘direct-productsuperscriptsubscriptğœ½ğ‘ subscriptğğ‘¡2subscriptsuperscriptğˆ2ğ‘¡ğ‘conditionalğ’šsubscriptğœ½ğ‘ ,\displaystyle=c_{\bm{\sigma}^{2}_{t}}+\bm{\sigma}^{-2}_{t}\odot\frac{1}{2S}% \sum_{s=1}^{S}\left[\left(\bm{1}_{d}-\left({\bm{\theta}}_{s}-{\bm{\mu}}_{t}% \right)^{2}\odot\bm{\sigma}^{-2}_{t}\right)\log p\left({\bm{y}}|{\bm{\theta}}_% {s}\right)\right]\text{,}= italic_c start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT + bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âŠ™ divide start_ARG 1 end_ARG start_ARG 2 italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT - ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT âŠ™ bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] , |   
  
where
cğˆt2=âˆ’1/2â¢ğˆtâˆ’2+Ï„/2subscriptğ‘subscriptsuperscriptğˆ2ğ‘¡12subscriptsuperscriptğˆ2ğ‘¡ğœ2c_{\bm{\sigma}^{2}_{t}}=-1/2\bm{\sigma}^{-2}_{t}+\tau/2italic_c
start_POSTSUBSCRIPT bold_italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT
start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = - 1 / 2
bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT
start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Ï„ / 2,
cğt=Ï„â¢ğˆt2âŠ™(ğtâˆ’ğ0)subscriptğ‘subscriptğğ‘¡direct-
productğœsubscriptsuperscriptğˆ2ğ‘¡subscriptğğ‘¡subscriptğ0c_{{\bm{\mu}}_{t}}=\tau\bm{\sigma}^{2}_{t}\odot\left({\bm{\mu}}_{t}-{\bm{\mu}}%
_{0}\right)italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT
italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_Ï„ bold_italic_Ïƒ
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t
end_POSTSUBSCRIPT âŠ™ ( bold_italic_Î¼ start_POSTSUBSCRIPT italic_t
end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ),
ğœ½sâˆ¼ğ’©â¢(ğt,diagâ¢(ğˆt2))similar-tosubscriptğœ½ğ‘
ğ’©subscriptğğ‘¡diagsubscriptsuperscriptğˆ2ğ‘¡{\bm{\theta}}_{s}\sim\mathcal{N}\left({\bm{\mu}}_{t},\text{diag}\left(\bm{%
\sigma}^{2}_{t}\right)\right)bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
end_POSTSUBSCRIPT âˆ¼ caligraphic_N ( bold_italic_Î¼ start_POSTSUBSCRIPT
italic_t end_POSTSUBSCRIPT , diag ( bold_italic_Ïƒ start_POSTSUPERSCRIPT 2
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ),
s=1â¢â€¦,Sğ‘ 1â€¦ğ‘†s=1\dots,Sitalic_s = 1 â€¦ , italic_S,
(ğœ½sâˆ’ğt)2superscriptsubscriptğœ½ğ‘
subscriptğğ‘¡2\left({\bm{\theta}}_{s}-{\bm{\mu}}_{t}\right)^{2}(
bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼
start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2
end_POSTSUPERSCRIPT is intended element-wise, and
ğŸd=(1,â€¦,1)âŠ¤âˆˆâ„dsubscript1ğ‘‘superscript1â€¦1topsuperscriptâ„ğ‘‘\bm{1}_{d}=\left(1,\dots,1\right)^{\top}\in\mathbb{R}^{d}bold_1
start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = ( 1 , â€¦ , 1 )
start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT âˆˆ blackboard_R
start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. In the Gaussian case with
a general diagonal covariance matrix, retrieving
ğˆ2superscriptğˆ2\bm{\sigma}^{2}bold_italic_Ïƒ start_POSTSUPERSCRIPT 2
end_POSTSUPERSCRIPT from the updated
ğˆâˆ’2superscriptğˆ2\bm{\sigma}^{-2}bold_italic_Ïƒ start_POSTSUPERSCRIPT - 2
end_POSTSUPERSCRIPT is inexpensive as
Ïƒi2=1/Ïƒiâˆ’2subscriptsuperscriptğœ2ğ‘–1subscriptsuperscriptğœ2ğ‘–\sigma^{2}_{i}=1/\sigma^{-2}_{i}italic_Ïƒ
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT = 1 / italic_Ïƒ start_POSTSUPERSCRIPT - 2
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.

#### Isotropic prior and full Gaussian posterior

Because of the full form of the covariance matrix, this case is rather analogous to the general one. In particular, factors cğtsubscriptğ‘subscriptğğ‘¡c_{{\bm{\mu}}_{t}}italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT and cÎ£tsubscriptğ‘subscriptÎ£ğ‘¡c_{\Sigma_{t}}italic_c start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT in eq. (32) are replaced by (i) cÎ£t=âˆ’1/2â¢Î£tâˆ’1+Ï„/2subscriptğ‘subscriptÎ£ğ‘¡12subscriptsuperscriptÎ£1ğ‘¡ğœ2c_{\Sigma_{t}}=-1/2\Sigma^{-1}_{t}+\tau/2italic_c start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = - 1 / 2 roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Ï„ / 2, cğt=Ï„â¢Î£tâ¢(ğtâˆ’ğ0)subscriptğ‘subscriptğğ‘¡ğœsubscriptÎ£ğ‘¡subscriptğğ‘¡subscriptğ0c_{{\bm{\mu}}_{t}}=\tau\Sigma_{t}\left({\bm{\mu}}_{t}-{\bm{\mu}}_{0}\right)italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_Ï„ roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) or (ii) cÎ£t=0subscriptğ‘subscriptÎ£ğ‘¡0c_{\Sigma_{t}}=0italic_c start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 0, cğt=0subscriptğ‘subscriptğğ‘¡0c_{{\bm{\mu}}_{t}}=0italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 0, respectively, under the Gaussian-prior case (logâ¡fâ¢(ğœ½s)=logâ¡pâ¢(ğ’š|ğœ½s)ğ‘“subscriptğœ½ğ‘ ğ‘conditionalğ’šsubscriptğœ½ğ‘ \log f\left({\bm{\theta}}_{s}\right)=\log p\left({\bm{y}}|{\bm{\theta}}_{s}\right)roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )) and the general one (logâ¡fâ¢(ğœ½s)=hâ¢(ğœ½)ğ‘“subscriptğœ½ğ‘ â„ğœ½\log f\left({\bm{\theta}}_{s}\right)=h\left({\bm{\theta}}\right)roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = italic_h ( bold_italic_Î¸ )). The MC estimators (15) and (16) apply but are of reduced variance.

###  Mean-field variant

Assume that for a dğ‘‘ditalic_d-variate model, the Gaussian variational
posterior can be factorized as

| qğœ»ğ’„â¢(ğœ½)=qğœ»1câ¢(ğœ½1)â¢qğœ»2c,â€¦,qğœ»hcâ¢(ğœ½h)=âˆj=1hqğœ»jcâ¢(ğœ½j)â¢,formulae-sequencesubscriptğ‘superscriptğœ»ğ’„ğœ½subscriptğ‘subscriptsuperscriptğœ»ğ‘1subscriptğœ½1subscriptğ‘subscriptsuperscriptğœ»ğ‘2â€¦subscriptğ‘subscriptsuperscriptğœ»ğ‘â„subscriptğœ½â„superscriptsubscriptproductğ‘—1â„subscriptğ‘subscriptsuperscriptğœ»ğ‘ğ‘—subscriptğœ½ğ‘—,q_{{\bm{\zeta^{c}}}}\left({\bm{\theta}}\right)=q_{\bm{\zeta}^{c}_{1}}\left({% \bm{\theta}}_{1}\right)q_{\bm{\zeta}^{c}_{2}},\dots,q_{\bm{\zeta}^{c}_{h}}% \left({\bm{\theta}}_{h}\right)=\prod_{j=1}^{h}q_{\bm{\zeta}^{c}_{j}}\left({\bm% {\theta}}_{j}\right)\text{,}italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) = italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , â€¦ , italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) = âˆ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , |   
---|---|---  
  
with hâ‰¤dâ„ğ‘‘h\leq ditalic_h â‰¤ italic_d. If h=dâ„ğ‘‘h=ditalic_h =
italic_d, this corresponds to a full-diagonal case where each
ğœ½jsubscriptğœ½ğ‘—{\bm{\theta}}_{j}bold_italic_Î¸ start_POSTSUBSCRIPT
italic_j end_POSTSUBSCRIPT is a scalar and
ğœ»ğ’„j=(Î¼j,Ïƒj)subscriptsuperscriptğœ»ğ’„ğ‘—subscriptğœ‡ğ‘—subscriptğœğ‘—{\bm{\zeta^{c}}}_{j}=\left(\mu_{j},\sigma_{j}\right)bold_italic_Î¶
start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT
italic_j end_POSTSUBSCRIPT = ( italic_Î¼ start_POSTSUBSCRIPT italic_j
end_POSTSUBSCRIPT , italic_Ïƒ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT
). If h<dâ„ğ‘‘h<ditalic_h < italic_d, the variational covariance matrix
Î£Î£\Sigmaroman_Î£ of
qğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„q_{{\bm{\zeta^{c}}}}italic_q
start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT end_POSTSUBSCRIPT corresponds to a block-diagonal matrix,
and
ğœ»jc=(ğjâŠ¤,vecâ¢(Î£j)âŠ¤)âŠ¤subscriptsuperscriptğœ»ğ‘ğ‘—superscriptsuperscriptsubscriptğğ‘—topvecsuperscriptsubscriptÎ£ğ‘—toptop\bm{\zeta}^{c}_{j}=\left({\bm{\mu}}_{j}^{\top},\text{vec}\left(\Sigma_{j}%
\right)^{\top}\right)^{\top}bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = (
bold_italic_Î¼ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT
start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT , vec ( roman_Î£
start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT âŠ¤
end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT, with
ğjsubscriptğğ‘—{\bm{\mu}}_{j}bold_italic_Î¼ start_POSTSUBSCRIPT italic_j
end_POSTSUBSCRIPT, Î£jsubscriptÎ£ğ‘—\Sigma_{j}roman_Î£ start_POSTSUBSCRIPT
italic_j end_POSTSUBSCRIPT
(Î£jâˆ’1subscriptsuperscriptÎ£1ğ‘—\Sigma^{-1}_{j}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j
end_POSTSUBSCRIPT) respectively denoting the mean and covariance (precision)
matrix of the iğ‘–iitalic_i-th block of Î£Î£\Sigmaroman_Î£
(Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT). Also, a Gaussian priorâ€™s covariance matrix can be
diagonal, full, or block-diagonal with a structure matching or not that of
Î£Î£\Sigmaroman_Î£. Equations (17), (18), with the condition (32) can be used
as a starting point to derive case-specific MGVBP variants based on the form
of the prior covariance.

Algorithm 2 summarizes the case with an isotropic Gaussian prior of zero-mean
and precision matrix Ï„â¢Idğœsubscriptğ¼ğ‘‘\tau I_{d}italic_Ï„ italic_I
start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, using the gradient estimator
based on the log-likelihood. In this case, the block-wise natural gradients
are estimated as

| Î£â¢âˆ‡^ğjâ¢â„’â¢(ğœ»jc)Î£subscript^âˆ‡subscriptğğ‘—â„’subscriptsuperscriptğœ»ğ‘ğ‘—\displaystyle\Sigma\hat{\nabla}_{{\bm{\mu}}_{j}}\mathcal{L}\left(\bm{\zeta}^{c% }_{j}\right)roman_Î£ over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) | =âˆ’Ï„â¢Î£jâˆ’1â¢ğj+1Sâ¢âˆ‘s=1S[(ğœ½sjâˆ’ğj)â¢logâ¡pâ¢(ğ’š|ğœ½sj)]â¢,absentğœsubscriptsuperscriptÎ£1ğ‘—subscriptğğ‘—1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptğœ½subscriptğ‘ ğ‘—subscriptğğ‘—ğ‘conditionalğ’šsubscriptğœ½subscriptğ‘ ğ‘—,\displaystyle=-\tau\Sigma^{-1}_{j}{\bm{\mu}}_{j}+\frac{1}{S}\sum_{s=1}^{S}% \left[\left({\bm{\theta}}_{s_{j}}-{\bm{\mu}}_{j}\right)\log p\left({\bm{y}}|{% \bm{\theta}}_{s_{j}}\right)\right]\text{,}= - italic_Ï„ roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ] , |   
---|---|---|---  
| âˆ’âˆ‡^Î£jâ¢â„’â¢(ğœ»jc)subscript^âˆ‡subscriptÎ£ğ‘—â„’subscriptsuperscriptğœ»ğ‘ğ‘—\displaystyle-\hat{\nabla}_{\Sigma_{j}}\mathcal{L}\left(\bm{\zeta}^{c}_{j}\right)\- over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) | =âˆ’12â¢Î£jâˆ’1+Ï„2â¢I+12â¢Sâ¢âˆ‘s=1S[(Î£jâˆ’1âˆ’ğ‚jâ¢ğ‚jâŠ¤)â¢logâ¡pâ¢(ğ’š|ğœ½sj)]â¢,absent12subscriptsuperscriptÎ£1ğ‘—ğœ2ğ¼12ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptsuperscriptÎ£1ğ‘—subscriptğ‚ğ‘—superscriptsubscriptğ‚ğ‘—topğ‘conditionalğ’šsubscriptğœ½subscriptğ‘ ğ‘—,\displaystyle=-\frac{1}{2}\Sigma^{-1}_{j}+\frac{\tau}{2}I+\frac{1}{2S}\sum_{s=% 1}^{S}\left[\left(\Sigma^{-1}_{j}-{\bm{\nu}}_{j}{\bm{\nu}}_{j}^{\top}\right)% \log p\left({\bm{y}}|{\bm{\theta}}_{s_{j}}\right)\right]\text{,}= - divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + divide start_ARG italic_Ï„ end_ARG start_ARG 2 end_ARG italic_I + divide start_ARG 1 end_ARG start_ARG 2 italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - bold_italic_Î½ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_italic_Î½ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ) roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ] , |   
  
where Iğ¼Iitalic_I is the identity matrix of appropriate size for the block
jğ‘—jitalic_j,
ğ‚j=Î£jâˆ’1â¢(ğœ½sjâˆ’ğj)subscriptğ‚ğ‘—subscriptsuperscriptÎ£1ğ‘—subscriptğœ½subscriptğ‘
ğ‘—subscriptğğ‘—{\bm{\nu}}_{j}=\Sigma^{-1}_{j}\left({\bm{\theta}}_{s_{j}}-{\bm{\mu}}_{j}\right)bold_italic_Î½
start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j
end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT -
bold_italic_Î¼ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ), and
ğœ½sjâˆ¼qğœ»jcsimilar-tosubscriptğœ½subscriptğ‘
ğ‘—subscriptğ‘subscriptsuperscriptğœ»ğ‘ğ‘—{\bm{\theta}}_{s_{j}}\sim
q_{\bm{\zeta}^{c}_{j}}bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ¼ italic_q
start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT
end_POSTSUBSCRIPT, j=1,â€¦,hğ‘—1â€¦â„j=1,\dots,hitalic_j = 1 , â€¦ , italic_h.

###  Computational aspects

In terms of computational complexity, the exact MGVBP implementation is at no
additional cost. Actually, the cost of computing the natural gradient in MGVBP
as
âˆ’âˆ‡Î£â„’â¢(ğœ»ğ’„)subscriptâˆ‡Î£â„’superscriptğœ»ğ’„-\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\-
âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L (
bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) is
much cheaper than the one in MGVB,
Î£â¢âˆ‡Î£â„’â¢(ğœ»ğ’„)â¢Î£Î£subscriptâˆ‡Î£â„’superscriptğœ»ğ’„Î£\Sigma\nabla_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\Sigmaroman_Î£
âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L (
bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT )
roman_Î£, involving Oâ¢(d3)ğ‘‚superscriptğ‘‘3O\left(d^{3}\right)italic_O (
italic_d start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) operations for each
matrix multiplication. However, both MGVB and MGVBP share a cumbersome matrix
inversion.

Going back to (18) and (17), it is noticeable that under the most general
estimator based on the hâ„hitalic_h-function, Î£Î£\Sigmaroman_Î£ is not
involved in any computation, neither in the gradient involved in the update
for Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT nor in that for ğğ{\bm{\mu}}bold_italic_Î¼, suggesting
that implicitly the MGVBP optimization routine does not require the inversion
of Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT. The above point, however, ignores that the underlying
retraction masks the update for Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. For the retraction form in eq.
(22), both Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT and its inverse Î£Î£\Sigmaroman_Î£ are needed, thus
implying a matrix inversion at every iteration. That is, the covariance matrix
inversion is implicit in MGVB and MGVBP methods, which both require
Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT and Î£Î£\Sigmaroman_Î£ at every iteration (with little
surprise, as the form of the retraction is a second-order approximation of the
exponential map).

With the hâ„hitalic_h-function estimator, even though neither (15) nor (16)
involve Î£Î£\Sigmaroman_Î£, the inversion of
Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT is still necessary, as Î£Î£\Sigmaroman_Î£ is required in
retraction. Similarly, adopting the log-likelihood estimator under the
Gaussian regime in (32), is not computationally more expensive than the
hâ„hitalic_h-function case, as Î£Î£\Sigmaroman_Î£, involved in the computation
of cğsubscriptğ‘ğc_{{\bm{\mu}}}italic_c start_POSTSUBSCRIPT bold_italic_Î¼
end_POSTSUBSCRIPT, is, once again, required by retraction. In Appendix B.4, we
compare the running times of MGVBP and MGVB, showing that they are indeed
rather aligned, despite the computational simplicity of the MGVBP natural
gradient.

As outlined in Section 5, Î£Î£\Sigmaroman_Î£ can be conveniently recovered
from the Cholesky factor of Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, with fewer flips. Lastly, if
Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT (or Î£Î£\Sigmaroman_Î£) is diagonal, the inversion is
trivial and, when applicable, (17) is preferred.

##  Implementation aspects

###  Classification vs. regression

We point out that the MGVBP framework applies to both regression and
classification problems. In generic classification problems, predictions are
based on the class of maximum probability, obtained through a softmax function
returning the KÃ—1ğ¾1K\times 1italic_K Ã— 1 probability vector
ğ’‘isubscriptğ’‘ğ‘–\bm{p}_{i}bold_italic_p start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT, whose kğ‘˜kitalic_k-th entry interprets as the probability
that the label of the iğ‘–iitalic_i-th sample is in class kğ‘˜kitalic_k,
kâˆˆ{1,â€¦,K}ğ‘˜1â€¦ğ¾k\in\\{1,\dots,K\\}italic_k âˆˆ { 1 , â€¦ , italic_K }.
From these probabilities, it is straightforward to compute the model log-
likelihood as
âˆ‘i=1Nğ’šiâ¢logâ¡ğ’‘isuperscriptsubscriptğ‘–1ğ‘subscriptğ’šğ‘–subscriptğ’‘ğ‘–\sum_{i=1}^{N}{\bm{y}}_{i}\log\bm{p}_{i}âˆ‘
start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT
italic_N end_POSTSUPERSCRIPT bold_italic_y start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT roman_log bold_italic_p start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT, where ğ’šisubscriptğ’šğ‘–{\bm{y}}_{i}bold_italic_y
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes the one-hot-key encoded
true label for the iğ‘–iitalic_i-th input: a 1Ã—K1ğ¾1\times K1 Ã— italic_K
vector with 1 at the element corresponding to the true label and 0 elsewhere.

For regression, the parametric form of logâ¡pâ¢(ğ’š|ğœ½)ğ‘conditionalğ’šğœ½\log p\left({\bm{y}}|{\bm{\theta}}\right)roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) is clearly different and model-specific (e.g., regression with normal errors as opposed to Poisson regression, with the latter being feasible as the use of the score estimator does not require the likelihood to be differentiable). Note that, however, additional parameters may enter into play besides the ones involved in the back-bone forward model. For instance, for a regression with normal errors tackled with an artificial neural network, the Gaussian likelihood involves the regression variance, which is an additional parameter over the networkâ€™s ones, or the degree of freedom parameter Î½ğœˆ\nuitalic_Î½ (with the constraint Î½>2ğœˆ2\nu>2italic_Î½ > 2) for Student-t errors. See the application in Appendix B.3.

###  Variance reduction

As MGVBP does not involve model gradients, the use of the Reparametrization Trick (RT) (Blundell etÂ al., 2015) is not immediate. While (12) and (13) would generally hold, the form of the MGVBP gradient estimators under the RT would differ from (15) and (16). We develop MGVBP as a general and ready-to-use solution for VI that does not require model-specific derivations, yet one may certainly enable the RT within MGVBP. The use of the RT is quite popular in VI and ML as it empirically yields more accurate estimates of the gradient of the variational objective than alternative approaches (Mohamed etÂ al., 2020). However, note that, in general, the use of the RT estimator is not necessarily preferable, as its variance can be higher than that of the score-function estimator (Xu etÂ al., 2019; Mohamed etÂ al., 2020). Furthermore, the applicability of the adopted score estimator is broader, as it does not require logâ¡pâ¢(ğ’š|ğœ½)ğ‘conditionalğ’šğœ½\log p\left({\bm{y}}|{\bm{\theta}}\right)roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) to be differentiable.

Control Variates (CV) is a simple and effective approach for reducing the
variance of the MC gradient estimator. The CV estimator

| 1Sâ¢âˆ‘s=1Sâˆ‡ğœ»ğ’„[logâ¡qğœ»ğ’„â¢(ğœ½s)]â¡(logâ¡pâ¢(ğ’š|ğœ½s)âˆ’c)â¢,1ğ‘†superscriptsubscriptğ‘ 1ğ‘†subscriptâˆ‡superscriptğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„subscriptğœ½ğ‘ ğ‘conditionalğ’šsubscriptğœ½ğ‘ ğ‘,\frac{1}{S}\sum_{s=1}^{S}\nabla_{{\bm{\zeta^{c}}}}\left[\log q_{\bm{\zeta^{c}}% }\left({\bm{\theta}}_{s}\right)\right]\left(\log p\left({\bm{y}}|{\bm{\theta}}% _{s}\right)-c\right)\text{,}divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] ( roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) - italic_c ) , |   
---|---|---  
  
is unbiased for the expected gradient but of equal or smaller variance than
the naive MC one. The optimal cğ‘citalic_c minimizing the variance of the CV
estimator is (see, e.g., Paisley etÂ al., 2012; Ranganath etÂ al., 2014)

| câ‹†=Covâ¢(âˆ‡ğœ»ğ’„[logâ¡qğœ»ğ’„â¢(ğœ½)]â¡logâ¡pâ¢(ğ’š|ğœ½),âˆ‡ğœ»ğ’„logâ¡qğœ»ğ’„â¢(ğœ½))Varâ¢(âˆ‡ğœ»ğ’„logâ¡qğœ»ğ’„â¢(ğœ½))â¢.superscriptğ‘â‹†Covsubscriptâˆ‡superscriptğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„ğœ½ğ‘conditionalğ’šğœ½subscriptâˆ‡superscriptğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„ğœ½Varsubscriptâˆ‡superscriptğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„ğœ½.c^{\star}=\frac{\mathrm{Cov}\left(\nabla_{{\bm{\zeta^{c}}}}\left[\log q_{\bm{% \zeta^{c}}}\left({\bm{\theta}}\right)\right]\log p\left({\bm{y}}|{\bm{\theta}}% \right),\nabla_{{\bm{\zeta^{c}}}}\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}% \right)\right)}{\mathrm{Var}\left(\nabla_{{\bm{\zeta^{c}}}}\log q_{\bm{\zeta^{% c}}}\left({\bm{\theta}}\right)\right)}\text{.}italic_c start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT = divide start_ARG roman_Cov ( âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) , âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ) end_ARG start_ARG roman_Var ( âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ) end_ARG . |  | (26)  
---|---|---|---  
  
For a given Sğ‘†Sitalic_S, the CV estimator reduces the variance of the
gradient estimates or, conversely, reduces the number of MC samples required
for attaining a desired level of variance. In Table 8, we asses that for
logistic regression, values of Sğ‘†Sitalic_S as little as 10 appear
satisfactory. Yet, if the iterative computation of the log-likelihood is not
prohibitive, we suggest adopting a more generous value. (Magris etÂ al., 2022)
furthermore shows that the denominator in (26) is analytically tractable for a
Gaussian choice for qğ‘qitalic_q, reducing the variance of the estimated
câ‹†superscriptğ‘â‹†c^{\star}italic_c start_POSTSUPERSCRIPT â‹†
end_POSTSUPERSCRIPT and thus improving the overall efficiency of the CV
estimator.

###  Constraints on model parameters

MGVBP assumes a Gaussian variational posterior: the mean parameter is unbounded and defined over the entire real line. Assuming that a model parameter ğœ½ğœ½{\bm{\theta}}bold_italic_Î¸ is required to lie on a support ğ’®ğ’®\mathcal{S}caligraphic_S, to impose such a constraint, it suffices to identify a feasible transform T:â„â†’ğ’®:ğ‘‡â†’â„ğ’®T:\mathbb{R}\rightarrow\mathcal{S}italic_T : blackboard_R â†’ caligraphic_S and apply the MGVBP update to the unconstrained parameter ğ=Tâˆ’1â¢(ğœ½)ğsuperscriptğ‘‡1ğœ½\bm{\psi}=T^{-1}\left({\bm{\theta}}\right)bold_italic_Ïˆ = italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Î¸ ). By applying VI on ğğ\bm{\psi}bold_italic_Ïˆ, we require that the variational posterior assumption holds for ğğ\bm{\psi}bold_italic_Ïˆ rather than ğœ½ğœ½{\bm{\theta}}bold_italic_Î¸. The actual distribution for ğœ½ğœ½{\bm{\theta}}bold_italic_Î¸ under a Gaussian variational distribution for ğğ\bm{\psi}bold_italic_Ïˆ can be computed (or approximated with a sampling method) as ğ’©â¢(Tâˆ’1â¢(ğœ½);ğ,Î£)â¢|det(JTâˆ’1â¢(ğœ½))|ğ’©superscriptğ‘‡1ğœ½ğÎ£subscriptğ½superscriptğ‘‡1ğœ½\mathcal{N}\left(T^{-1}\left({\bm{\theta}}\right);{\bm{\mu}},\Sigma\right)|% \det\left(J_{T^{-1}}\left({\bm{\theta}}\right)\right)|caligraphic_N ( italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Î¸ ) ; bold_italic_Î¼ , roman_Î£ ) | roman_det ( italic_J start_POSTSUBSCRIPT italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ) |, with JTâˆ’1subscriptğ½superscriptğ‘‡1J_{T^{-1}}italic_J start_POSTSUBSCRIPT italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT the Jacobian of the inverse transform, and ğ’©ğ’©\mathcal{N}caligraphic_N denoting the multivariate normal probability density function (Kucukelbir etÂ al., 2015).

Example. For the GARCH(1,1) model, the intercept Ï‰ğœ”\omegaitalic_Ï‰, the
autoregressive coefficient of the lag-one squared return Î±ğ›¼\alphaitalic_Î±,
and the moving-average coefficient Î²ğ›½\betaitalic_Î² of the lag-one
conditional variances need to satisfy the stationarity condition
Î±+Î²<1ğ›¼ğ›½1\alpha+\beta<1italic_Î± + italic_Î² < 1 with
Ï‰>0ğœ”0\omega>0italic_Ï‰ > 0, Î±â‰¥0ğ›¼0\alpha\geq 0italic_Î± â‰¥ 0,
Î²â‰¥0ğ›½0\beta\geq 0italic_Î² â‰¥ 0. Such conditions are unfeasible under a
Gaussian variational approximation. As in (Magris and Iosifidis, 2023b), we
estimate the unconstrained parameters
ÏˆÏ‰,ÏˆÎ±,ÏˆÎ²subscriptğœ“ğœ”subscriptğœ“ğ›¼subscriptğœ“ğ›½\psi_{\omega},\psi_{\alpha},\psi_{\beta}italic_Ïˆ
start_POSTSUBSCRIPT italic_Ï‰ end_POSTSUBSCRIPT , italic_Ïˆ
start_POSTSUBSCRIPT italic_Î± end_POSTSUBSCRIPT , italic_Ïˆ
start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT, where
Ï‰=Tâ¢(ÏˆÏ‰),Î±=Tâ¢(ÏˆÎ±)â¢(1âˆ’Tâ¢(ÏˆÎ²)),Î²=Tâ¢(ÏˆÎ±)â¢Tâ¢(ÏˆÎ²)formulae-
sequenceğœ”ğ‘‡subscriptğœ“ğœ”formulae-
sequenceğ›¼ğ‘‡subscriptğœ“ğ›¼1ğ‘‡subscriptğœ“ğ›½ğ›½ğ‘‡subscriptğœ“ğ›¼ğ‘‡subscriptğœ“ğ›½\omega=T\left(\psi_{\omega}\right),\alpha=T\left(\psi_{\alpha}\right)\left(1-T%
\left(\psi_{\beta}\right)\right),\beta=T\left(\psi_{\alpha}\right)T\left(\psi_%
{\beta}\right)italic_Ï‰ = italic_T ( italic_Ïˆ start_POSTSUBSCRIPT italic_Ï‰
end_POSTSUBSCRIPT ) , italic_Î± = italic_T ( italic_Ïˆ start_POSTSUBSCRIPT
italic_Î± end_POSTSUBSCRIPT ) ( 1 - italic_T ( italic_Ïˆ start_POSTSUBSCRIPT
italic_Î² end_POSTSUBSCRIPT ) ) , italic_Î² = italic_T ( italic_Ïˆ
start_POSTSUBSCRIPT italic_Î± end_POSTSUBSCRIPT ) italic_T ( italic_Ïˆ
start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT ) with
Tâ¢(x)=expâ¡(x)/(1+expâ¡(x))ğ‘‡ğ‘¥ğ‘¥1ğ‘¥T\left(x\right)=\exp\left(x\right)/\left(1+\exp\left(x\right)\right)italic_T
( italic_x ) = roman_exp ( italic_x ) / ( 1 + roman_exp ( italic_x ) ) for
xğ‘¥xitalic_x real, on which Gaussian prior-posterior assumptions apply.

###  LB smoothing and stopping criterion

The stochastic nature of the lower bound estimator (21) introduces some noise
that can violate the expected non-decreasing behavior of the lower bound
across the iterations. By setting a window of size wğ‘¤witalic_w we compute
the moving average
â„’Â¯t=1/wâ¢âˆ‘i=1wâ„’^tâˆ’i+1â¢(ğœ»ğ’„)subscriptÂ¯â„’ğ‘¡1ğ‘¤superscriptsubscriptğ‘–1ğ‘¤subscript^â„’ğ‘¡ğ‘–1superscriptğœ»ğ’„\bar{\mathcal{L}}_{t}=1/w\sum_{i=1}^{w}\hat{\mathcal{L}}_{t-i+1}\left({\bm{%
\zeta^{c}}}\right)overÂ¯ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT
italic_t end_POSTSUBSCRIPT = 1 / italic_w âˆ‘ start_POSTSUBSCRIPT italic_i = 1
end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT over^
start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT italic_t - italic_i + 1
end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c
end_POSTSUPERSCRIPT ), whose variance is reduced, and behavior stabilized.

By keeping track of
â„’â‹†:=maxâ¡â„’Â¯tassignsuperscriptâ„’â‹†subscriptÂ¯â„’ğ‘¡\mathcal{L}^{\star}:=\max\bar{\mathcal{L}}_{t}caligraphic_L
start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT := roman_max overÂ¯ start_ARG
caligraphic_L end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT,
occurring at some iteration tâ‹†superscriptğ‘¡â‹†t^{\star}italic_t
start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT,
1â‰¤tâ‹†â‰¤t1superscriptğ‘¡â‹†ğ‘¡1\leq t^{\star}\leq t1 â‰¤ italic_t
start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â‰¤ italic_t, we terminate the
optimization after â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L
start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT did not improve for
Pğ‘ƒPitalic_P iterations (patience parameter), or after a maximum number of
iterations tmaxsubscriptğ‘¡maxt_{\text{max}}italic_t start_POSTSUBSCRIPT max
end_POSTSUBSCRIPT is reached. Therefore, the termination of the algorithm is
determined by an exit condition which depends on tğ‘¡titalic_t,
tmaxsubscriptğ‘¡maxt_{\text{max}}italic_t start_POSTSUBSCRIPT max
end_POSTSUBSCRIPT, the distance
tâˆ’tâ‹†ğ‘¡superscriptğ‘¡â‹†t-t^{\star}italic_t - italic_t
start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, the values of
â„’^tâˆ’w+1,â€¦,â„’^tsubscript^â„’ğ‘¡ğ‘¤1â€¦subscript^â„’ğ‘¡\hat{\mathcal{L}}_{t-w+1},\dots,\hat{\mathcal{L}}_{t}over^
start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT italic_t - italic_w + 1
end_POSTSUBSCRIPT , â€¦ , over^ start_ARG caligraphic_L end_ARG
start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and
â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT
â‹† end_POSTSUPERSCRIPT. Here tmax,Psubscriptğ‘¡maxğ‘ƒt_{\text{max}},Pitalic_t
start_POSTSUBSCRIPT max end_POSTSUBSCRIPT , italic_P and wğ‘¤witalic_w are
hyperparameters. In algorithms 1 and 2, this exit function is compactly
denoted by
fexitâ¢(t,â€¦)subscriptğ‘“exitğ‘¡â€¦f_{\text{exit}}\left(t,\dots\right)italic_f
start_POSTSUBSCRIPT exit end_POSTSUBSCRIPT ( italic_t , â€¦ ).

###  Gradient clipping

Especially for low values of Sğ‘†Sitalic_S, and even more, if a variance control method is not adopted, the stochastic gradient estimate may be poor, and the offset from its actual value may be large. This can result in updates whose magnitude is too big, either in a positive or negative direction. Especially at early iterations and with poor initial values, this issue may, e.g., cause complex roots in (23). At each iteration tğ‘¡titalic_t, to control for the magnitude of the stochastic gradient ğ’ˆ^tsubscript^ğ’ˆğ‘¡\hat{\bm{g}}_{t}over^ start_ARG bold_italic_g end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, we rescale its â„“2subscriptâ„“2\mathcal{\ell}_{2}roman_â„“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-norm â€–ğ’ˆ^tâ€–normsubscript^ğ’ˆğ‘¡||\hat{\bm{g}}_{t}||| | over^ start_ARG bold_italic_g end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | | whenever it is larger than a fixed threshold lmaxsubscriptğ‘™maxl_{\text{max}}italic_l start_POSTSUBSCRIPT max end_POSTSUBSCRIPT by replacing ğ’ˆ^tsubscript^ğ’ˆğ‘¡\hat{\bm{g}}_{t}over^ start_ARG bold_italic_g end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with ğ’ˆ^tâ¢lmax/â€–ğ’ˆ^tâ€–subscript^ğ’ˆğ‘¡subscriptğ‘™maxnormsubscript^ğ’ˆğ‘¡\hat{\bm{g}}_{t}l_{\text{max}}/||\hat{\bm{g}}_{t}||over^ start_ARG bold_italic_g end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT max end_POSTSUBSCRIPT / | | over^ start_ARG bold_italic_g end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | |, which bounds the norm of the rescaled gradient, while preserving its direction. Gradient clipping can be either applied to the gradients âˆ‡^ğâ¢â„’â¢(ğœ»ğ’„)subscript^âˆ‡ğâ„’superscriptğœ»ğ’„\hat{\nabla}_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ), âˆ‡^Î£â¢â„’â¢(ğœ»ğ’„)subscript^âˆ‡Î£â„’superscriptğœ»ğ’„\hat{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) or/and to the natural gradients Î£â¢âˆ‡^ğâ¢â„’â¢(ğœ»ğ’„)Î£subscript^âˆ‡ğâ„’superscriptğœ»ğ’„\Sigma\hat{\nabla}_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)roman_Î£ over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ), âˆ’âˆ‡^Î£â¢â„’â¢(ğœ»ğ’„)subscript^âˆ‡Î£â„’superscriptğœ»ğ’„-\hat{\nabla}_{\Sigma}\mathcal{L}\left({\bm{\zeta^{c}}}\right)\- over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ). We suggest applying gradient clipping on both gradients to promptly mitigate the impact that far-from-the-mean estimates may have on natural gradient computations and to control the norm of the product Î£â¢âˆ‡^ğâ¢â„’â¢(ğœ»ğ’„)Î£subscript^âˆ‡ğâ„’superscriptğœ»ğ’„\Sigma\hat{\nabla}_{\bm{\mu}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)roman_Î£ over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ).

###  Adaptive learning rate

Adopting an adaptive learning rate or scheduler for decreasing the learning
rate Î²ğ›½\betaitalic_Î² after a certain number of iterations is convenient.
Typical choices include multiplying Î²ğ›½\betaitalic_Î² by a certain factor
(e.g., 0.2) every set number of iterations (say, 100) or dynamically updating
it after a certain iteration tâ€²superscriptğ‘¡â€²t^{\prime}italic_t
start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. We adopt
Î²t=minâ¡(Î²,Î²â¢tâ€²t)subscriptğ›½ğ‘¡ğ›½ğ›½superscriptğ‘¡â€²ğ‘¡\beta_{t}=\min(\beta,\beta\frac{t^{\prime}}{t})italic_Î²
start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = roman_min ( italic_Î² ,
italic_Î² divide start_ARG italic_t start_POSTSUPERSCRIPT â€²
end_POSTSUPERSCRIPT end_ARG start_ARG italic_t end_ARG ), where
tâ€²superscriptğ‘¡â€²t^{\prime}italic_t start_POSTSUPERSCRIPT â€²
end_POSTSUPERSCRIPT is a fraction (e.g., 0.7) of the maximum number of
iterations tmaxsubscriptğ‘¡maxt_{\text{max}}italic_t start_POSTSUBSCRIPT max
end_POSTSUBSCRIPT.

##  Experiments

We validate and explore our suggested optimizerâ€™s empirical validity and
feasibility over five datasets and 14 models. These include logistic
regression (Labor dataset), different volatility models on the S&P 500 returns
(S&P 500 dataset), linear regression on different stock indexes (Istanbul
data), a neural network (Limit-Order Book (LOB) data), and a non-
differentiable model (Synthetic data). Details on the datasets and models
appear in Table 15. Our experiments deal with both classification and
regression tasks. For classification, we discuss binary-class prediction with
logistic regression and time-series multi-class classification with a neural
network. For regression, we propose a linear model and several GARCH-like
models for volatility modeling. Lastly, we exploit the advantage of the black-
box setting for regression within a non-differentiable model. Extended
results, with additional datasets and models, appear in Appendix B. The
relevant codes are available at github.com/mmagris/MGVBP. The main baseline
for model comparison is the MGVB optimizer and (sequential) MCMC estimates
representative of the true posterior. Additionally, we also include results
related to the QBVI optimizer (Magris etÂ al., 2022), Maximum Likelihood (ML),
and ADAM when applicable.

###  Classification tasks

The logistic regression experiments provide a sanity and qualitative check on
the feasibility, robustness, and learning process of MGVBP. From the convex
form of the likelihood, we expect to observe a tight alignment of the
parametersâ€™ estimates and performance metrics due to the similar optimum the
different algorithms attain for the relatively simple form of the LB under
this experimental setting. This is indeed the case for the results presented
later. In such a setting, we can grasp and largely comment on the qualitative
differences in the learning process and final results obtained with the
different optimizers.

![Refer to caption]() Figure 3: Logistic regression. Left: dynamics of the
lower bound across the iterations. Center and right: marginal posteriors for
the mode internet and coefficient of the first regressor for MGVBP, MGBV. A
kernel density from MCMC samples and the ML solutions are overlaid. ![Refer to
caption]() Figure 4: Logistic regression. Dynamics of the performance metrics
across the iterations on the train and test set for the MGVBP and MGVB
optimizers.

The upper panels in Figure 3 depict the LB optimization process across the
iterations for the logistic regression. MGVBPâ€™s LB shows a steeper growth
rate and improved convergence rate on the training set, irrespective of
whether the hâ„hitalic_h-function estimator is used. The central and left
panels of Figure 3 furthermore highlight an excellent accordance of the
learned MGVBP variational posterior with its MCMC counterpart, with means
aligned with the ML estimates. This underlines that the Gaussian variational
assumption is well-suited and that the moments of the variational marginal
match those derived from the MCMC sampler. In Figure 3, we plot the dynamics
of standard performance metrics for classification tasks. On both the training
and test set, at early iterations, MGVBP displays a very steep growth in model
accuracy, precision, recall, and f1-score compared to MGVB. Eventually, at
later iterations, MGVBP and MGVB performance metrics converge to a similar
level. Yet, as, e.g., depicted by the vertical lines in the left panel of
Figure 3, MGVBP leads to LB convergence approximately 200 iterations earlier
than MGVB, reflected in the values of the performance metrics. That is, the
training is completed in a much smaller number of iterations.

In fact, this is aligned with 1, showing that the LB reaches a similar maximum
value corresponding to rather analogous performance metrics. As a consequence,
the final variational solution MGVBP attains with respect to the existing
optimizers is similar. Table 2 reports the estimated posterior means, 3 the
variances, and Table 4 the estimated covariances.

Table 1: Performance metrics and value of the optimized lower bound
(â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT
â‹† end_POSTSUPERSCRIPT) for all the classification experiments on the Labour
dataset. hâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h
-func. end_FLOATSUPERSCRIPT denotes the use of the hâ„hitalic_h-function
gradient estimator, and diag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT
diag. end_FLOATSUPERSCRIPT the use of a diagonal variational posterior.

| Train | Test  
---|---|---  
| â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | Accuracy | Precision | Recall | f1 | Accuracy | Precision | Recall | f1  
MGVBP | âˆ’356.645-356.645-356.645\- 356.645 | 0.7130.7130.7130.713 | 0.7120.7120.7120.712 | 0.7030.7030.7030.703 | 0.7080.7080.7080.708 | 0.6980.6980.6980.698 | 0.6790.6790.6790.679 | 0.6740.6740.6740.674 | 0.6760.6760.6760.676  
MGVB | âˆ’356.646-356.646-356.646\- 356.646 | 0.7110.7110.7110.711 | 0.7100.7100.7100.710 | 0.7010.7010.7010.701 | 0.7060.7060.7060.706 | 0.6980.6980.6980.698 | 0.6790.6790.6790.679 | 0.6740.6740.6740.674 | 0.6760.6760.6760.676  
QBVI | âˆ’356.645-356.645-356.645\- 356.645 | 0.7130.7130.7130.713 | 0.7120.7120.7120.712 | 0.7030.7030.7030.703 | 0.7080.7080.7080.708 | 0.6980.6980.6980.698 | 0.6790.6790.6790.679 | 0.6740.6740.6740.674 | 0.6760.6760.6760.676  
MGVBPhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | âˆ’356.635-356.635-356.635\- 356.635 | 0.7110.7110.7110.711 | 0.7100.7100.7100.710 | 0.7010.7010.7010.701 | 0.7060.7060.7060.706 | 0.6980.6980.6980.698 | 0.6790.6790.6790.679 | 0.6740.6740.6740.674 | 0.6760.6760.6760.676  
MGVBhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | âˆ’356.635-356.635-356.635\- 356.635 | 0.7110.7110.7110.711 | 0.7100.7100.7100.710 | 0.7010.7010.7010.701 | 0.7060.7060.7060.706 | 0.6980.6980.6980.698 | 0.6790.6790.6790.679 | 0.6740.6740.6740.674 | 0.6760.6760.6760.676  
QBVIhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | âˆ’356.635-356.635-356.635\- 356.635 | 0.7110.7110.7110.711 | 0.7100.7100.7100.710 | 0.7010.7010.7010.701 | 0.7060.7060.7060.706 | 0.6980.6980.6980.698 | 0.6790.6790.6790.679 | 0.6740.6740.6740.674 | 0.6760.6760.6760.676  
MGVBPdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | âˆ’358.605-358.605-358.605\- 358.605 | 0.7130.7130.7130.713 | 0.7120.7120.7120.712 | 0.7030.7030.7030.703 | 0.7080.7080.7080.708 | 0.6720.6720.6720.672 | 0.6500.6500.6500.650 | 0.6470.6470.6470.647 | 0.6490.6490.6490.649  
MGVBdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | âˆ’358.681-358.681-358.681\- 358.681 | 0.7090.7090.7090.709 | 0.7080.7080.7080.708 | 0.7010.7010.7010.701 | 0.7040.7040.7040.704 | 0.6670.6670.6670.667 | 0.6450.6450.6450.645 | 0.6430.6430.6430.643 | 0.6440.6440.6440.644  
QBVIdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | âˆ’358.594-358.594-358.594\- 358.594 | 0.7130.7130.7130.713 | 0.7120.7120.7120.712 | 0.7030.7030.7030.703 | 0.7080.7080.7080.708 | 0.6720.6720.6720.672 | 0.6500.6500.6500.650 | 0.6470.6470.6470.647 | 0.6490.6490.6490.649  
MGVBPhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | âˆ’358.601-358.601-358.601\- 358.601 | 0.7130.7130.7130.713 | 0.7120.7120.7120.712 | 0.7030.7030.7030.703 | 0.7080.7080.7080.708 | 0.6670.6670.6670.667 | 0.6440.6440.6440.644 | 0.6400.6400.6400.640 | 0.6420.6420.6420.642  
MGVBhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | âˆ’358.680-358.680-358.680\- 358.680 | 0.7110.7110.7110.711 | 0.7100.7100.7100.710 | 0.7020.7020.7020.702 | 0.7060.7060.7060.706 | 0.6670.6670.6670.667 | 0.6440.6440.6440.644 | 0.6400.6400.6400.640 | 0.6420.6420.6420.642  
QBVIhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | âˆ’358.590-358.590-358.590\- 358.590 | 0.7130.7130.7130.713 | 0.7120.7120.7120.712 | 0.7030.7030.7030.703 | 0.7080.7080.7080.708 | 0.6670.6670.6670.667 | 0.6440.6440.6440.644 | 0.6400.6400.6400.640 | 0.6420.6420.6420.642  
  
Table 2: Posterior variational means for all the classification experiments on
the Labour dataset.

| Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT  
---|---|---|---|---|---|---|---|---  
MGVBP | 0.6790.6790.6790.679 | âˆ’1.490-1.490-1.490\- 1.490 | âˆ’0.083-0.083-0.083\- 0.083 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4940.4940.4940.494 | âˆ’0.640-0.640-0.640\- 0.640 | 0.6070.6070.6070.607 | 0.0480.0480.0480.048  
MGVB | 0.6790.6790.6790.679 | âˆ’1.489-1.489-1.489\- 1.489 | âˆ’0.083-0.083-0.083\- 0.083 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4930.4930.4930.493 | âˆ’0.639-0.639-0.639\- 0.639 | 0.6070.6070.6070.607 | 0.0480.0480.0480.048  
QBVI | 0.6790.6790.6790.679 | âˆ’1.489-1.489-1.489\- 1.489 | âˆ’0.083-0.083-0.083\- 0.083 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4930.4930.4930.493 | âˆ’0.639-0.639-0.639\- 0.639 | 0.6070.6070.6070.607 | 0.0480.0480.0480.048  
MGVBPhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | 0.6780.6780.6780.678 | âˆ’1.487-1.487-1.487\- 1.487 | âˆ’0.084-0.084-0.084\- 0.084 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4920.4920.4920.492 | âˆ’0.638-0.638-0.638\- 0.638 | 0.6090.6090.6090.609 | 0.0480.0480.0480.048  
MGVBhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | 0.6780.6780.6780.678 | âˆ’1.487-1.487-1.487\- 1.487 | âˆ’0.084-0.084-0.084\- 0.084 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4920.4920.4920.492 | âˆ’0.638-0.638-0.638\- 0.638 | 0.6090.6090.6090.609 | 0.0480.0480.0480.048  
QBVIhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | 0.6780.6780.6780.678 | âˆ’1.487-1.487-1.487\- 1.487 | âˆ’0.084-0.084-0.084\- 0.084 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4920.4920.4920.492 | âˆ’0.638-0.638-0.638\- 0.638 | 0.6090.6090.6090.609 | 0.0480.0480.0480.048  
MGVBPdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 0.5440.5440.5440.544 | âˆ’1.414-1.414-1.414\- 1.414 | âˆ’0.045-0.045-0.045\- 0.045 | âˆ’0.529-0.529-0.529\- 0.529 | 0.4940.4940.4940.494 | âˆ’0.629-0.629-0.629\- 0.629 | 0.5830.5830.5830.583 | 0.1190.1190.1190.119  
MGVBdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 0.5420.5420.5420.542 | âˆ’1.413-1.413-1.413\- 1.413 | âˆ’0.044-0.044-0.044\- 0.044 | âˆ’0.529-0.529-0.529\- 0.529 | 0.4930.4930.4930.493 | âˆ’0.629-0.629-0.629\- 0.629 | 0.5830.5830.5830.583 | 0.1210.1210.1210.121  
QBVIdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 0.5460.5460.5460.546 | âˆ’1.415-1.415-1.415\- 1.415 | âˆ’0.045-0.045-0.045\- 0.045 | âˆ’0.530-0.530-0.530\- 0.530 | 0.4940.4940.4940.494 | âˆ’0.629-0.629-0.629\- 0.629 | 0.5830.5830.5830.583 | 0.1190.1190.1190.119  
MGVBPhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | 0.5440.5440.5440.544 | âˆ’1.413-1.413-1.413\- 1.413 | âˆ’0.047-0.047-0.047\- 0.047 | âˆ’0.531-0.531-0.531\- 0.531 | 0.4920.4920.4920.492 | âˆ’0.628-0.628-0.628\- 0.628 | 0.5850.5850.5850.585 | 0.1210.1210.1210.121  
MGVBhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | 0.5410.5410.5410.541 | âˆ’1.411-1.411-1.411\- 1.411 | âˆ’0.046-0.046-0.046\- 0.046 | âˆ’0.530-0.530-0.530\- 0.530 | 0.4920.4920.4920.492 | âˆ’0.629-0.629-0.629\- 0.629 | 0.5860.5860.5860.586 | 0.1230.1230.1230.123  
QBVIhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | 0.5450.5450.5450.545 | âˆ’1.414-1.414-1.414\- 1.414 | âˆ’0.047-0.047-0.047\- 0.047 | âˆ’0.531-0.531-0.531\- 0.531 | 0.4920.4920.4920.492 | âˆ’0.628-0.628-0.628\- 0.628 | 0.5850.5850.5850.585 | 0.1210.1210.1210.121  
MCMC | 0.6790.6790.6790.679 | âˆ’1.487-1.487-1.487\- 1.487 | âˆ’0.084-0.084-0.084\- 0.084 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4930.4930.4930.493 | âˆ’0.638-0.638-0.638\- 0.638 | 0.6070.6070.6070.607 | 0.0470.0470.0470.047  
ML | 0.6790.6790.6790.679 | âˆ’1.476-1.476-1.476\- 1.476 | âˆ’0.085-0.085-0.085\- 0.085 | âˆ’0.571-0.571-0.571\- 0.571 | 0.4850.4850.4850.485 | âˆ’0.625-0.625-0.625\- 0.625 | 0.5990.5990.5990.599 | 0.0450.0450.0450.045  
  
Table 2 underlines the unbiasedness of the MGVBP posterior estimates of the
variational parameter with respect to the MCMC baseline and the ML target for
large samples where the prior is asymptotically negligible.

Similarly, the variational variances of the model parameters in Table 3 are
closely aligned between MGVBP, the other models, and the MCMC and the ML
counterparts. As well the covariance matrix also closely replicates the one
from the MCMC chain; see Table 4.

The above empirically validates the suitability of the Gaussian approximation,
the unbiasedness of the MGVBP posterior estimates of the variational
parameter, and its overall consistency in terms of the asymptotic ML
covariance matrix.

A diagonal constraint on the covariance harms the performance metrics as a
consequence of a different value of the optimized LB as for 1. This results
from the different overall levels of the estimated posterior means and
variances under this setup (see Table 2, and Table 3), and is expected
considering the considerably smaller overall number of free parameters.
However, the variational parameters and the corresponding performance metrics
are still aligned between the different optimizers. As an insight, we comment
that the diagonal assumption practically translates the variational Gaussian
as the posterior means indeed shift. Indeed, variational methods can be biased
with respect to the true posterior distribution (Carbonetto and Stephens,
2012). This is, after all, justified, thinking that the LB and its gradients,
see, e.g., eq. (7), depend on the interaction of variational elements
concerning both the variational mean and variational covariance elements.
I.e., the variational mean under the full case would, in general, differ from
that under the diagonal case.

Table 3: Variances of the variational approximation on the parameters for the
Labour dataset. Entries are multiplied by 102superscript10210^{2}10
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.

| Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT  
---|---|---|---|---|---|---|---|---  
MGVBP | 3.9563.9563.9563.956 | 5.4185.4185.4185.418 | 0.6040.6040.6040.604 | 1.5311.5311.5311.531 | 1.3761.3761.3761.376 | 2.1052.1052.1052.105 | 1.9641.9641.9641.964 | 4.3324.3324.3324.332  
MGVB | 4.2884.2884.2884.288 | 5.6445.6445.6445.644 | 0.6240.6240.6240.624 | 1.4961.4961.4961.496 | 1.3541.3541.3541.354 | 2.1912.1912.1912.191 | 2.0242.0242.0242.024 | 4.3144.3144.3144.314  
QBVI | 3.9833.9833.9833.983 | 5.4465.4465.4465.446 | 0.6080.6080.6080.608 | 1.5401.5401.5401.540 | 1.3841.3841.3841.384 | 2.1182.1182.1182.118 | 1.9771.9771.9771.977 | 4.3614.3614.3614.361  
MGVBPhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | 4.0714.0714.0714.071 | 5.3775.3775.3775.377 | 0.5960.5960.5960.596 | 1.4631.4631.4631.463 | 1.2841.2841.2841.284 | 2.0572.0572.0572.057 | 1.9721.9721.9721.972 | 4.3894.3894.3894.389  
MGVBhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | 4.0434.0434.0434.043 | 5.3845.3845.3845.384 | 0.5940.5940.5940.594 | 1.4571.4571.4571.457 | 1.2841.2841.2841.284 | 2.0702.0702.0702.070 | 1.9631.9631.9631.963 | 4.3724.3724.3724.372  
QBVIhâ¢-func.â„-func.{}^{h\text{-func.}}start_FLOATSUPERSCRIPT italic_h -func. end_FLOATSUPERSCRIPT | 4.0724.0724.0724.072 | 5.3785.3785.3785.378 | 0.5960.5960.5960.596 | 1.4631.4631.4631.463 | 1.2841.2841.2841.284 | 2.0572.0572.0572.057 | 1.9721.9721.9721.972 | 4.3894.3894.3894.389  
MGVBPdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 0.8150.8150.8150.815 | 3.3043.3043.3043.304 | 0.2550.2550.2550.255 | 0.9300.9300.9300.930 | 1.0861.0861.0861.086 | 1.0661.0661.0661.066 | 0.9150.9150.9150.915 | 1.3671.3671.3671.367  
MGVBdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 0.8080.8080.8080.808 | 3.2993.2993.2993.299 | 0.2550.2550.2550.255 | 0.8820.8820.8820.882 | 1.1071.1071.1071.107 | 1.0591.0591.0591.059 | 0.9130.9130.9130.913 | 1.3211.3211.3211.321  
QBVIdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 0.8170.8170.8170.817 | 3.3113.3113.3113.311 | 0.2550.2550.2550.255 | 0.9320.9320.9320.932 | 1.0881.0881.0881.088 | 1.0681.0681.0681.068 | 0.9170.9170.9170.917 | 1.3711.3711.3711.371  
MGVBPhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | 0.8520.8520.8520.852 | 3.2223.2223.2223.222 | 0.2470.2470.2470.247 | 0.9000.9000.9000.900 | 1.0051.0051.0051.005 | 1.0181.0181.0181.018 | 0.9350.9350.9350.935 | 1.3661.3661.3661.366  
MGVBhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | 0.8530.8530.8530.853 | 3.2283.2283.2283.228 | 0.2480.2480.2480.248 | 0.8780.8780.8780.878 | 1.0131.0131.0131.013 | 1.0121.0121.0121.012 | 0.9400.9400.9400.940 | 1.3481.3481.3481.348  
QBVIhâ¢-func. diagâ„-func. diag{}^{h\text{-func. diag}}start_FLOATSUPERSCRIPT italic_h -func. diag end_FLOATSUPERSCRIPT | 0.8530.8530.8530.853 | 3.2273.2273.2273.227 | 0.2480.2480.2480.248 | 0.9010.9010.9010.901 | 1.0061.0061.0061.006 | 1.0191.0191.0191.019 | 0.9360.9360.9360.936 | 1.3681.3681.3681.368  
MCMC | 3.9673.9673.9673.967 | 5.3645.3645.3645.364 | 0.5890.5890.5890.589 | 1.4061.4061.4061.406 | 1.2741.2741.2741.274 | 2.0712.0712.0712.071 | 1.9661.9661.9661.966 | 4.2484.2484.2484.248  
ML | 4.0794.0794.0794.079 | 5.4365.4365.4365.436 | 0.5890.5890.5890.589 | 1.4571.4571.4571.457 | 1.2761.2761.2761.276 | 2.0592.0592.0592.059 | 1.9601.9601.9601.960 | 4.3934.3934.3934.393  
  
Table 4: Variational covariance matrices, MCMC covariance matrix and ML
asymptotic covariance matrix. Entries are multiplied by
102superscript10210^{2}10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.

|  | MGVB  
---|---|---  
|  | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT  
MGVBP | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT |  | âˆ’1.710-1.710-1.710\- 1.710 | âˆ’0.877-0.877-0.877\- 0.877 | âˆ’0.674-0.674-0.674\- 0.674 | 0.1630.1630.1630.163 | 0.4780.4780.4780.478 | 0.0310.0310.0310.031 | âˆ’2.716-2.716-2.716\- 2.716  
Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | âˆ’1.458-1.458-1.458\- 1.458 |  | 0.2820.2820.2820.282 | 1.4651.4651.4651.465 | âˆ’0.561-0.561-0.561\- 0.561 | âˆ’0.192-0.192-0.192\- 0.192 | 0.2290.2290.2290.229 | âˆ’0.002-0.002-0.002\- 0.002  
Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | âˆ’0.790-0.790-0.790\- 0.790 | 0.2180.2180.2180.218 |  | 0.3850.3850.3850.385 | 0.0890.0890.0890.089 | 0.0350.0350.0350.035 | âˆ’0.024-0.024-0.024\- 0.024 | âˆ’0.090-0.090-0.090\- 0.090  
Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | âˆ’0.712-0.712-0.712\- 0.712 | 1.4061.4061.4061.406 | 0.4060.4060.4060.406 |  | âˆ’0.014-0.014-0.014\- 0.014 | 0.1230.1230.1230.123 | âˆ’0.063-0.063-0.063\- 0.063 | âˆ’0.414-0.414-0.414\- 0.414  
Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | 0.1350.1350.1350.135 | âˆ’0.395-0.395-0.395\- 0.395 | 0.1000.1000.1000.100 | 0.0460.0460.0460.046 |  | âˆ’0.104-0.104-0.104\- 0.104 | âˆ’0.280-0.280-0.280\- 0.280 | âˆ’0.131-0.131-0.131\- 0.131  
Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | 0.2280.2280.2280.228 | âˆ’0.136-0.136-0.136\- 0.136 | 0.0310.0310.0310.031 | 0.0820.0820.0820.082 | âˆ’0.111-0.111-0.111\- 0.111 |  | âˆ’1.366-1.366-1.366\- 1.366 | âˆ’0.743-0.743-0.743\- 0.743  
Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | 0.2350.2350.2350.235 | 0.1770.1770.1770.177 | âˆ’0.057-0.057-0.057\- 0.057 | âˆ’0.176-0.176-0.176\- 0.176 | âˆ’0.313-0.313-0.313\- 0.313 | âˆ’1.305-1.305-1.305\- 1.305 |  | âˆ’0.039-0.039-0.039\- 0.039  
Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | âˆ’2.584-2.584-2.584\- 2.584 | âˆ’0.090-0.090-0.090\- 0.090 | âˆ’0.124-0.124-0.124\- 0.124 | âˆ’0.389-0.389-0.389\- 0.389 | âˆ’0.199-0.199-0.199\- 0.199 | âˆ’0.561-0.561-0.561\- 0.561 | âˆ’0.122-0.122-0.122\- 0.122 |   
  
|  | ML  
---|---|---  
|  | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT  
MCMC | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT |  | âˆ’1.578-1.578-1.578\- 1.578 | âˆ’0.800-0.800-0.800\- 0.800 | âˆ’0.684-0.684-0.684\- 0.684 | 0.0670.0670.0670.067 | 0.3180.3180.3180.318 | 0.1940.1940.1940.194 | âˆ’2.701-2.701-2.701\- 2.701  
Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | âˆ’1.510-1.510-1.510\- 1.510 |  | 0.2740.2740.2740.274 | 1.3781.3781.3781.378 | âˆ’0.422-0.422-0.422\- 0.422 | âˆ’0.027-0.027-0.027\- 0.027 | 0.0740.0740.0740.074 | âˆ’0.084-0.084-0.084\- 0.084  
Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | âˆ’0.791-0.791-0.791\- 0.791 | 0.2490.2490.2490.249 |  | 0.3930.3930.3930.393 | 0.1020.1020.1020.102 | 0.0240.0240.0240.024 | âˆ’0.058-0.058-0.058\- 0.058 | âˆ’0.093-0.093-0.093\- 0.093  
Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | âˆ’0.654-0.654-0.654\- 0.654 | 1.3071.3071.3071.307 | 0.3810.3810.3810.381 |  | 0.0670.0670.0670.067 | 0.1170.1170.1170.117 | âˆ’0.164-0.164-0.164\- 0.164 | âˆ’0.352-0.352-0.352\- 0.352  
Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | 0.0470.0470.0470.047 | âˆ’0.414-0.414-0.414\- 0.414 | 0.1110.1110.1110.111 | 0.0720.0720.0720.072 |  | âˆ’0.172-0.172-0.172\- 0.172 | âˆ’0.265-0.265-0.265\- 0.265 | âˆ’0.109-0.109-0.109\- 0.109  
Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | 0.3340.3340.3340.334 | âˆ’0.049-0.049-0.049\- 0.049 | 0.0060.0060.0060.006 | 0.0890.0890.0890.089 | âˆ’0.177-0.177-0.177\- 0.177 |  | âˆ’1.286-1.286-1.286\- 1.286 | âˆ’0.613-0.613-0.613\- 0.613  
Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | 0.1720.1720.1720.172 | 0.1110.1110.1110.111 | âˆ’0.036-0.036-0.036\- 0.036 | âˆ’0.137-0.137-0.137\- 0.137 | âˆ’0.266-0.266-0.266\- 0.266 | âˆ’1.282-1.282-1.282\- 1.282 |  | âˆ’0.097-0.097-0.097\- 0.097  
Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | âˆ’2.603-2.603-2.603\- 2.603 | âˆ’0.085-0.085-0.085\- 0.085 | âˆ’0.097-0.097-0.097\- 0.097 | âˆ’0.349-0.349-0.349\- 0.349 | âˆ’0.119-0.119-0.119\- 0.119 | âˆ’0.590-0.590-0.590\- 0.590 | âˆ’0.102-0.102-0.102\- 0.102 |   
  
Furthermore, Figure 5 shows that the variational mean and covariance learning
dynamics are smooth and steady, without wigglings or anomalies. At the same
time, using the hâ„hitalic_h-function estimator stabilizes the learning
process, but has a minor effect on the maximized LB (Tables 1, 2, 3). Indeed,
the adoption of the gradient estimator based on the hâ„hitalic_h-function has
an overall minimal impact on the optimized results. In fact, the hâ„hitalic_h-
function alternative impacts only the variance of the gradient estimator
without affecting the expected direction of the steps in the SGD update.

Table 8 in Appendix B, addressed the impact the number of MC samples
Sğ‘†Sitalic_S has on the posterior mean, performance measures, and the
optimized LB. is minor for both the training and test phases (Table 8). We
conclude that about 20 samples are sufficient (also for complex models such as
the following neural network) and are a feasible compromise between the
quality of the MC approximation of the LB gradient and computational
efficiency (also in the following experiments). Our experiments are, however,
oriented toward discussing the improvements and advantages of MGVBP over MGVB.
To this end, we require a precise estimation of the LB and thus employ a much
higher value of Sğ‘†Sitalic_S, see Table 15.

![Refer to caption]() Figure 5: Parameter learning across the iterations for
the MGVBP algorithm on the Labour dataset for some selected variational
parameters. Dotted lines correspond to the diagonal case, dashed lines to the
use of the hâ„hitalic_h-function gradient estimator.

We additionally train a (bilinear) neural network with a TABL layer (Tran etÂ
al., 2019). The bottom panel of Table 5 reports the estimation results on the
TABL neural network, referred to as the LOB experiment. The prediction of
high-frequency mid-price movements in limit-order book markets is known to be
a challenging task, where Bayesian techniques are particularly relevant for
implementing risk-aware trading strategies (Magris etÂ al., 2023), and where
the TABL architecture is very effective. Training MGVB on such a machine
learning model empirically appears unstable and generally unfeasible, both
with our own and the original implementation of Tran etÂ al. (2021a). Indeed
we are unaware of any MGVB application for typical machine learning models. We
adopt different baselines. Because of the inability of QBVI to grant the
positive definite constraint, we train it under a diagonal assumption.
Furthermore, we include the state-of-the-art VOGN optimizer (Osawa etÂ al.,
2019). VOGN also assumes a diagonal Gaussian posterior and relies on natural
gradient computations employing modelsâ€™ (per-sample) gradients and an
approximate Hessian. Its second-order nature closely resembles the standard
ADAM optimizer, included as a non-Bayesian baseline, included as a non-
Bayesian baseline.

The problem is highly non-convex: indeed, we observe the algorithms converging
at different optima and heterogeneous performance measures. Yet, MGVBP clearly
outperforms the other Bayesian alternatives on both the training and test set,
promoting its use beyond standard statistical models. Concerning the MCMC
estimates, we observe ubiquitous evidence of multi-modality, skewness, and
asymmetry in the margins with an underlying non-Gaussian copula, that any
fixed-form Gaussian VI can not, by construction, deal with. However, the peak
performance obtained with the Gaussian Assumption under MGVBP suggests a main
mode where most of the density is concentrated, captured by the VI
approximation. The evolution of the learning process for the LB and
performance metrics in Figure 6 remarks a very stable behavior of the
stochastic LB estimate (Ns=20subscriptğ‘ğ‘ 20N_{s}=20italic_N
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 20 only), along with a
consistent and resolved improvement of the performance measures since early
epochs. The performance under MGVBP is also remarkably improved on the test
set.

Table 5: Performance measures for the LOB experiment.

| Train | Test  
---|---|---  
| â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | Accuracy | Precision | Recall | f1 | Accuracy | Precision | Recall | f1  
MGVBP | â€„â€„-91948.593 | 0.738 | 0.804 | 0.610 | 0.651 | 0.795 | 0.798 | 0.614 | 0.665  
QBVI (diag.) | -104261.698 | 0.717 | 0.697 | 0.637 | 0.658 | 0.686 | 0.650 | 0.605 | 0.603  
VOGN (diag.) | -100791.406 | 0.673 | 0.631 | 0.629 | 0.630 | 0.758 | 0.677 | 0.637 | 0.652  
MCMC |  | 0.689 | 0.773 | 0.536 | 0.567 | 0.761 | 0.779 | 0.536 | 0.585  
ADAM (non Bayesian) |  |  |  |  |  | 0.755 | 0.668 | 0.640 | 0.652  
  
![Refer to caption]() Figure 6: Lower bound (right axis) and performance
measure cross iterations (left axis) for the LOB experiment on the train set
(continuous lines) test set (dotted lines) for MGVBP.

###  Regression tasks

Table 6: Regression task. Parametersâ€™ estimates and performance measures.

|  |  |  |  | Train | Test  
---|---|---|---|---|---|---  
| Ï‰ğœ”\omegaitalic_Ï‰ | Î±ğ›¼\alphaitalic_Î± | Î³ğ›¾\gammaitalic_Î³ | Î²ğ›½\betaitalic_Î² | â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | pâ¢(y|Î¼â‹†)ğ‘conditionalğ‘¦superscriptğœ‡â‹†p\left(y|\mu^{\star}\right)italic_p ( italic_y | italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSErv | QLIKErv | MSErv | QLIKErv  
GARCH(1,1) |  |  |  |  |  |  |  |  |  |   
MGVBP | 0.043 | 0.230 |  | 0.737 | -2027.489 | -2017.633 | 5.124 | 0.531 | 2.564 | 0.566  
MGVB | 0.043 | 0.229 |  | 0.737 | -2027.493 | -2017.626 | 5.113 | 0.531 | 2.561 | 0.566  
QBVI | 0.043 | 0.229 |  | 0.737 | -2027.493 | -2017.626 | 5.114 | 0.531 | 2.561 | 0.566  
MCMC | 0.042 | 0.231 |  | 0.738 |  | -2017.713 | 5.189 | 0.531 | 2.565 | 0.566  
ML | 0.042 | 0.226 |  | 0.739 |  | -2017.593 | 5.073 | 0.531 | 2.531 | 0.566  
GJR(1,1) |  |  |  |  |  |  |  |  |  |   
MGVBP | 0.043 | 0.108 | 0.294 | 0.722 | -2003.542 | -1990.907 | 7.039 | 0.310 | 1.556 | 0.346  
MGVB | 0.044 | 0.108 | 0.292 | 0.722 | -2003.550 | -1990.896 | 6.969 | 0.310 | 1.551 | 0.347  
QBVI | 0.044 | 0.108 | 0.292 | 0.722 | -2003.550 | -1990.897 | 6.971 | 0.310 | 1.551 | 0.347  
MCMC | 0.043 | 0.108 | 0.298 | 0.724 |  | -1991.095 | 7.318 | 0.309 | 1.576 | 0.344  
ML | 0.042 | 0.108 | 0.291 | 0.723 |  | -1990.853 | 6.940 | 0.311 | 1.548 | 0.349  
HAR | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT |  |  |  102Ã—10^{2}\times10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã—QLIKErv |  |  |  102Ã—10^{2}\times10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã—QLIKErv  
MGVBP | 1.032 | 0.489 | 0.420 | -0.009 | -5080.191 | -5058.113 | 24.193 | 5.324 | 18.861 | 5.712  
MGVB | 1.019 | 0.485 | 0.426 | -0.011 | -5082.123 | -5058.167 | 24.194 | 5.328 | 18.879 | 5.720  
QBVIdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 1.036 | 0.485 | 0.423 | -0.007 | -5083.253 | -5058.143 | 24.193 | 5.326 | 18.862 | 5.710  
MCMC | 1.075 | 0.488 | 0.421 | -0.012 |  | -5058.090 | 24.192 | 5.328 | 18.858 | 5.706  
ML | 1.080 | 0.488 | 0.421 | -0.012 |  | -5058.086 | 24.192 | 5.328 | 18.857 | 5.706  
  
The GARCH-related models in the lower panels of Table 6 address the predictive
ability of MGVBP under non-standard and non-convex likelihood functions. As
performance measures, we adopt the Mean Squared Error (MSE) and the QLIKE loss
(Patton, 2011) computed with respect to the 5-minute sub-sampled realized
volatility (Zhang etÂ al., 2005) as a robust proxy for daily compared to
squared daily returns. Additional information and extended results appear in
Appendix B.2, also involving performance measures computed with respect to the
squared daily returns.

As for the logistic regression experiments, we observe that all the algorithms
approach the very same ML optimum, indicating that both MGVBP and MGVB move
towards the same LB maximum. Furthermore, all variational approximations are
well-aligned with the MCMC and ML estimates. Thus, the estimates, performance
metrics, and value of the optimized LB are similar across the optimizers: they
all converge to the minimum but in a qualitatively different way. Indeed, in
7, the LB improvement across the iterations is steeper for MGVBP and also for
a diagonal implementation. With respect to the dynamics of the performance
metrics computed on the test depicted on the second and third columns of the
plots in Figure 7, we observed that MGVBP dominates, both in terms of MSE and
QLIKE the MGVB baseline for the HAR model. On the other hand, for the GARCH
family models, whereas MGVBP clearly outperforms MGVB in terms of the QLIKE
loss both on the training and test set, this dominance is not so evident for
the MSE. In fact, there is no direct link between the dynamics of the LB and
that of the performance measures, i.e., there are no trivial guarantees that,
e.g., as the LB is maximized, the MSE is minimized. In fact, while this is the
case for the HAR model, for the GARCH modes, we observe an increase of the MSE
across the iterations, despite the fact that the LB approaches the optimum.
The same hold for the test MSE: for the GARCH(1,1) it decreases across the
iterations, for the GJR(1,1) not. This should not be surprising as the
optimization objective is the overall minimization of the KL divergence
between the true posterior and the variational one, achieved via LB
maximization, and not the minimization of the MSE, Q-link, or other
performance metrics. Figure 7 validates this interpretation and reminds us
that VI has to be interpreted for what it is: a solution for approximating the
posterior, not a performance-metric minimizing tool, as, e.g., ordinary least
squares (w.r.t. the squared loss) or ML (w.r.t. model negative loglikelihood).
Consequently, the reference measure for an actual comparison of the VI
algorithms should be the value of the optimized lower bound and its dynamics.
The additional results in Appendix B.2 confirm the improved ability of MGVBP
in optimizing the LB and the overall alignment between the solution obtained
with MGVB, MCMC, and ML on further GARCH-type models. See tables 9,10,11, and
the Figures 8,9,10, comparing the posterior marginals obtained via VI and
MCMC.

![Refer to caption]()![Refer to caption]()![Refer to caption]()

Figure 7: Progression of the lower bound optimization, dynamics of the MSE and
QLIKE, both computed with respect to RV5ss, across the iterations for the HAR
model (top row), GARCH(1,1) model (middle row) and GJR(1,1) (bottom row).
Vertical lines denote the iteration tâ‹†superscriptğ‘¡â‹†t^{\star}italic_t
start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, for which the line style and
color match the one of the corresponding optimizer. For the GARCH and GJR, see
Appendix A for the corresponding plots of the parametersâ€™ posterior margins.

###  Non-differentiable model

Table 7 reports the estimation results for a non-differentiable model, where,
e.g., the reparametrization trick and methods generally relying on gradients
and automatic differentiation are inapplicable. We adopt the reconstruction
benchmark problem in (Lyu and Tsang, 2021):

| yisubscriptğ‘¦ğ‘–\displaystyle y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | =fâ¢(ğ’™i)+Îµiâ¢,Îµiâˆ¼ğ’©â¢(0,Ïƒ2)â¢,formulae-sequenceabsentğ‘“subscriptğ’™ğ‘–subscriptğœ€ğ‘–,similar-tosubscriptğœ€ğ‘–ğ’©0superscriptğœ2,\displaystyle=f\left(\bm{x}_{i}\right)+\varepsilon_{i}\text{,}\qquad% \varepsilon_{i}\sim\mathcal{N}\left(0,\sigma^{2}\right)\text{,}= italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_Îµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_Îµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ¼ caligraphic_N ( 0 , italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , |   
---|---|---|---  
| fâ¢(ğ’™i)ğ‘“subscriptğ’™ğ‘–\displaystyle f\left(\bm{x}_{i}\right)italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | =â€–signâ¢(ğ’™iâˆ’0.5)âˆ’Î²â€–22âˆ’â€–signâ¢(Î²)âˆ’Î²â€–22â¢,ğ’™iâˆˆ{0,1}dâ¢.formulae-sequenceabsentsuperscriptsubscriptnormsignsubscriptğ’™ğ‘–0.5ğ›½22superscriptsubscriptnormsignğ›½ğ›½22,subscriptğ’™ğ‘–superscript01ğ‘‘.\displaystyle=||\text{sign}\left(\bm{x}_{i}-0.5\right)-\beta||_{2}^{2}-||\text% {sign}\left(\beta\right)-\beta||_{2}^{2}\text{,}\qquad\bm{x}_{i}\in\left\\{0,1% \right\\}^{d}\text{.}= | | sign ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - 0.5 ) - italic_Î² | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - | | sign ( italic_Î² ) - italic_Î² | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ { 0 , 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . |   
  
We simulate i=1,â€¦,5000ğ‘–1â€¦5000i=1,\dots,5000italic_i = 1 , â€¦ , 5000
observations for the above model with d=20ğ‘‘20d=20italic_d = 20,
Î²âˆ¼Uniformâ¢(0,1)similar-
toğ›½Uniform01\beta\sim\text{Uniform}\left(0,1\right)italic_Î² âˆ¼ Uniform ( 0
, 1 ), p=0.5ğ‘0.5p=0.5italic_p = 0.5,
Ïƒ2=1superscriptğœ21\sigma^{2}=1italic_Ïƒ start_POSTSUPERSCRIPT 2
end_POSTSUPERSCRIPT = 1, and xğ‘¥xitalic_x sampled from dğ‘‘ditalic_d
independent Bernoulli distributions. MGVBP and MGVB are both implemented over
the hâ„hitalic_h-function estimator and for a full specification of the
variational covariance matrix. For reference, the initial value of the lower
bound for all the experiments is âˆ’11591.33911591.339-11591.339\- 11591.339,
and we include the mean absolute deviation (MAD). Details on the
hyperparameters are provided in Table 15.

Table 7: Results for the non-differentiable model.

| Train | Test  
---|---|---  
| â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p(y|\bm{\mu}^{\star})italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSE | MAD | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p(y|\bm{\mu}^{\star})italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSE | MAD  
MGVB | -2901.67 | -2848.05 | 1.020 | 0.800 | -707.451 | 1.033 | 0.798  
MGVBP | -2898.86 | -2845.41 | 1.018 | 0.798 | -705.945 | 1.027 | 0.794  
QBVIdiag. | -2904.48 | -2846.67 | 1.019 | 0.798 | -706.78 | 1.03 | 0.796  
  
From Table 7, we confirm the overall accordance of the three black box
approaches considered in this work and the improved performance of MGVBP also
for the above non-differentiable model.

##  Conclusion

We propose an algorithm based on manifold optimization to guarantee the
positive-definite constraint on the covariance matrix of the Gaussian
variational posterior. Extending the baseline method of (Tran etÂ al., 2021a),
we exploit the computational advantage of the Gaussian parametrization in
terms of the precision matrix and employ simple and computationally convenient
analytical solutions for the natural gradients. Furthermore, we provide a
theoretically consistent setup justifying the use of the SPD-manifold
retraction form. Our MGVBP black-box optimizer results in a ready-to-use
solution for VI, scalable to structured covariance matrices, that can take
advantage of alternative forms of the stochastic gradient estimator, control
variates, and momentum. We show our solutionâ€™s feasibility on many
statistical, econometric, and ML models over different baseline optimizers.
Our results align with sampling methods and highlight the advantages of the
suggested approach over state-of-the-art baselines in terms of convergence and
performance metrics. Future research may investigate the applicability of our
approach to a broader set of variational distributions, and the use of the
reparametrization trick in place of black-box gradients.

## Acknowledgments

This project has received funding from the European Unionâ€™s Horizon 2020
research and innovation programme under the Marie SkÅ‚odowska-Curie grant
agreement No. 890690, and the Independent Research Fund Denmark project DISPA
(project No. 9041-00004)

## References

  * Absil etÂ al. (2008) Absil, P.-A., Mahony, R., and Sepulchre, R. (2008).  Optimization algorithms on matrix manifolds.  Princeton University Press. 
  * Akbilgic etÂ al. (2014) Akbilgic, O., Bozdogan, H., and Balaban, M.Â E. (2014).  A novel hybrid rbf neural networks model as a forecaster.  Statistics and Computing, 24(3):365â€“375. 
  * Barfoot (2020) Barfoot, T.Â D. (2020).  Multivariate gaussian variational inference by natural gradient descent.  arXiv preprint arXiv:2001.10025. 
  * Bhatia etÂ al. (2019) Bhatia, R., Jain, T., and Lim, Y. (2019).  On the buresâ€“wasserstein distance between positive definite matrices.  Expositiones Mathematicae, 37(2):165â€“191. 
  * Blei etÂ al. (2017) Blei, D.Â M., Kucukelbir, A., and McAuliffe, J.Â D. (2017).  Variational inference: A review for statisticians.  Journal of the American statistical Association, 112(518):859â€“877. 
  * Blundell etÂ al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015).  Weight uncertainty in neural network.  In International Conference on Machine Learning, pages 1613â€“1622. 
  * Boumal etÂ al. (2014) Boumal, N., Mishra, B., Absil, P.-A., and Sepulchre, R. (2014).  Manopt, a Matlab toolbox for optimization on manifolds.  Journal of Machine Learning Research, 15(42):1455â€“1459. 
  * Carbonetto and Stephens (2012) Carbonetto, P. and Stephens, M. (2012).  Scalable Variational Inference for Bayesian Variable Selection in Regression, and Its Accuracy in Genetic Association Studies.  Bayesian Analysis, 7(1):73 â€“ 108. 
  * Corsi (2009) Corsi, F. (2009).  A simple approximate long-memory model of realized volatility.  Journal of Financial Econometrics, 7(2):174â€“196. 
  * Ganguly and Earp (2021) Ganguly, A. and Earp, S. W.Â F. (2021).  An introduction to variational inference.  arXiv preprint arXiv:2108.13083. 
  * Graves (2011) Graves, A. (2011).  Practical variational inference for neural networks.  Advances in neural information processing systems, 24. 
  * Han etÂ al. (2021) Han, A., Mishra, B., Jawanpuria, P.Â K., and Gao, J. (2021).  On riemannian optimization over positive definite matrices with the bures-wasserstein geometry.  Advances in Neural Information Processing Systems, 34:8940â€“8953. 
  * Hoffman etÂ al. (2013) Hoffman, M.Â D., Blei, D.Â M., Wang, C., and Paisley, J. (2013).  Stochastic variational inference.  Journal of Machine Learning Research. 
  * Hosseini and Sra (2015) Hosseini, R. and Sra, S. (2015).  Matrix manifold optimization for gaussian mixtures.  Advances in Neural Information Processing Systems, 28. 
  * Jeuris etÂ al. (2012) Jeuris, B., Vandebril, R., and Vandereycken, B. (2012).  A survey and comparison of contemporary algorithms for computing the matrix geometric mean.  Electronic Transactions on Numerical Analysis, 39:379â€“402. 
  * Khan and Lin (2017) Khan, M. and Lin, W. (2017).  Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models.  In Artificial Intelligence and Statistics, pages 878â€“887. Proceedings of Machine Learning Research. 
  * Khan etÂ al. (2018) Khan, M., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y., and Srivastava, A. (2018).  Fast and scalable bayesian deep learning by weight-perturbation in adam.  In International Conference on Machine Learning, pages 2611â€“2620. 
  * Khan and Nielsen (2018) Khan, M.Â E. and Nielsen, D. (2018).  Fast yet simple natural-gradient descent for variational inference in complex models.  In 2018 International Symposium on Information Theory and Its Applications, pages 31â€“35. 
  * Kingma and Ba (2014) Kingma, D.Â P. and Ba, J. (2014).  Adam: A method for stochastic optimization.  arXiv preprint arXiv:1412.6980. 
  * Kingma and Welling (2013) Kingma, D.Â P. and Welling, M. (2013).  Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114. 
  * Kucukelbir etÂ al. (2015) Kucukelbir, A., Ranganath, R., Gelman, A., and Blei, D. (2015).  Automatic variational inference in stan.  Advances in neural information processing systems, 28. 
  * Lampinen and Vehtari (2001) Lampinen, J. and Vehtari, A. (2001).  Bayesian approach for neural networksâ€”review and case studies.  Neural networks, 14(3):257â€“274. 
  * Lin etÂ al. (2021) Lin, W., Nielsen, F., Khan, M.Â E., and Schmidt, M. (2021).  Structured second-order methods via natural gradient descent.  arXiv preprint arXiv:2107.10884. 
  * Lin etÂ al. (2020) Lin, W., Schmidt, M., and Khan, M.Â E. (2020).  Handling the positive-definite constraint in the bayesian learning rule.  In International Conference on Machine Learning, pages 6116â€“6126. PMLR. 
  * Lyu and Tsang (2021) Lyu, Y. and Tsang, I.Â W. (2021).  Black-box optimizer with stochastic implicit natural gradient.  In Machine Learning and Knowledge Discovery in Databases. Research Track, pages 217â€“232. Springer International Publishing. 
  * Mackay (1992) Mackay, D. J.Â C. (1992).  Bayesian methods for adaptive models.  PhD thesis, California Institute of Technology. 
  * Mackay (1995) Mackay, D. J.Â C. (1995).  Probable networks and plausible predictions â€” a review of practical bayesian methods for supervised neural networks.  Network: Computation In Neural Systems, 6:469â€“505. 
  * Magris and Iosifidis (2023a) Magris, M. and Iosifidis, A. (2023a).  Bayesian learning for neural networks: an algorithmic survey.  Artificial Intelligence Review, pages 1â€“51. 
  * Magris and Iosifidis (2023b) Magris, M. and Iosifidis, A. (2023b).  Variational inference for garch-family models.  In Proceedings of the Fourth ACM International Conference on AI in Finance, ICAIFâ€™23, page 541â€“548. Association for Computing Machinery. 
  * Magris etÂ al. (2022) Magris, M., Shabani, M., and Iosifidis, A. (2022).  Quasi black-box variational inference with natural gradients for bayesian learning.  arXiv preprint arXiv:2205.11568. 
  * Magris etÂ al. (2023) Magris, M., Shabani, M., and Iosifidis, A. (2023).  Bayesian bilinear neural network for predicting the mid-price dynamics in limit-order book markets.  Journal of Forecasting, 42(6):1407â€“1428. 
  * Mardia and Marshall (1984) Mardia, K.Â V. and Marshall, R.Â J. (1984).  Maximum likelihood estimation of models for residual covariance in spatial regression.  Biometrika, 71(1):135â€“146. 
  * Mohamed etÂ al. (2020) Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A. (2020).  Monte carlo gradient estimation in machine learning.  Journal of Machine Learning Research, 21(132):1â€“62. 
  * Mroz (1984) Mroz, T.Â A. (1984).  The sensitivity of an empirical model of married womenâ€™s hours of work to economic and statistical assumptions.  PhD thesis, Stanford University. 
  * Ntakaris etÂ al. (2018) Ntakaris, A., Magris, M., Kanniainen, J., Gabbouj, M., and Iosifidis, A. (2018).  Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods.  Journal of Forecasting, 37(8):852â€“866. 
  * Osawa etÂ al. (2019) Osawa, K., Swaroop, S., Khan, M.Â E., Jain, A., Eschenhagen, R., Turner, R.Â E., and Yokota, R. (2019).  Practical deep learning with bayesian principles.  In Advances in Neural Information Processing Systems, volumeÂ 32, pages 1â€“13. 
  * Paisley etÂ al. (2012) Paisley, J., Blei, D.Â M., and Jordan, M.Â I. (2012).  Variational bayesian inference with stochastic search.  arXiv preprint arXiv:1206.6430. 
  * Patton (2011) Patton, A.Â J. (2011).  Volatility forecast comparison using imperfect volatility proxies.  Journal of Econometrics, 160(1):246â€“256. 
  * Pennec (2020) Pennec, X. (2020).  Manifold-valued image processing with spd matrices.  In Riemannian geometric statistics in medical image analysis, pages 75â€“134. Elsevier. 
  * Ranganath etÂ al. (2014) Ranganath, R., Gerrish, S., and Blei, D.Â M. (2014).  Black box variational inference.  In Artificial intelligence and statistics, pages 814â€“822. 
  * Robbins and Monro (1951) Robbins, H. and Monro, S. (1951).  A stochastic approximation method.  The annals of mathematical statistics, pages 400â€“407. 
  * Salimans etÂ al. (2015) Salimans, T., Kingma, D., and Welling, M. (2015).  Markov chain monte carlo and variational inference: Bridging the gap.  In International conference on machine learning, pages 1218â€“1226. PMLR. 
  * Salimans and Knowles (2014) Salimans, T. and Knowles, D.Â A. (2014).  On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression.  arXiv preprint arXiv:1401.1022. 
  * Saul etÂ al. (1996) Saul, L.Â K., Jaakkola, T., and Jordan, M.Â I. (1996).  Mean field theory for sigmoid belief networks.  Journal of artificial intelligence research, 4:61â€“76. 
  * Tan (2021) Tan, L.Â S. (2021).  Natural gradient updates for cholesky factor in gaussian and structured variational inference.  arXiv preprint arXiv:2109.00375. 
  * TerÃ¤svirta (2009) TerÃ¤svirta, T. (2009).  An introduction to univariate garch models.  In Handbook of financial time series, pages 17â€“42. Springer. 
  * Tran etÂ al. (2019) Tran, D.Â T., Iosifidis, A., Kanniainen, J., and Gabbouj, M. (2019).  Temporal attention-augmented bilinear network for financial time-series data analysis.  IEEE Transactions on Neural Networks and Learning Systems, 30(5):1407â€“1418. 
  * Tran etÂ al. (2021a) Tran, M.-N., Nguyen, D.Â H., and Nguyen, D. (2021a).  Variational Bayes on manifolds.  Statistics and Computing, 31(6):71. 
  * Tran etÂ al. (2021b) Tran, M.-N., Nguyen, T.-N., and Dao, V.-H. (2021b).  A practical tutorial on variational bayes.  arXiv preprint arXiv:2103.01327. 
  * Trusheim etÂ al. (2018) Trusheim, F., Condurache, A., and Mertins, A. (2018).  Boosting black-box variational inference by incorporating the natural gradient.  In 2018 24th International Conference on Pattern Recognition (ICPR), pages 19â€“24. IEEE. 
  * Wainwright and Jordan (2008) Wainwright, M.Â J. and Jordan, M.Â I. (2008).  Graphical models, exponential families, and variational inference.  Foundations and TrendsÂ® in Machine Learning, 1(1â€“2):1â€“305. 
  * Wierstra etÂ al. (2014) Wierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., and Schmidhuber, J. (2014).  Natural evolution strategies.  The Journal of Machine Learning Research, 15(1):949â€“980. 
  * Xu etÂ al. (2019) Xu, M., Quiroz, M., Kohn, R., and Sisson, S.Â A. (2019).  Variance reduction properties of the reparameterization trick.  In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2711â€“2720. PMLR. 
  * Zhang etÂ al. (2018) Zhang, G., Sun, S., Duvenaud, D., and Grosse, R. (2018).  Noisy natural gradient as variational inference.  In International Conference on Machine Learning, pages 5852â€“5861. PMLR. 
  * Zhang etÂ al. (2005) Zhang, L., Mykland, P.Â A., and AÃ¯t-Sahalia, Y. (2005).  A tale of two time scales: Determining integrated volatility with noisy high-frequency data.  Journal of the American Statistical Association, 100(472):1394â€“1411. 

##  Appendix A Block-diagonal implementation

Algorithm 2 MGVBP for a block-diagonal covariance matrix (prior with zero-mean
and covariance matrix Ï„âˆ’1â¢Isuperscriptğœ1ğ¼\tau^{-1}Iitalic_Ï„
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_I)

1:Â Â Set hyper-parameters: 0<Î²,Ï‰<1formulae-
sequence0ğ›½ğœ”10<\beta,\omega<10 < italic_Î² , italic_Ï‰ < 1, Sğ‘†Sitalic_S

2:Â Â Set the type of gradient estimator, i.e. function
logâ¡fâ¢(ğœ½s)ğ‘“subscriptğœ½ğ‘ \log f\left({\bm{\theta}}_{s}\right)roman_log
italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )

3:Â Â Set prior pâ¢(ğœ½;ğŸ,Ï„â¢I)ğ‘ğœ½0ğœğ¼p\left({\bm{\theta}};\bm{0},\tau I\right)italic_p ( bold_italic_Î¸ ; bold_0 , italic_Ï„ italic_I ), likelihood pâ¢(ğ’š|ğœ½)ğ‘conditionalğ’šğœ½p\left({\bm{y}}|{\bm{\theta}}\right)italic_p ( bold_italic_y | bold_italic_Î¸ ), and initial values ğğ{\bm{\mu}}bold_italic_Î¼, Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT

4:Â Â t=1ğ‘¡1t=1italic_t = 1,
Stop=falseStopfalse\text{Stop}=\texttt{false}Stop = false

5:Â Â Generate:  ğœ½s=[ğœ½s1,â€¦,ğœ½sh]subscriptğœ½ğ‘ subscriptğœ½subscriptğ‘
1â€¦subscriptğœ½subscriptğ‘
â„{\bm{\theta}}_{s}=\left[{\bm{\theta}}_{s_{1}},\dots,{\bm{\theta}}_{s_{h}}\right]bold_italic_Î¸
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = [ bold_italic_Î¸
start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT
end_POSTSUBSCRIPT , â€¦ , bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT ],
â€ƒğœ½siâˆ¼qği,Î£isimilar-tosubscriptğœ½subscriptğ‘
ğ‘–subscriptğ‘subscriptğğ‘–subscriptÎ£ğ‘–{\bm{\theta}}_{s_{i}}\sim
q_{{\bm{\mu}}_{i},\Sigma_{i}}bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ¼ italic_q
start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT , roman_Î£ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT, s=1,â€¦,Sğ‘ 1â€¦ğ‘†s=1,\dots,Sitalic_s = 1 , â€¦ ,
italic_S, i=1,â€¦,hğ‘–1â€¦â„i=1,\dots,hitalic_i = 1 , â€¦ , italic_h

6:Â Â forÂ i=1,â€¦,hğ‘–1â€¦â„i=1,\dots,hitalic_i = 1 , â€¦ , italic_hÂ do

7:Â Â Â Â Â Compute:
g^ği=Î£iâ¢âˆ‡^ğiâ¢â„’subscript^ğ‘”subscriptğğ‘–subscriptÎ£ğ‘–subscript^âˆ‡subscriptğğ‘–â„’\hat{g}_{{\bm{\mu}}_{i}}=\Sigma_{i}\hat{\nabla}_{{\bm{\mu}}_{i}}\mathcal{L}over^
start_ARG italic_g end_ARG start_POSTSUBSCRIPT bold_italic_Î¼
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = roman_Î£
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG âˆ‡ end_ARG
start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT end_POSTSUBSCRIPT
caligraphic_L,â€ƒg^Î£iâˆ’1=âˆ’âˆ‡^Î£iâ¢â„’subscript^ğ‘”subscriptsuperscriptÎ£1ğ‘–subscript^âˆ‡subscriptÎ£ğ‘–â„’\hat{g}_{\Sigma^{-1}_{i}}=-\hat{\nabla}_{\Sigma_{i}}\mathcal{L}over^
start_ARG italic_g end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT
- 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT = - over^ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L

8:Â Â Â Â Â
ğ’ği=g^ğisubscriptğ’subscriptğğ‘–subscript^ğ‘”subscriptğğ‘–\bm{m}_{{\bm{\mu}}_{i}}=\hat{g}_{{\bm{\mu}}_{i}}bold_italic_m
start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT end_POSTSUBSCRIPT = over^ start_ARG italic_g end_ARG
start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT
end_POSTSUBSCRIPT,â€ƒğ’Î£iâˆ’1=g^Î£iâˆ’1subscriptğ’subscriptsuperscriptÎ£1ğ‘–subscript^ğ‘”subscriptsuperscriptÎ£1ğ‘–\bm{m}_{\Sigma^{-1}_{i}}=\hat{g}_{\Sigma^{-1}_{i}}bold_italic_m
start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = over^
start_ARG italic_g end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT
- 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT

9:Â Â endÂ for

10:Â Â whileÂ Stop=falseStopfalse\text{Stop}=\texttt{false}Stop = falseÂ do

11:Â Â Â Â Â forÂ i=1,â€¦,hğ‘–1â€¦â„i=1,\dots,hitalic_i = 1 , â€¦ , italic_hÂ
do

12:Â Â Â Â Â Â Â Â
ği=ği+Î²â¢ğ’ğisubscriptğğ‘–subscriptğğ‘–ğ›½subscriptğ’subscriptğğ‘–{\bm{\mu}}_{i}={\bm{\mu}}_{i}+\beta\bm{m}_{{\bm{\mu}}_{i}}bold_italic_Î¼
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_Î¼
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_Î² bold_italic_m
start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT end_POSTSUBSCRIPT

13:Â Â Â Â Â Â Â Â
Î£old,iâˆ’1=Î£iâˆ’1subscriptsuperscriptÎ£1oldğ‘–subscriptsuperscriptÎ£1ğ‘–\Sigma^{-1}_{\text{old},i}=\Sigma^{-1}_{i}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT old ,
italic_i end_POSTSUBSCRIPT = roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT,â€ƒÎ£iâˆ’1=RÎ£old,iâˆ’1â¢(Î²â¢ğ’Î£iâˆ’1)subscriptsuperscriptÎ£1ğ‘–subscriptğ‘…subscriptsuperscriptÎ£1oldğ‘–ğ›½subscriptğ’subscriptsuperscriptÎ£1ğ‘–\Sigma^{-1}_{i}=R_{\Sigma^{-1}_{\text{old},i}}(\beta\bm{m}_{\Sigma^{-1}_{i}})roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT = italic_R start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT old ,
italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Î² bold_italic_m
start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT )

14:Â Â Â Â Â endÂ for

15:Â Â Â Â Â Generate:  ğœ½s=[ğœ½s1,â€¦,ğœ½sh]subscriptğœ½ğ‘
subscriptğœ½subscriptğ‘ 1â€¦subscriptğœ½subscriptğ‘
â„{\bm{\theta}}_{s}=\left[{\bm{\theta}}_{s_{1}},\dots,{\bm{\theta}}_{s_{h}}\right]bold_italic_Î¸
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = [ bold_italic_Î¸
start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT
end_POSTSUBSCRIPT , â€¦ , bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT ],
â€ƒğœ½siâˆ¼qği,Î£isimilar-tosubscriptğœ½subscriptğ‘
ğ‘–subscriptğ‘subscriptğğ‘–subscriptÎ£ğ‘–{\bm{\theta}}_{s_{i}}\sim
q_{{\bm{\mu}}_{i},\Sigma_{i}}bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ¼ italic_q
start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT , roman_Î£ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT, s=1â¢â€¦â¢Sğ‘ 1â€¦ğ‘†s=1\dots Sitalic_s = 1 â€¦ italic_S,
i=1,â€¦,hğ‘–1â€¦â„i=1,\dots,hitalic_i = 1 , â€¦ , italic_h

16:Â Â Â Â Â Set:  logâ¡qs=0subscriptğ‘ğ‘ 0\log q_{s}=0roman_log italic_q
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 0, s=1,â€¦,Sğ‘
1â€¦ğ‘†s=1,\dots,Sitalic_s = 1 , â€¦ , italic_S

17:Â Â Â Â Â forÂ i=1,â€¦,hğ‘–1â€¦â„i=1,\dots,hitalic_i = 1 , â€¦ , italic_hÂ
do

18:Â Â Â Â Â Â Â Â Compute:
g^ğisubscript^ğ‘”subscriptğğ‘–\hat{g}_{{\bm{\mu}}_{i}}over^ start_ARG italic_g
end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT end_POSTSUBSCRIPT,
g^Î£iâˆ’1subscript^ğ‘”subscriptsuperscriptÎ£1ğ‘–\hat{g}_{\Sigma^{-1}_{i}}over^
start_ARG italic_g end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT
- 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT

19:Â Â Â Â Â Â Â Â
ğ’ği=Ï‰â¢ğ’ği+(1âˆ’Ï‰)â¢g^ğisubscriptğ’subscriptğğ‘–ğœ”subscriptğ’subscriptğğ‘–1ğœ”subscript^ğ‘”subscriptğğ‘–\bm{m}_{{\bm{\mu}}_{i}}=\omega\bm{m}_{{\bm{\mu}}_{i}}+\left(1-\omega\right)%
\hat{g}_{{\bm{\mu}}_{i}}bold_italic_m start_POSTSUBSCRIPT bold_italic_Î¼
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_Ï‰
bold_italic_m start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( 1 - italic_Ï‰ ) over^ start_ARG
italic_g end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT
italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT

20:Â Â Â Â Â Â Â Â
ğ’Î£iâˆ’1=ğ’¯Î£old,iâˆ’1â†’Î£iâˆ’1â¢(ğ’Î£iâˆ’1)+(1âˆ’Ï‰)â¢g^Î£iâˆ’1subscriptğ’subscriptsuperscriptÎ£1ğ‘–subscriptğ’¯â†’subscriptsuperscriptÎ£1oldğ‘–subscriptsuperscriptÎ£1ğ‘–subscriptğ’subscriptsuperscriptÎ£1ğ‘–1ğœ”subscript^ğ‘”subscriptsuperscriptÎ£1ğ‘–\bm{m}_{\Sigma^{-1}_{i}}=\mathcal{T}_{\Sigma^{-1}_{\text{old},i}\rightarrow%
\Sigma^{-1}_{i}}(\bm{m}_{\Sigma^{-1}_{i}})+\left(1-\omega\right)\hat{g}_{%
\Sigma^{-1}_{i}}bold_italic_m start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT end_POSTSUBSCRIPT = caligraphic_T start_POSTSUBSCRIPT
roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT old
, italic_i end_POSTSUBSCRIPT â†’ roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT ( bold_italic_m start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) + ( 1 - italic_Ï‰ ) over^ start_ARG
italic_g end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT

21:Â Â Â Â Â Â Â Â logâ¡qs=logâ¡qs+logâ¡qği,Î£iâ¢(ğœ½si)subscriptğ‘ğ‘
subscriptğ‘ğ‘ subscriptğ‘subscriptğğ‘–subscriptÎ£ğ‘–subscriptğœ½subscriptğ‘
ğ‘–\log q_{s}=\log q_{s}+\log
q_{{\bm{\mu}}_{i},\Sigma_{i}}\left({\bm{\theta}}_{s% _{i}}\right)roman_log
italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = roman_log italic_q
start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT + roman_log italic_q
start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_i
end_POSTSUBSCRIPT , roman_Î£ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s
start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT )

22:Â Â Â Â Â endÂ for

23:Â Â Â Â Â â„’^=1Sâ¢âˆ‘s=1S[logâ¡pâ¢(ğœ½s)+logâ¡pâ¢(ğ’š|ğœ½s)+logâ¡qs]^â„’1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]ğ‘subscriptğœ½ğ‘ ğ‘conditionalğ’šsubscriptğœ½ğ‘ subscriptğ‘ğ‘ \hat{\mathcal{L}}=\frac{1}{S}\sum_{s=1}^{S}\left[\log p\left({\bm{\theta}}_{s}% \right)+\log p\left({\bm{y}}|{\bm{\theta}}_{s}\right)+\log q_{s}\right]over^ start_ARG caligraphic_L end_ARG = divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ roman_log italic_p ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) + roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) + roman_log italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ]

24:Â Â Â Â Â t=t+1ğ‘¡ğ‘¡1t=t+1italic_t = italic_t + 1,
Stop=fexitâ¢(t,â€¦)Stopsubscriptğ‘“exitğ‘¡â€¦\text{Stop}=f_{\text{exit}}\left(t,\dots\right)Stop
= italic_f start_POSTSUBSCRIPT exit end_POSTSUBSCRIPT ( italic_t , â€¦ )

25:Â Â endÂ while

##  Appendix B Experiments

###  Additional results for the labour data

Table 8: Estimated parameters and performance measures on the Labour dataset
for MGVBP (full-posterior) for different sizes of the number of MC draws for
the estimation of the stochastic gradients Sğ‘†Sitalic_S. tğ‘¡titalic_t refers
to the run-time per iteration (in milliseconds),
â„’â¢(ğœ½0)â„’subscriptğœ½0\mathcal{L}({\bm{\theta}}_{0})caligraphic_L (
bold_italic_Î¸ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) to the LB evaluated
at the initial parameters. For each Sğ‘†Sitalic_S, a common random seed is
used.

Sğ‘†Sitalic_S | tğ‘¡titalic_t | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | â„’â¢(ğœ»0c)â„’subscriptsuperscriptğœ»ğ‘0\mathcal{L}\left(\bm{\zeta}^{c}_{0}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  
---|---|---|---|---|---|---|---|---|---|---  
10 | 0.0020.0020.0020.002 | 0.6740.6740.6740.674 | âˆ’1.478-1.478-1.478\- 1.478 | âˆ’0.080-0.080-0.080\- 0.080 | âˆ’0.569-0.569-0.569\- 0.569 | 0.5000.5000.5000.500 | âˆ’0.634-0.634-0.634\- 0.634 | 0.5930.5930.5930.593 | 0.0410.0410.0410.041 | âˆ’430.216-430.216-430.216\- 430.216  
20 | 0.0020.0020.0020.002 | 0.6800.6800.6800.680 | âˆ’1.490-1.490-1.490\- 1.490 | âˆ’0.083-0.083-0.083\- 0.083 | âˆ’0.573-0.573-0.573\- 0.573 | 0.4910.4910.4910.491 | âˆ’0.637-0.637-0.637\- 0.637 | 0.6020.6020.6020.602 | 0.0440.0440.0440.044 | âˆ’429.626-429.626-429.626\- 429.626  
30 | 0.0030.0030.0030.003 | 0.6800.6800.6800.680 | âˆ’1.491-1.491-1.491\- 1.491 | âˆ’0.083-0.083-0.083\- 0.083 | âˆ’0.573-0.573-0.573\- 0.573 | 0.4920.4920.4920.492 | âˆ’0.635-0.635-0.635\- 0.635 | 0.6020.6020.6020.602 | 0.0450.0450.0450.045 | âˆ’434.863-434.863-434.863\- 434.863  
50 | 0.0040.0040.0040.004 | 0.6790.6790.6790.679 | âˆ’1.489-1.489-1.489\- 1.489 | âˆ’0.083-0.083-0.083\- 0.083 | âˆ’0.573-0.573-0.573\- 0.573 | 0.4950.4950.4950.495 | âˆ’0.639-0.639-0.639\- 0.639 | 0.6070.6070.6070.607 | 0.0450.0450.0450.045 | âˆ’435.890-435.890-435.890\- 435.890  
75 | 0.0060.0060.0060.006 | 0.6790.6790.6790.679 | âˆ’1.489-1.489-1.489\- 1.489 | âˆ’0.083-0.083-0.083\- 0.083 | âˆ’0.574-0.574-0.574\- 0.574 | 0.4930.4930.4930.493 | âˆ’0.639-0.639-0.639\- 0.639 | 0.6070.6070.6070.607 | 0.0480.0480.0480.048 | âˆ’436.282-436.282-436.282\- 436.282  
100 | 0.0080.0080.0080.008 | 0.6790.6790.6790.679 | âˆ’1.490-1.490-1.490\- 1.490 | âˆ’0.084-0.084-0.084\- 0.084 | âˆ’0.575-0.575-0.575\- 0.575 | 0.4940.4940.4940.494 | âˆ’0.639-0.639-0.639\- 0.639 | 0.6080.6080.6080.608 | 0.0490.0490.0490.049 | âˆ’436.924-436.924-436.924\- 436.924  
150 | 0.0110.0110.0110.011 | 0.6790.6790.6790.679 | âˆ’1.488-1.488-1.488\- 1.488 | âˆ’0.085-0.085-0.085\- 0.085 | âˆ’0.575-0.575-0.575\- 0.575 | 0.4930.4930.4930.493 | âˆ’0.639-0.639-0.639\- 0.639 | 0.6090.6090.6090.609 | 0.0480.0480.0480.048 | âˆ’436.759-436.759-436.759\- 436.759  
200 | 0.0130.0130.0130.013 | 0.6800.6800.6800.680 | âˆ’1.489-1.489-1.489\- 1.489 | âˆ’0.085-0.085-0.085\- 0.085 | âˆ’0.575-0.575-0.575\- 0.575 | 0.4930.4930.4930.493 | âˆ’0.638-0.638-0.638\- 0.638 | 0.6090.6090.6090.609 | 0.0470.0470.0470.047 | âˆ’437.036-437.036-437.036\- 437.036  
300 | 0.0200.0200.0200.020 | 0.6790.6790.6790.679 | âˆ’1.487-1.487-1.487\- 1.487 | âˆ’0.084-0.084-0.084\- 0.084 | âˆ’0.575-0.575-0.575\- 0.575 | 0.4930.4930.4930.493 | âˆ’0.638-0.638-0.638\- 0.638 | 0.6090.6090.6090.609 | 0.0470.0470.0470.047 | âˆ’436.529-436.529-436.529\- 436.529  
  
| Train | Train  
---|---|---  
Sğ‘†Sitalic_S | â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | Accuracy | Precision | Recall | f1 | Accuracy | Precision | Recall | f1  
10 | -356.687 | 0.711 | 0.710 | 0.701 | 0.706 | 0.698 | 0.679 | 0.674 | 0.676  
20 | -356.667 | 0.713 | 0.712 | 0.703 | 0.708 | 0.698 | 0.679 | 0.674 | 0.676  
30 | -356.653 | 0.713 | 0.712 | 0.703 | 0.708 | 0.693 | 0.673 | 0.667 | 0.670  
50 | -356.645 | 0.713 | 0.712 | 0.703 | 0.708 | 0.698 | 0.679 | 0.674 | 0.676  
75 | -356.642 | 0.713 | 0.712 | 0.703 | 0.708 | 0.698 | 0.679 | 0.674 | 0.676  
100 | -356.640 | 0.711 | 0.710 | 0.701 | 0.706 | 0.698 | 0.679 | 0.674 | 0.676  
150 | -356.640 | 0.711 | 0.710 | 0.701 | 0.706 | 0.698 | 0.679 | 0.674 | 0.676  
200 | -356.639 | 0.711 | 0.710 | 0.701 | 0.706 | 0.698 | 0.679 | 0.674 | 0.676  
300 | -356.639 | 0.711 | 0.710 | 0.701 | 0.706 | 0.698 | 0.679 | 0.674 | 0.676  
  
###  Volatility models

Our second set of experiments involves the estimation of several GARCH-family
volatility models. The models in Tables 9, 10, 11 differ for the number of
estimated parameters, the form of the likelihood function (which can be quite
complex as for the FIGARCH models), and constraints imposed on the parameters.
Besides the GARCH-type models, we include the well-known linear HAR model for
realized volatility (Corsi, 2009). We performed a preliminary study for
retaining only relevant models, e.g., we observed that for a GARCH(1,0,2)
Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is
not significant, so we trained a GARCH(1,0,1), or that the autoregressive
coefficient of the squared innovations is always significant only at lag one,
so we did not consider further lags for Î±ğ›¼\alphaitalic_Î±. For
Î±,Î²,Î³ğ›¼ğ›½ğ›¾\alpha,\beta,\gammaitalic_Î± , italic_Î² , italic_Î³, we
restricted the search up to lag 2. Except for HARâ€™s parameter
Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT,
all the parameters of all the models are statistically significant under
standard ML at 5%percent55\%5 %. Note that the aim of this experiment is to
perform VI to the above class of models, not to discuss their empirical
performance or forecasting ability. For the reader unfamiliar with the above
(standard) econometric models, discussion, and notation we refer, e.g., to the
accessible introduction of (TerÃ¤svirta, 2009).

We report the values of the smoothed lower bound computed at the optimized parameter â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ), the modelâ€™s log-likelihood in the estimates posterior parameter pâ¢(ğ’š|ğœ½â‹†)ğ‘conditionalğ’šsuperscriptğœ½â‹†p\left({\bm{y}}|{\bm{\theta}}^{\star}\right)italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) and the MSE and QLIKE between the fitted values and squared daily returns and subsampled realized variances, used as a volatility proxy. Details on the data and hyperparameters are provided in Appendix B.4.

A visual inspection of the marginal densities in Figures 8,9,10, reveals that,
in general, both MGVBP and MGVB perform quite well compared to MCMC sampling
and that the variational Gaussian assumption is feasible for all the
volatility models. Note that the skews observed in the figures are due to the
parameter transformation: VI is applied on the unconstrained parameters
(ÏˆÏ‰,ÏˆÏ•)subscriptğœ“ğœ”subscriptğœ“italic-Ï•(\psi_{\omega},\psi_{\phi})(
italic_Ïˆ start_POSTSUBSCRIPT italic_Ï‰ end_POSTSUBSCRIPT , italic_Ïˆ
start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT ) and such variational
Gaussians are back-transformed on the original constrained parameter space
where the distributions are generally no longer Gaussian.

Table 9: Estimation results and performance metrics for some GARCH variants.

|  |  |  |  |  | Train | Test  
---|---|---|---|---|---|---|---  
| Ï‰ğœ”\omegaitalic_Ï‰ | Î±ğ›¼\alphaitalic_Î± | Î³ğ›¾\gammaitalic_Î³ | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p\left(y|{\bm{\mu}}^{\star}\right)italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSEr | MSErv | QLIKEr | QLIKErv | pâ¢(y|Î¸â‹†)ğ‘conditionalğ‘¦superscriptğœƒâ‹†p\left(y|\theta^{\star}\right)italic_p ( italic_y | italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSEr | MSErv | QLIKEr | QLIKErv  
| ARCH  
MGVBP | 0.519 | 0.640 |  |  |  | -2262.243 | -2256.136 | 29.659 | 8.164 | 1.918 | 0.531 | -665.516 | 6.575 | 2.564 | 1.685 | 0.566  
MGVB | 0.520 | 0.639 |  |  |  | -2262.244 | -2256.133 | 29.647 | 8.146 | 1.918 | 0.531 | -665.499 | 6.568 | 2.561 | 1.685 | 0.566  
QBVI | 0.520 | 0.639 |  |  |  | -2262.244 | -2256.133 | 29.647 | 8.146 | 1.918 | 0.531 | -665.498 | 6.568 | 2.561 | 1.685 | 0.566  
MCMC | 0.519 | 0.640 |  |  |  |  | -2256.136 | 29.660 | 8.165 | 1.918 | 0.531 | -665.537 | 6.575 | 2.565 | 1.685 | 0.566  
ML | 0.520 | 0.628 |  |  |  |  | -2256.116 | 29.538 | 7.993 | 1.918 | 0.531 | -665.486 | 6.511 | 2.531 | 1.685 | 0.566  
| GARCH(1,0,1)  
MGVBP | 0.043 | 0.230 |  | 0.737 |  | -2027.489 | -2017.633 | 25.560 | 5.124 | 1.637 | 0.330 | -609.837 | 4.772 | 1.577 | 1.423 | 0.382  
MGVB | 0.043 | 0.229 |  | 0.737 |  | -2027.493 | -2017.626 | 25.557 | 5.113 | 1.637 | 0.330 | -609.852 | 4.771 | 1.577 | 1.423 | 0.382  
QBVI | 0.043 | 0.229 |  | 0.737 |  | -2027.493 | -2017.626 | 25.557 | 5.114 | 1.637 | 0.330 | -609.852 | 4.771 | 1.577 | 1.423 | 0.382  
MCMC | 0.042 | 0.231 |  | 0.738 |  |  | -2017.713 | 25.586 | 5.189 | 1.637 | 0.330 | -609.678 | 4.779 | 1.574 | 1.422 | 0.381  
ML | 0.042 | 0.226 |  | 0.739 |  |  | -2017.593 | 25.555 | 5.073 | 1.637 | 0.331 | -610.093 | 4.762 | 1.581 | 1.424 | 0.384  
| GJR(1,1,1)  
MGVBP | 0.043 | 0.108 | 0.294 | 0.722 |  | -2003.542 | -1990.907 | 27.125 | 7.039 | 1.606 | 0.310 | -603.476 | 5.464 | 1.556 | 1.393 | 0.346  
MGVB | 0.044 | 0.108 | 0.292 | 0.722 |  | -2003.550 | -1990.896 | 27.071 | 6.969 | 1.606 | 0.310 | -603.494 | 5.451 | 1.551 | 1.393 | 0.347  
QBVI | 0.044 | 0.108 | 0.292 | 0.722 |  | -2003.550 | -1990.897 | 27.073 | 6.971 | 1.606 | 0.310 | -603.492 | 5.451 | 1.551 | 1.393 | 0.347  
MCMC | 0.043 | 0.108 | 0.298 | 0.724 |  |  | -1991.095 | 27.339 | 7.318 | 1.606 | 0.309 | -603.116 | 5.504 | 1.576 | 1.391 | 0.344  
ML | 0.042 | 0.108 | 0.291 | 0.723 |  |  | -1990.853 | 27.048 | 6.940 | 1.606 | 0.311 | -603.851 | 5.441 | 1.548 | 1.394 | 0.349  
| GJR(1,1,2)  
MGVBP | 0.045 | 0.116 | 0.323 | 0.649 | 0.054 | -2003.193 | -1990.888 | 27.657 | 7.517 | 1.606 | 0.311 | -604.242 | 5.623 | 1.604 | 1.396 | 0.348  
MGVB | 0.045 | 0.114 | 0.319 | 0.643 | 0.061 | -2003.247 | -1990.842 | 27.499 | 7.291 | 1.606 | 0.310 | -604.097 | 5.581 | 1.584 | 1.396 | 0.347  
QBVI | 0.045 | 0.114 | 0.319 | 0.643 | 0.060 | -2003.247 | -1990.843 | 27.501 | 7.294 | 1.606 | 0.310 | -604.098 | 5.582 | 1.585 | 1.396 | 0.347  
MCMC | 0.045 | 0.115 | 0.322 | 0.667 | 0.039 |  | -1990.928 | 27.690 | 7.615 | 1.606 | 0.311 | -604.171 | 5.631 | 1.611 | 1.396 | 0.348  
ML | 0.044 | 0.111 | 0.307 | 0.652 | 0.058 |  | -1990.807 | 27.236 | 6.979 | 1.606 | 0.311 | -603.993 | 5.500 | 1.555 | 1.395 | 0.348  
  
Table 10: Estimation results and performance metrics for EGARCH and FIGARCH
models.

|  |  |  |  | Train | Test  
---|---|---|---|---|---|---  
| Ï‰ğœ”\omegaitalic_Ï‰ | Î±ğ›¼\alphaitalic_Î± | Î³ğ›¾\gammaitalic_Î³ | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p\left(y|{\bm{\mu}}^{\star}\right)italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSEr | MSErv | QLIKEr | QLIKErv | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p\left(y|{\bm{\mu}}^{\star}\right)italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSEr | MSErv | QLIKEr | QLIKErv  
EGARCH(1,0,1) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   
MGVBP | -0.003 | 0.413 |  | 0.929 | -2048.242 | -2032.756 | 26.713 | 5.156 | 1.655 | 0.337 | -610.894 | 4.740 | 1.667 | 1.428 | 0.381  
MGVB | -0.003 | 0.413 |  | 0.929 | -2048.242 | -2032.757 | 26.713 | 5.157 | 1.655 | 0.337 | -610.895 | 4.740 | 1.667 | 1.428 | 0.381  
QBVI | -0.003 | 0.416 |  | 0.929 | -2048.247 | -2032.779 | 26.709 | 5.154 | 1.655 | 0.337 | -610.911 | 4.742 | 1.666 | 1.428 | 0.381  
MCMC | -0.003 | 0.413 |  | 0.929 |  | -2032.752 | 26.711 | 5.155 | 1.655 | 0.337 | -610.878 | 4.739 | 1.667 | 1.428 | 0.381  
ML | -0.003 | 0.405 |  | 0.932 |  | -2032.723 | 26.732 | 5.169 | 1.655 | 0.337 | -610.840 | 4.730 | 1.672 | 1.427 | 0.381  
EGARCH(1,1,1) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   
MGVBP | -0.014 | 0.350 | -0.172 | 0.930 | -2010.242 | -1989.666 | 26.039 | 4.413 | 1.604 | 0.303 | -600.116 | 4.972 | 1.356 | 1.377 | 0.327  
MGVB | -0.014 | 0.350 | -0.172 | 0.930 | -2010.242 | -1989.667 | 26.040 | 4.413 | 1.604 | 0.303 | -600.119 | 4.973 | 1.356 | 1.377 | 0.327  
QBVI | -0.015 | 0.364 | -0.175 | 0.926 | -2010.403 | -1989.914 | 26.081 | 4.454 | 1.604 | 0.304 | -600.654 | 5.022 | 1.363 | 1.379 | 0.328  
MCMC | -0.015 | 0.349 | -0.171 | 0.930 |  | -1989.665 | 26.051 | 4.419 | 1.604 | 0.303 | -600.112 | 4.970 | 1.356 | 1.377 | 0.327  
ML | -0.014 | 0.340 | -0.170 | 0.932 |  | -1989.610 | 26.011 | 4.386 | 1.604 | 0.303 | -599.682 | 4.935 | 1.352 | 1.375 | 0.327  
FIGARCH(0,d,1) | Ï‰Â¯Â¯ğœ”\bar{\omega}overÂ¯ start_ARG italic_Ï‰ end_ARG | Ï•italic-Ï•\phiitalic_Ï• | dğ‘‘ditalic_d | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT |  |  |  |  |  |  |  |  |  |  |   
MGVBP | 0.099 |  | 0.644 | 0.415 | -2022.562 | -2015.130 | 25.533 | 5.540 | 1.634 | 0.326 | -609.607 | 4.789 | 1.574 | 1.422 | 0.382  
MGVB | 0.099 |  | 0.643 | 0.414 | -2022.562 | -2015.130 | 25.532 | 5.538 | 1.634 | 0.326 | -609.604 | 4.788 | 1.574 | 1.422 | 0.382  
QBVI | 0.099 |  | 0.643 | 0.414 | -2022.562 | -2015.130 | 25.532 | 5.539 | 1.634 | 0.326 | -609.607 | 4.789 | 1.574 | 1.422 | 0.382  
MCMC | 0.099 |  | 0.655 | 0.422 |  | -2015.186 | 25.537 | 5.566 | 1.634 | 0.327 | -609.886 | 4.800 | 1.574 | 1.423 | 0.383  
ML | 0.102 |  | 0.653 | 0.428 |  | -2015.094 | 25.582 | 5.570 | 1.634 | 0.326 | -609.345 | 4.780 | 1.574 | 1.420 | 0.381  
FIGARCH(1,d,1) |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   
MGVBP | 0.100 | 0.060 | 0.662 | 0.481 | -2022.666 | -2014.747 | 25.639 | 5.480 | 1.634 | 0.326 | -609.772 | 4.802 | 1.570 | 1.422 | 0.380  
MGVB | 0.100 | 0.060 | 0.661 | 0.480 | -2022.667 | -2014.746 | 25.637 | 5.478 | 1.634 | 0.326 | -609.761 | 4.802 | 1.570 | 1.422 | 0.380  
QBVI | 0.100 | 0.060 | 0.661 | 0.480 | -2022.667 | -2014.746 | 25.638 | 5.478 | 1.634 | 0.326 | -609.762 | 4.802 | 1.570 | 1.422 | 0.380  
MCMC | 0.100 | 0.059 | 0.668 | 0.482 |  | -2014.805 | 25.628 | 5.495 | 1.634 | 0.326 | -610.007 | 4.813 | 1.570 | 1.423 | 0.381  
ML | 0.100 | 0.064 | 0.653 | 0.479 |  | -2014.721 | 25.649 | 5.458 | 1.634 | 0.326 | -609.489 | 4.791 | 1.571 | 1.421 | 0.379  
  
Table 11: Additional results for the HAR model involving the hâ„hitalic_h-
function gradient estimation and the diagonal posterior form.

|  |  |  |  |  | Train | Test  
---|---|---|---|---|---|---|---  
| Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | ÏƒÎµsubscriptğœğœ€\sigma_{\varepsilon}italic_Ïƒ start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT | â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p\left(y|{\bm{\mu}}^{\star}\right)italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSErv |  102Ã—10^{2}\times10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã—QLIKErv | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p\left(y|{\bm{\mu}}^{\star}\right)italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSErv |  102Ã—10^{2}\times10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã—QLIKErv  
MGVBP | 1.032 | 0.489 | 0.420 | -0.009 | 4.922 | -5080.191 | -5058.113 | 24.193 | 5.324 | -1220.154 | 18.861 | 5.712  
MGVB | 1.019 | 0.485 | 0.426 | -0.011 | 4.935 | -5082.123 | -5058.167 | 24.194 | 5.328 | -1220.550 | 18.879 | 5.720  
QBVI | 1.023 | 0.489 | 0.420 | -0.008 | 4.924 | -5080.679 | -5058.127 | 24.193 | 5.323 | -1220.206 | 18.862 | 5.714  
MGVBP-func.hsuperscript-func.â„{}^{h}\text{-func.}start_FLOATSUPERSCRIPT italic_h end_FLOATSUPERSCRIPT -func. | 1.032 | 0.489 | 0.420 | -0.009 | 4.922 | -5080.192 | -5058.113 | 24.193 | 5.324 | -1220.148 | 18.861 | 5.712  
MGVB-func.hsuperscript-func.â„{}^{h}\text{-func.}start_FLOATSUPERSCRIPT italic_h end_FLOATSUPERSCRIPT -func. | 1.019 | 0.485 | 0.426 | -0.011 | 4.934 | -5082.127 | -5058.166 | 24.194 | 5.328 | -1220.545 | 18.879 | 5.720  
QBVI-func.hsuperscript-func.â„{}^{h}\text{-func.}start_FLOATSUPERSCRIPT italic_h end_FLOATSUPERSCRIPT -func. | 1.023 | 0.489 | 0.420 | -0.008 | 4.924 | -5080.652 | -5058.126 | 24.193 | 5.323 | -1220.200 | 18.862 | 5.714  
MGVBPdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 1.042 | 0.487 | 0.423 | -0.010 | 4.925 | -5082.818 | -5058.111 | 24.193 | 5.327 | -1220.265 | 18.868 | 5.714  
MGVBdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 1.020 | 0.485 | 0.430 | -0.015 | 4.944 | -5084.909 | -5058.230 | 24.195 | 5.331 | -1220.894 | 18.897 | 5.729  
QBVIdiag.diag.{}^{\text{diag.}}start_FLOATSUPERSCRIPT diag. end_FLOATSUPERSCRIPT | 1.036 | 0.485 | 0.423 | -0.007 | 4.931 | -5083.253 | -5058.143 | 24.193 | 5.326 | -1220.339 | 18.862 | 5.710  
MCMC | 1.075 | 0.488 | 0.421 | -0.012 | 4.922 |  | -5058.090 | 24.192 | 5.328 | -1220.130 | 18.858 | 5.706  
ML | 1.080 | 0.488 | 0.421 | -0.012 | 4.919 |  | -5058.086 | 24.192 | 5.328 | -1220.053 | 18.857 | 5.706  
  
![Refer to caption]()![Refer to caption]()

Figure 8: GARCH(1,1) model. MCMC and variational marginals in for the
unconstrained parameters
(ÏˆÏ‰,ÏˆÎ±,ÏˆÎ³,ÏˆÎ²)subscriptğœ“ğœ”subscriptğœ“ğ›¼subscriptğœ“ğ›¾subscriptğœ“ğ›½(\psi_{\omega},\psi_{\alpha},\psi_{\gamma},\psi_{\beta})(
italic_Ïˆ start_POSTSUBSCRIPT italic_Ï‰ end_POSTSUBSCRIPT , italic_Ïˆ
start_POSTSUBSCRIPT italic_Î± end_POSTSUBSCRIPT , italic_Ïˆ
start_POSTSUBSCRIPT italic_Î³ end_POSTSUBSCRIPT , italic_Ïˆ
start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT ) (top row) and constrained
parameters (Ï‰,Î±,Î³,Î²)ğœ”ğ›¼ğ›¾ğ›½(\omega,\alpha,\gamma,\beta)( italic_Ï‰ ,
italic_Î± , italic_Î³ , italic_Î² ) (bottom row).

![Refer to caption]()![Refer to caption]()

Figure 9: GJR(1,1,1) model. MCMC and variational marginals in for the
unconstrained parameters (top row) and constrained parameters (bottom row).

![Refer to caption]()![Refer to caption]()

Figure 10: FIGARCH(1,d,1) model. MCMC and variational marginals in for the
unconstrained parameters (top row) and constrained parameters (bottom row).

###  Istanbul dataset: block-diagonal covariance

In this section, we apply MGVBP under different assumptions for the structure
of the variational covariance matrix. We use the Istanbul stock exchange
dataset of (Akbilgic etÂ al., 2014), (details are provided in Appendix B.4).
To demonstrate the feasibility of the block-diagonal estimation under the
mean-field framework outlined in Section 5.6, we consider the following model
for the daily returns of the Istanbul Stock Exchange National 100 index (ISE):

| ISEt=Î²0+Î²1â¢SPt+Î²2â¢NIKt+Î²3â¢BOVt+Î²4â¢DAXt+Î²5â¢FTSEt+Î²6â¢EUt+Î²7â¢EMt+Îµtâ¢,subscriptISEğ‘¡subscriptğ›½0subscriptğ›½1subscriptSPğ‘¡subscriptğ›½2subscriptNIKğ‘¡subscriptğ›½3subscriptBOVğ‘¡subscriptğ›½4subscriptDAXğ‘¡subscriptğ›½5subscriptFTSEğ‘¡subscriptğ›½6subscriptEUğ‘¡subscriptğ›½7subscriptEMğ‘¡subscriptğœ€ğ‘¡,\text{ISE}_{t}=\beta_{0}+\beta_{1}\text{SP}_{t}+\beta_{2}\text{NIK}_{t}+\beta_% {3}\text{BOV}_{t}+\beta_{4}\text{DAX}_{t}+\beta_{5}\text{FTSE}_{t}+\beta_{6}% \text{EU}_{t}+\beta_{7}\text{EM}_{t}+\varepsilon_{t}\text{,}ISE start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT SP start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT NIK start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT BOV start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT DAX start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT FTSE start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT EU start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT EM start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Îµ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , |   
---|---|---  
  
with Îµtâˆ¼ğ’©â¢(0,ÏƒÎµ2)similar-
tosubscriptğœ€ğ‘¡ğ’©0superscriptsubscriptğœğœ€2\varepsilon_{t}\sim\mathcal{N}\left(0,\sigma_{\varepsilon}^{2}\right)italic_Îµ
start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ¼ caligraphic_N ( 0 ,
italic_Ïƒ start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) (conditionally on the regressors
at time tğ‘¡titalic_t). The covariates respectively correspond to daily
returns of the S&P 500 index, the Japanese Nikkei index, the Brazilian Bovespa
index, the German DAX index, the UK FTSE index, the MSCI European index, and
of the MSCI emerging market index. We estimate the coefficients
Î²0,â€¦,Î²7subscriptğ›½0â€¦subscriptğ›½7\beta_{0},\dots,\beta_{7}italic_Î²
start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , â€¦ , italic_Î² start_POSTSUBSCRIPT
7 end_POSTSUBSCRIPT and the transformed parameter
ÏˆÏƒ=logâ¡(Ïƒ)subscriptğœ“ğœğœ\psi_{\sigma}=\log\left(\sigma\right)italic_Ïˆ
start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT = roman_log ( italic_Ïƒ ),
from which Ïƒğœ\sigmaitalic_Ïƒ is computed as
Ïƒ=expâ¡(ÏˆÏƒ)+Varâ¢(ÏˆÏƒ)/2ğœsubscriptğœ“ğœVarsubscriptğœ“ğœ2\sigma=\exp\left(\psi_{\sigma}\right)+\text{Var}\left(\psi_{\sigma}\right)/2italic_Ïƒ
= roman_exp ( italic_Ïˆ start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT ) +
Var ( italic_Ïˆ start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT ) / 2, with
Varâ¢(ÏˆÏƒ)Varsubscriptğœ“ğœ\text{Var}\left(\psi_{\sigma}\right)Var (
italic_Ïˆ start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT ) taken from the
variational posterior covariance matrix.

We consider the following structures for the variational posterior: (i) full
covariance matrix (Full), (ii) diagonal covariance matrix (Diagonal), (iii)
block-diagonal structure with two blocks of sizes 8Ã—8888\times 88 Ã— 8 and
1Ã—1111\times 11 Ã— 1 (Block 1) and, (iv) block diagonal structure with blocks
of sizes 1Ã—1111\times 11 Ã— 1, 3Ã—3333\times 33 Ã— 3, 2Ã—2222\times 22 Ã— 2,
2Ã—2222\times 22 Ã— 2 and 1Ã—1111\times 11 Ã— 1 (Block 2). Case (iii), models
the covariance between the regression coefficients (Î²ğ›½\betaitalic_Î²s) but
neglects their covariance with the variance of the error
ÏƒÎµ2subscriptsuperscriptğœ2ğœ€\sigma^{2}_{\varepsilon}italic_Ïƒ
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Îµ
end_POSTSUBSCRIPT. Case (iv) groups the indices traded in non-European stock
exchanges in a 3Ã—3333\times 33 Ã— 3 block, and in the remaining 2Ã—2222\times
22 Ã— 2 blocks, the indices referring to European exchanges and the two MSCI
indexes. As for the earlier case, the covariances between the regression
parameters and
ÏƒÎµ2subscriptsuperscriptğœ2ğœ€\sigma^{2}_{\varepsilon}italic_Ïƒ
start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Îµ
end_POSTSUBSCRIPT are ignored. Note that the purpose of this application is to
provide an example for Algorithm 2 and for the discussion in Appendix 5.6. To
this end, structures (iii) and (iv) correspond to an intuitive and
economically motivated grouping of the variables. Providing an effective
predictive model supported by a solid econometric rationale is here out of
scope.

Table 12 and 13 summarize the estimation results. Table 12 shows that the
impact of the different structures of the covariance matrix is somewhat
marginal in terms of the performance measures, with respect to each other and
with respect to the ML estimates. As for the logistic regression example, in
the most constrained cases (ii) and (iii), we observe that the estimates of
certain posterior means slightly deviate from the others, indicating that the
algorithm perhaps reaches a different maximum. Regarding the variational
covariances reported in Table 13, there is remarkable accordance between the
covariance structures (i), (iii), and ML, while for the diagonal structure
(ii) and block-diagonal structure (iv) the covariances are misaligned with the
ML and full-diagonal case, further suggesting the convergence of the
algorithms at different maxima of the LB.

From a theoretical perspective, if Î£Î£\Sigmaroman_Î£ is the covariance matrix
of the joint Gaussian distribution of the variates (case (i)), estimates of
the block-diagonal entries (or main diagonal) should match the corresponding
elements in Î£Î£\Sigmaroman_Î£. However, the elements in the sub-matrices,
e.g., in cases (ii) and (iv), deviate from those Î£Î£\Sigmaroman_Î£, case (i).
Indeed, the results refer to independent optimizations of alternative models
(over the same dataset) that are not granted to converge at the same LB
maximum (and thus variational Gaussian). Across the covariance structures (i)
to (iv) the optimal variational parameters correspond to different
multivariate distributions, that independently maximize the lower bound, and
that are not constrained to be related to each other. This is indeed confirmed
by the differences in the maximized Lower bound
â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L
( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT
start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) in Table 12, and in the
different levels at which the curves in Figure 11 are observed to converge. In
this light, the ML estimatesâ€™ variances in Table 14 can be compared to those
of case (i), but are misleading for the other cases, as the covariance matrix
of the asymptotic (Gaussian) distribution of the ML estimator is implicitly a
full matrix.

Table 12: Posterior means, ML estimates, and performance measures on the train
and test set.

| Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | ÏƒÎµsubscriptğœğœ€\sigma_{\varepsilon}italic_Ïƒ start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT  
---|---|---|---|---|---|---|---|---|---  
Full (case i) | 0.0010.0010.0010.001 | 0.0980.0980.0980.098 | 0.0790.0790.0790.079 | âˆ’0.271-0.271-0.271\- 0.271 | âˆ’0.167-0.167-0.167\- 0.167 | âˆ’0.354-0.354-0.354\- 0.354 | 1.1641.1641.1641.164 | 0.9440.9440.9440.944 | 0.0140.0140.0140.014  
Diagonal (case ii) | 0.0010.0010.0010.001 | 0.0740.0740.0740.074 | 0.0920.0920.0920.092 | âˆ’0.239-0.239-0.239\- 0.239 | 0.0930.0930.0930.093 | âˆ’0.015-0.015-0.015\- 0.015 | 0.5550.5550.5550.555 | 0.9350.9350.9350.935 | 0.0140.0140.0140.014  
Block 1 (case iii) | 0.0010.0010.0010.001 | 0.0980.0980.0980.098 | 0.0790.0790.0790.079 | âˆ’0.272-0.272-0.272\- 0.272 | âˆ’0.167-0.167-0.167\- 0.167 | âˆ’0.353-0.353-0.353\- 0.353 | 1.1641.1641.1641.164 | 0.9430.9430.9430.943 | 0.0140.0140.0140.014  
Block 2 (case iv) | 0.0010.0010.0010.001 | 0.0670.0670.0670.067 | 0.1150.1150.1150.115 | âˆ’0.218-0.218-0.218\- 0.218 | 0.2010.2010.2010.201 | 0.1620.1620.1620.162 | 0.2930.2930.2930.293 | 0.8710.8710.8710.871 | 0.0140.0140.0140.014  
ML | 0.0010.0010.0010.001 | 0.0990.0990.0990.099 | 0.0780.0780.0780.078 | âˆ’0.273-0.273-0.273\- 0.273 | âˆ’0.174-0.174-0.174\- 0.174 | âˆ’0.363-0.363-0.363\- 0.363 | 1.1791.1791.1791.179 | 0.9460.9460.9460.946 | 0.0140.0140.0140.014  
  
| Train | Test  
---|---|---  
| â„’â‹†superscriptâ„’â‹†\mathcal{L}^{\star}caligraphic_L start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p\left(y|{\bm{\mu}}^{\star}\right)italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) |  102Ã—10^{2}\times10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã—MSE | pâ¢(y|ğâ‹†)ğ‘conditionalğ‘¦superscriptğâ‹†p\left(y|{\bm{\mu}}^{\star}\right)italic_p ( italic_y | bold_italic_Î¼ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) |  102Ã—10^{2}\times10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Ã—MSE  
Full | 1186.0821186.0821186.0821186.082 | 1223.701223.701223.701223.70 | 19.4119.4119.4119.41 | 316.84316.84316.84316.84 | 4.154.154.154.15  
Diagonal | 1173.6621173.6621173.6621173.662 | 1214.831214.831214.831214.83 | 20.2320.2320.2320.23 | 316.71316.71316.71316.71 | 4.124.124.124.12  
Block 1 | 1186.0871186.0871186.0871186.087 | 1223.701223.701223.701223.70 | 19.4119.4119.4119.41 | 316.86316.86316.86316.86 | 4.154.154.154.15  
Block 2 | 1172.5801172.5801172.5801172.580 | 1212.281212.281212.281212.28 | 20.4820.4820.4820.48 | 316.72316.72316.72316.72 | 4.104.104.104.10  
ML |  | 1223.731223.731223.731223.73 | 19.4119.4119.4119.41 | 316.87316.87316.87316.87 | 4.154.154.154.15  
  
Table 13: Top panel: posterior covariance matrix under the full specification
(case i), and covariances of the ML estimates. Bottom panel: posterior
covariance matrices under the Block 1 and Block 2 structures. All the entries
are multiplied by 104superscript10410^{4}10 start_POSTSUPERSCRIPT 4
end_POSTSUPERSCRIPT.

|  | Full  
---|---|---  
|  | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | Î²8subscriptğ›½8\beta_{8}italic_Î² start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT | ÏƒÎµsubscriptğœğœ€\sigma_{\varepsilon}italic_Ïƒ start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT  
ML | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT |  | 0.0010.0010.0010.001 | 0.0020.0020.0020.002 | âˆ’0.001-0.001-0.001\- 0.001 | âˆ’0.002-0.002-0.002\- 0.002 | âˆ’0.001-0.001-0.001\- 0.001 | 0.0060.0060.0060.006 | âˆ’0.008-0.008-0.008\- 0.008 | 0.0000.0000.0000.000  
Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | 0.0010.0010.0010.001 |  | 0.0080.0080.0080.008 | âˆ’2.947-2.947-2.947\- 2.947 | âˆ’1.915-1.915-1.915\- 1.915 | âˆ’0.767-0.767-0.767\- 0.767 | âˆ’0.094-0.094-0.094\- 0.094 | 1.5201.5201.5201.520 | âˆ’0.001-0.001-0.001\- 0.001  
Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | 0.0030.0030.0030.003 | âˆ’0.032-0.032-0.032\- 0.032 |  | 1.0941.0941.0941.094 | 0.4420.4420.4420.442 | 0.7680.7680.7680.768 | âˆ’0.640-0.640-0.640\- 0.640 | âˆ’4.141-4.141-4.141\- 4.141 | 0.0120.0120.0120.012  
Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | âˆ’0.001-0.001-0.001\- 0.001 | âˆ’2.992-2.992-2.992\- 2.992 | 1.0941.0941.0941.094 |  | 1.0181.0181.0181.018 | 0.6830.6830.6830.683 | âˆ’1.213-1.213-1.213\- 1.213 | âˆ’4.710-4.710-4.710\- 4.710 | 0.0170.0170.0170.017  
Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | âˆ’0.002-0.002-0.002\- 0.002 | âˆ’1.906-1.906-1.906\- 1.906 | 0.2970.2970.2970.297 | 0.9610.9610.9610.961 |  | 3.7423.7423.7423.742 | âˆ’20.149-20.149-20.149\- 20.149 | âˆ’0.748-0.748-0.748\- 0.748 | 0.0280.0280.0280.028  
Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | âˆ’0.002-0.002-0.002\- 0.002 | âˆ’0.729-0.729-0.729\- 0.729 | 0.6790.6790.6790.679 | 0.6220.6220.6220.622 | 3.8363.8363.8363.836 |  | âˆ’28.773-28.773-28.773\- 28.773 | âˆ’1.594-1.594-1.594\- 1.594 | 0.0520.0520.0520.052  
Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | 0.0070.0070.0070.007 | âˆ’0.170-0.170-0.170\- 0.170 | âˆ’0.393-0.393-0.393\- 0.393 | âˆ’1.065-1.065-1.065\- 1.065 | âˆ’20.477-20.477-20.477\- 20.477 | âˆ’28.813-28.813-28.813\- 28.813 |  | âˆ’2.984-2.984-2.984\- 2.984 | âˆ’0.142-0.142-0.142\- 0.142  
Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | âˆ’0.009-0.009-0.009\- 0.009 | 1.6051.6051.6051.605 | âˆ’4.148-4.148-4.148\- 4.148 | âˆ’4.700-4.700-4.700\- 4.700 | âˆ’0.563-0.563-0.563\- 0.563 | âˆ’1.386-1.386-1.386\- 1.386 | âˆ’3.324-3.324-3.324\- 3.324 |  | 0.0300.0300.0300.030  
|  | Block 1  
|  | Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | Î²8subscriptğ›½8\beta_{8}italic_Î² start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT | ÏƒÎµsubscriptğœğœ€\sigma_{\varepsilon}italic_Ïƒ start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT  
Block 2 | Î²0subscriptğ›½0\beta_{0}italic_Î² start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT |  | 0.0010.0010.0010.001 | 0.0020.0020.0020.002 | âˆ’0.001-0.001-0.001\- 0.001 | âˆ’0.003-0.003-0.003\- 0.003 | âˆ’0.002-0.002-0.002\- 0.002 | 0.0080.0080.0080.008 | âˆ’0.009-0.009-0.009\- 0.009 |   
Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT |  |  | âˆ’0.064-0.064-0.064\- 0.064 | âˆ’2.966-2.966-2.966\- 2.966 | âˆ’2.005-2.005-2.005\- 2.005 | âˆ’0.728-0.728-0.728\- 0.728 | âˆ’0.049-0.049-0.049\- 0.049 | 1.6651.6651.6651.665 |   
Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT |  | 0.3220.3220.3220.322 |  | 1.0791.0791.0791.079 | 0.2960.2960.2960.296 | 0.6900.6900.6900.690 | âˆ’0.382-0.382-0.382\- 0.382 | âˆ’4.075-4.075-4.075\- 4.075 |   
Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT |  | âˆ’3.327-3.327-3.327\- 3.327 | âˆ’0.525-0.525-0.525\- 0.525 |  | 0.9500.9500.9500.950 | 0.5120.5120.5120.512 | âˆ’1.010-1.010-1.010\- 1.010 | âˆ’4.558-4.558-4.558\- 4.558 |   
Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT |  |  |  |  |  | 3.8413.8413.8413.841 | âˆ’20.608-20.608-20.608\- 20.608 | âˆ’0.584-0.584-0.584\- 0.584 |   
Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT |  |  |  |  | âˆ’8.708-8.708-8.708\- 8.708 |  | âˆ’28.509-28.509-28.509\- 28.509 | âˆ’1.107-1.107-1.107\- 1.107 |   
Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT |  |  |  |  |  |  |  | âˆ’3.584-3.584-3.584\- 3.584 |   
Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT |  |  |  |  |  |  | âˆ’4.452-4.452-4.452\- 4.452 |  |   
  
Table 14: Standard deviations of the posterior marginals for the different
cases, along with ML standard errors. Entries are multiplied by
102superscript10210^{2}10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.

| Î²1subscriptğ›½1\beta_{1}italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î²2subscriptğ›½2\beta_{2}italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | Î²3subscriptğ›½3\beta_{3}italic_Î² start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | Î²4subscriptğ›½4\beta_{4}italic_Î² start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT | Î²5subscriptğ›½5\beta_{5}italic_Î² start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT | Î²6subscriptğ›½6\beta_{6}italic_Î² start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT | Î²7subscriptğ›½7\beta_{7}italic_Î² start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT | Î²8subscriptğ›½8\beta_{8}italic_Î² start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT | Ïƒğœ\sigmaitalic_Ïƒ  
---|---|---|---|---|---|---|---|---|---  
Full | 0.0690.0690.0690.069 | 7.4727.4727.4727.472 | 5.6415.6415.6415.641 | 7.3227.3227.3227.322 | 12.91512.91512.91512.915 | 16.66316.66316.66316.663 | 23.05623.05623.05623.056 | 12.39512.39512.39512.395 | 3.4633.4633.4633.463  
Diagonal | 0.0730.0730.0730.073 | 4.6624.6624.6624.662 | 4.0564.0564.0564.056 | 4.4774.4774.4774.477 | 4.3434.3434.3434.343 | 5.5875.5875.5875.587 | 4.7334.7334.7334.733 | 5.9765.9765.9765.976 | 3.5993.5993.5993.599  
Block 1 | 0.0680.0680.0680.068 | 7.4707.4707.4707.470 | 5.6295.6295.6295.629 | 7.2747.2747.2747.274 | 13.07213.07213.07213.072 | 16.53416.53416.53416.534 | 23.09223.09223.09223.092 | 12.25712.25712.25712.257 | 3.4213.4213.4213.421  
Block 2 | 0.0710.0710.0710.071 | 6.5306.5306.5306.530 | 4.4514.4514.4514.451 | 6.6116.6116.6116.611 | 9.3339.3339.3339.333 | 10.62010.62010.62010.620 | 7.0017.0017.0017.001 | 8.7248.7248.7248.724 | 3.3673.3673.3673.367  
ML | 0.0680.0680.0680.068 | 7.4897.4897.4897.489 | 5.6315.6315.6315.631 | 7.3227.3227.3227.322 | 12.99712.99712.99712.997 | 16.63616.63616.63616.636 | 23.13323.13323.13323.133 | 12.36312.36312.36312.363 |   
  
![Refer to caption]() Figure 11: Lower bound optimization for the Istanbul
data under the different posterior variational covariance matrix
specifications.

###  Datasets and hyperparameters

Table 15: Details on the datasets and corresponding models (upper panel), and
hyperparameters used in the experiments (lower panel).

Dataset |  Model |  Number of variational parameters |  Number of samples |  Samples in train set |  Samples in test set |  Period  
---|---|---|---|---|---|---  
Labour | Logistic regression | 72 | 753 | 564 (75%) | 189 (25%) |   
S&P 500 | ARCH | 6 | 2123 | 1689 (80%) | 425 | 3-Jan-2014 / 28-Jun-2022  
| GARCH(1,0,1) | 12 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| GJR(1,1,1) | 20 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| GJR(1,1,2) | 30 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| EGARCH(1,0,1) | 12 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| EGARCH(1,1,1) | 20 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| EGARCH(1,1,2) | 30 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| FIGARCH(0,1,1) | 12 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| FIGARCH(1,1,2) | 20 | 2123 | 1689 | 425 | 3-Jan-2014 / 28-Jun-2022  
| HAR (Linear regr.) | 30 | 2102 | 1681 | 421 | 4-Feb-2014 / 28-Jun-2022  
Istanbul | Linear regression | 90 | 536 | 428 (80%) | 108 | 5-Jan-2009 / 22-Feb-2022  
LOB | Neural network | 54990 | 557297 | 256461(65%) | 150418 | 1-Jun-2010 / 14-Jun-2010  
Synthetic | Non-diff. model | 420 | 5000 | 4000 (80%) | 1000 |   
  
| MGVBP optimizer | Initial values | Prior  
---|---|---|---  
Experiment | Î²ğ›½\betaitalic_Î² | Grad. clip | Grad. clip init. | Ï‰ğœ”\omegaitalic_Ï‰ | wğ‘¤witalic_w | tmaxsubscriptğ‘¡maxt_{\text{max}}italic_t start_POSTSUBSCRIPT max end_POSTSUBSCRIPT | tâ€²superscriptğ‘¡â€²t^{\prime}italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | Pğ‘ƒPitalic_P | Sğ‘†Sitalic_S | ğ1subscriptğ1{\bm{\mu}}_{1}bold_italic_Î¼ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | Î£1subscriptÎ£1\Sigma_{1}roman_Î£ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | ğ0subscriptğ0{\bm{\mu}}_{0}bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | Î£0subscriptÎ£0\Sigma_{0}roman_Î£ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  
Labour data | 0.01 | 3000 | 1000 | 0.4 | 30 | 1200 | 1000 | 500 | 75 | âˆ¼ğ’©â¢(0,Î£1)similar-toabsentğ’©0subscriptÎ£1\sim\mathcal{N}\left(0,\Sigma_{1}\right)âˆ¼ caligraphic_N ( 0 , roman_Î£ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) | 0.05 | 0 | 5  
ARCH-GARCH-GJR | 0.01 | 1000 | 1000 | 0.4 | 30 | 1200 | 1000 | 500 | 150 | ML | 0.05 | 0 | 5  
EGARCH | 0.01 | 1000 | 1000 | 0.4 | 30 | 3000 | 2500 | 500 | 150 | ML | 0.05 | 0 | 5  
FIGARCH | 0.01 | 1000 | 1000 | 0.4 | 30 | 1200 | 1000 | 500 | 150 | ML | 0.05 | 0 | 5  
HAR | 0.001 | 50000 | 1000 | 0.4 | 30 | 1000 | 750 | 100 | 150 | ML | 0.01 | 0 | 5  
Istanbul data | 0.07 | 50000 | 500 | 0.4 | 30 | 1200 | 1000 | 500 | 100 | âˆ¼ğ’©â¢(0,Î£1)similar-toabsentğ’©0subscriptÎ£1\sim\mathcal{N}\left(0,\Sigma_{1}\right)âˆ¼ caligraphic_N ( 0 , roman_Î£ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) | 0.01 | 0 | 5  
LOB data | 0.1 | 5000 | 100 | 0.4 | 30 | 4000 | 3000 | 500 | 20 | ADAM | 0.01 | 0 | 5  
Non-diff. model | 0.1 | 150 | 1000 | 0.4 | 30 | 5000 | 5000 | 500 | 15 | âˆ¼ğ’©â¢(0,Î£1)similar-toabsentğ’©0subscriptÎ£1\sim\mathcal{N}\left(0,\Sigma_{1}\right)âˆ¼ caligraphic_N ( 0 , roman_Î£ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) | 0.01 | 0 | 1  
  
Table 15 summarizes some information about the datasets and the setup used
across the experiments. For the experiments on the Labour and S&P 500
datasets, the same set of hyperparameters applies to MGVBP, MGVB (and QBVI).
While the Labour333Publicly available at key2stats.com/data-set/view/140. See
(Mroz, 1984) for details. The data is also adopted in other VI applications,
e.g., (Tran etÂ al., 2021b; Magris etÂ al., 2022). and Istanbul444Publicly
available at the UCI repository,
archive.ics.uci.edu/ml/datasets/istanbul+stock+exchange. See (Akbilgic etÂ
al., 2014) for details. datasets are readily available, the S&P 500 dataset is
extracted from the Oxford-Man Institute realized volatility
library555realized.oxford-man.ox.ac.uk.. We use daily close-to-close demeaned
returns for the GARCH-family models and 5-minute sub-sampled daily measures of
realized volatilities (further annualized) for the HAR model. For information
on the publicly available limit-order book (LOB) dataset, see (Ntakaris etÂ
al., 2018). MGVBP and all optimizers appearing in Table 5 are initialized
after 30 ADAM iterations for the neural network experiments. VOGN is
implemented on the LOB data following the scheme presented in (Magris etÂ al.,
2023). The synthetic data used for the non-differentiable model is discussed
within Section 7.

#### Starting values

As a robustness check, in Table 16 and Table 17 we provide summary statistics
of the estimation results and performance metrics for two representative
models. With both a random and constant initialization of
ğœ½0subscriptğœ½0{\bm{\theta}}_{0}bold_italic_Î¸ start_POSTSUBSCRIPT 0
end_POSTSUBSCRIPT, we see a tight consistency in the results, underlining the
robustness of the estimation routine with respect to the choice of the
starting values.

Table 16: Estimation results for the Logistic regression model (Labour
dataset) under different starting values. Statistics are computed across 200
replications. Upper table: random initialization of
ğœ½0subscriptğœ½0{\bm{\theta}}_{0}bold_italic_Î¸ start_POSTSUBSCRIPT 0
end_POSTSUBSCRIPT from a multivariate Gaussian with diagonal covariance
matrix. Lower table: initialization of
ğœ½0subscriptğœ½0{\bm{\theta}}_{0}bold_italic_Î¸ start_POSTSUBSCRIPT 0
end_POSTSUBSCRIPT as a vector of constants.

ğœ½0âˆ¼ğ’©â¢(ğ,Ïƒ2â¢I)similar-tosubscriptğœ½0ğ’©ğsuperscriptğœ2ğ¼{\bm{\theta}}_{0}\sim\mathcal{N}({\bm{\mu}},\sigma^{2}I)bold_italic_Î¸ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âˆ¼ caligraphic_N ( bold_italic_Î¼ , italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_I ) | Mean | Std. Dev. (Ã—100)(\times 100)( Ã— 100 )  
---|---|---  
ğğ{\bm{\mu}}bold_italic_Î¼ | Ïƒ2superscriptğœ2\sigma^{2}italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | Accuracy | Precision | Recall | F1 | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | Accuracy | Precision | Recall | F1  
0 | 0.1 | -356.640 | 71.179 | 71.108 | 70.216 | 70.659 | 0.091 | 0.090 | 0.103 | 0.084 | 0.093  
0 | 1 | -356.640 | 71.152 | 71.076 | 70.194 | 70.632 | 0.081 | 0.083 | 0.096 | 0.076 | 0.086  
0 | 5 | -356.640 | 71.117 | 71.037 | 70.160 | 70.595 | 0.117 | 0.079 | 0.087 | 0.077 | 0.082  
0 | 10 | -356.641 | 71.126 | 71.047 | 70.168 | 70.605 | 0.065 | 0.065 | 0.074 | 0.060 | 0.067  
5 | 0.1 | -356.640 | 71.117 | 71.035 | 70.161 | 70.596 | 0.111 | 0.055 | 0.063 | 0.049 | 0.056  
5 | 1 | -356.640 | 71.126 | 71.047 | 70.168 | 70.605 | 0.117 | 0.087 | 0.096 | 0.084 | 0.089  
5 | 5 | -356.640 | 71.137 | 71.061 | 70.175 | 70.616 | 0.081 | 0.095 | 0.105 | 0.092 | 0.098  
5 | 10 | -356.640 | 71.183 | 71.111 | 70.222 | 70.664 | 0.072 | 0.091 | 0.105 | 0.082 | 0.093  
-5 | 0.1 | -356.640 | 71.117 | 71.035 | 70.161 | 70.596 | 0.113 | 0.055 | 0.063 | 0.049 | 0.056  
-5 | 1 | -356.640 | 71.126 | 71.045 | 70.170 | 70.605 | 0.090 | 0.065 | 0.075 | 0.059 | 0.067  
-5 | 5 | -356.640 | 71.152 | 71.078 | 70.192 | 70.632 | 0.098 | 0.101 | 0.113 | 0.097 | 0.104  
-5 | 10 | -356.640 | 71.146 | 71.068 | 70.188 | 70.625 | 0.086 | 0.080 | 0.092 | 0.073 | 0.082  
ğœ½0=câ¢ğŸdsubscriptğœ½0ğ‘subscript1ğ‘‘{\bm{\theta}}_{0}=c\bm{1}_{d}bold_italic_Î¸ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_c bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | Mean | Std. Dev. (Ã—100)(\times 100)( Ã— 100 )  
cğ‘citalic_c | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | Accuracy | Precision | Recall | F1 | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | Accuracy | Precision | Recall | F1  
0 | -356.640 | 71.165 | 71.092 | 70.203 | 70.644 | 0.064 | 0.088 | 0.100 | 0.082 | 0.090  
1 | -356.640 | 71.146 | 71.068 | 70.188 | 70.625 | 0.080 | 0.080 | 0.092 | 0.073 | 0.082  
-1 | -356.640 | 71.144 | 71.067 | 70.184 | 70.623 | 0.126 | 0.098 | 0.108 | 0.093 | 0.100  
5 | -356.640 | 71.137 | 71.059 | 70.177 | 70.616 | 0.080 | 0.095 | 0.105 | 0.091 | 0.098  
-5 | -356.640 | 71.144 | 71.066 | 70.186 | 70.623 | 0.077 | 0.079 | 0.091 | 0.071 | 0.081  
20 | -356.641 | 71.135 | 71.057 | 70.176 | 70.614 | 0.112 | 0.073 | 0.083 | 0.067 | 0.075  
-20 | -356.641 | 71.179 | 71.108 | 70.216 | 70.659 | 0.114 | 0.107 | 0.119 | 0.102 | 0.110  
  
Table 17: Estimation results for the GARCH(1,0,1) model under different
starting values. Statistics are computed across 200 replications. Upper table:
random initialization of ğœ½0subscriptğœ½0{\bm{\theta}}_{0}bold_italic_Î¸
start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT from a multivariate Gaussian with
diagonal covariance matrix. Lower table: initialization of
ğœ½0subscriptğœ½0{\bm{\theta}}_{0}bold_italic_Î¸ start_POSTSUBSCRIPT 0
end_POSTSUBSCRIPT as a vector of constants.

ğœ½0âˆ¼ğ’©â¢(ğ,Ïƒ2â¢I)similar-tosubscriptğœ½0ğ’©ğsuperscriptğœ2ğ¼{\bm{\theta}}_{0}\sim\mathcal{N}({\bm{\mu}},\sigma^{2}I)bold_italic_Î¸ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âˆ¼ caligraphic_N ( bold_italic_Î¼ , italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_I ) | Mean | Std. Dev. (Ã—100)(\times 100)( Ã— 100 )  
---|---|---  
ğğ{\bm{\mu}}bold_italic_Î¼ | Ïƒ2superscriptğœ2\sigma^{2}italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | pâ¢(ğ’š|ğœ½â‹†)ğ‘conditionalğ’šsuperscriptğœ½â‹†p\left({\bm{y}}|{\bm{\theta}}^{\star}\right)italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSE | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | pâ¢(ğ’š|ğœ½â‹†)ğ‘conditionalğ’šsuperscriptğœ½â‹†p\left({\bm{y}}|{\bm{\theta}}^{\star}\right)italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSE  
0 | 0.1 | -2012.404 | 2002.560 | 25.690 | 0.150 | 0.243 | 0.122  
0 | 1 | -2012.404 | 2002.560 | 25.690 | 0.148 | 0.252 | 0.129  
0 | 5 | -2012.404 | 2002.561 | 25.691 | 0.144 | 0.259 | 0.112  
0 | 10 | -2012.412 | 2002.564 | 25.689 | 3.046 | 0.744 | 1.245  
5 | 0.1 | -2012.404 | 2002.562 | 25.692 | 0.156 | 0.345 | 0.160  
5 | 1 | -2012.404 | 2002.562 | 25.691 | 0.156 | 0.348 | 0.188  
5 | 5 | -2012.404 | 2002.563 | 25.692 | 0.146 | 0.603 | 0.280  
5 | 10 | -2012.405 | 2002.562 | 25.691 | 0.179 | 0.557 | 0.315  
-5 | 0.1 | -2012.404 | 2002.563 | 25.691 | 0.136 | 0.292 | 0.129  
-5 | 1 | -2012.404 | 2002.563 | 25.691 | 0.134 | 0.646 | 0.240  
-5 | 5 | -2012.404 | 2002.563 | 25.691 | 0.147 | 0.513 | 0.092  
-5 | 10 | -2012.405 | 2002.565 | 25.693 | 0.216 | 1.042 | 0.450  
ğœ½0=câ¢ğŸdsubscriptğœ½0ğ‘subscript1ğ‘‘{\bm{\theta}}_{0}=c\bm{1}_{d}bold_italic_Î¸ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_c bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | Mean | Std. Dev. (Ã—100)(\times 100)( Ã— 100 )  
cğ‘citalic_c | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | pâ¢(ğ’š|ğœ½â‹†)ğ‘conditionalğ’šsuperscriptğœ½â‹†p\left({\bm{y}}|{\bm{\theta}}^{\star}\right)italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSE | â„’â¢(ğœ»ğ’„â‹†)â„’superscriptsuperscriptğœ»ğ’„â‹†\mathcal{L}\left({\bm{\zeta^{c}}}^{\star}\right)caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | pâ¢(ğ’š|ğœ½â‹†)ğ‘conditionalğ’šsuperscriptğœ½â‹†p\left({\bm{y}}|{\bm{\theta}}^{\star}\right)italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) | MSE  
0 | -2012.403 | 2002.560 | 25.691 | 0.234 | 0.212 | 0.107  
1 | -2012.403 | 2002.560 | 25.691 | 0.233 | 0.191 | 0.099  
-1 | -2012.403 | 2002.560 | 25.691 | 0.233 | 0.177 | 0.089  
5 | -2012.403 | 2002.562 | 25.692 | 0.238 | 0.401 | 0.188  
-5 | -2012.404 | 2002.562 | 25.691 | 0.229 | 0.302 | 0.161  
20 | -2012.405 | 2002.563 | 25.691 | 0.254 | 0.822 | 0.472  
-20 | -2012.402 | 2002.563 | 25.691 | 0.206 | 0.180 | 0.115  
  
#### Runtime

Table 18 compares the running times of the adopted Variational Bayes
algorithms for two representative models. As expected, MGVBP and MGVB show a
rather aligned performance (the factor 2 in (5.2) compared to (8) is indeed
irrelevant in terms of running time). On the other hand, QBVI relies on update
rules that are completely unrelated and different from those of MGVBP and
MGVB; as such, the corresponding running times are provided as a reference
since they are not directly comparable. Irrespective of the adopted
optimizers, the major factor deriving the runtime appears to be the complexity
of the loglikelihood (with little surprise the iterative nature of the
expensive computations in the GARCH likelihood severely affects the runtime)
the logâ¡pğ‘\log proman_log italic_p estimator variant (eq. (20)) is
consistently outperforming the classical approach based on the hâ„hitalic_h-
function estimator (eq. (19)).

Table 18: Running time for the logistic regression (Labour dataset).
Statistics in seconds per 1000 iterations averaged over 200 replications
(Ns=75subscriptğ‘ğ‘ 75N_{s}=75italic_N start_POSTSUBSCRIPT italic_s
end_POSTSUBSCRIPT = 75).

|  |  | Logistic regression | GARCH(1,0,1) | GJR(1,1,1)  
---|---|---|---|---|---  
|  |  | Mean | Median | Std. | Mean | Median | Std. | Mean | Median | Std. | Mean | Median | Std.  
hâ„hitalic_h-func. estimator |  | Full cov. | Diagonal cov. | Full cov. | Full cov.  
MGVB | 5.411 | 5.387 | 1.137 | 5.375 | 5.346 | 0.181 | 32.76 | 33.05 | 0.78 | 30.73 | 30.45 | 2.44  
MGVBP | 5.375 | 5.368 | 1.483 | 5.321 | 5.328 | 0.127 | 33.02 | 32.90 | 0.45 | 30.91 | 32.07 | 2.30  
QBVI | 5.340 | 5.305 | 0.884 | 5.425 | 5.408 | 0.254 | 32.70 | 32.67 | 0.66 | 30.79 | 31.03 | 2.53  
logâ¡pğ‘\log proman_log italic_p estimator |  | Full cov. | Diagonal cov. | Full cov. | Full cov.  
MGVB | 5.883 | 5.877 | 0.121 | 5.791 | 5.733 | 0.419 | 30.94 | 31.18 | 1.52 | 29.18 | 30.18 | 1.89  
MGVBP | 5.929 | 5.904 | 0.184 | 5.666 | 5.656 | 0.743 | 31.47 | 31.69 | 0.82 | 29.21 | 29.79 | 1.72  
QBVI | 5.879 | 5.858 | 0.119 | 5.543 | 5.551 | 0.353 | 31.14 | 31.39 | 0.94 | 28.86 | 29.98 | 2.39  
  
Table 19 reports the runtime for a linear regression experiment (simulated
data, Ns=100subscriptğ‘ğ‘ 100N_{s}=100italic_N start_POSTSUBSCRIPT italic_s
end_POSTSUBSCRIPT = 100, averaged across 50 replications) per iteration with a
growing number of variational parameters. It can be seen that MGVBP is
slightly but consistently faster than MGVB. As discussed in Section 5.7,
despite the simple natural gradient form the MGVBP has, the actual
computational effort is a retraction, which requires inverting the variational
covariance matrix at each iteration. Nevertheless, the experiments we
performed show a steeper LB optimization and an improved optimum.

Table 19: Running times (seconds per iteration) for a linear regression
problem of increasing complexity.

Number of parameters | 6 | 12 | 30 | 110 | 240 | 650 | 2550 | 10100 | 22650  
---|---|---|---|---|---|---|---|---|---  
MGVB | 0.0365 | 0.0174 | 0.0194 | 0.0247 | 0.0323 | 0.0766 | 0.2112 | 0.6997 | 1.5176  
MGVBP | 0.0355 | 0.0169 | 0.0185 | 0.0237 | 0.0308 | 0.0760 | 0.2044 | 0.6795 | 1.5037  
Difference | 2.8% | 2.8% | 4.6% | 4.0% | 4.7% | 0.8% | 3.2% | 2.9% | 0.9%  
  
##  Appendix C Proofs

###  Preliminaries: the Gaussian FIM

From (Barfoot, 2020) the natural gradients for a dğ‘‘ditalic_d-variate
Gaussian distribution with mean ğğ{\bm{\mu}}bold_italic_Î¼ and covariance
matrix Î£Î£\Sigmaroman_Î£, in the parametrizations
ğœ»ğ’„superscriptğœ»ğ’„{\bm{\zeta^{c}}}bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_c end_POSTSUPERSCRIPT and
ğœ»ğ’Šsuperscriptğœ»ğ’Š{\bm{\zeta^{i}}}bold_italic_Î¶ start_POSTSUPERSCRIPT
bold_italic_i end_POSTSUPERSCRIPT are respectively given by:

| â„ğœ»ğ’„âˆ’1=[Î£ğŸğŸ2â¢(Î£âŠ—Î£)]â¢,â„ğœ»ğ’Šâˆ’1=[Î£ğŸğŸ2â¢(Î£âŠ—Î£)âˆ’1]â¢.formulae-sequencesubscriptsuperscriptâ„1superscriptğœ»ğ’„matrixÎ£002tensor-productÎ£Î£,subscriptsuperscriptâ„1superscriptğœ»ğ’ŠmatrixÎ£002superscripttensor-productÎ£Î£1.\displaystyle\mathcal{I}^{-1}_{{\bm{\zeta^{c}}}}=\begin{bmatrix}\Sigma&\bm{0}% \\\ \bm{0}&2\left(\Sigma\otimes\Sigma\right)\end{bmatrix}\text{,}\qquad\mathcal{I}% ^{-1}_{{\bm{\zeta^{i}}}}=\begin{bmatrix}\Sigma&\bm{0}\\\ \bm{0}&2\left(\Sigma\otimes\Sigma\right)^{-1}\end{bmatrix}\text{.}caligraphic_I start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL roman_Î£ end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL 2 ( roman_Î£ âŠ— roman_Î£ ) end_CELL end_ROW end_ARG ] , caligraphic_I start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL roman_Î£ end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL 2 ( roman_Î£ âŠ— roman_Î£ ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] . |  | (27)  
---|---|---|---  
  
###  Preliminaries: a useful relation

For a generic SPD matrix Pğ‘ƒPitalic_P and a function fğ‘“fitalic_f of
Pğ‘ƒPitalic_P that can be as well reparametrized in terms of
Pâˆ’1superscriptğ‘ƒ1P^{-1}italic_P start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT:

| Pâ¢âˆ‡Pfâ¢P=âˆ’âˆ‡Pâˆ’1fâ¢,ğ‘ƒsubscriptâˆ‡ğ‘ƒğ‘“ğ‘ƒsubscriptâˆ‡superscriptğ‘ƒ1ğ‘“,P\nabla_{P}fP=-\nabla_{P^{-1}}f\text{,}italic_P âˆ‡ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_f italic_P = - âˆ‡ start_POSTSUBSCRIPT italic_P start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_f , |  | (28)  
---|---|---|---  
  
which can be shown to hold according to matrix calculus with Jacobian
transformations of the gradient âˆ‡Pfsubscriptâˆ‡ğ‘ƒğ‘“\nabla_{P}fâˆ‡
start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_f.

###  Proof of Proposition 5.1

From the natural gradient definition:

| âˆ‡~ğœ»ğ’Šâ¢â„’i=def.â„ğœ»ğ’Šâˆ’1â¢âˆ‡Î£âˆ’1â„’i=(â¢27â¢)[Î£â¢âˆ‡ğâ„’i2â¢(Î£âŠ—Î£)âˆ’1â¢âˆ‡Î£âˆ’1â„’i]â¢.superscriptdef.subscript~âˆ‡superscriptğœ»ğ’Šsuperscriptâ„’ğ‘–subscriptsuperscriptâ„1superscriptğœ»ğ’Šsubscriptâˆ‡superscriptÎ£1superscriptâ„’ğ‘–superscriptitalic-(27italic-)matrixÎ£subscriptâˆ‡ğsuperscriptâ„’ğ‘–2superscripttensor-productÎ£Î£1superscriptsubscriptâˆ‡Î£1superscriptâ„’ğ‘–.\displaystyle\tilde{\nabla}_{{\bm{\zeta^{i}}}}\mathcal{L}^{i}\stackrel{{% \scriptstyle\text{def.}}}{{=}}\mathcal{I}^{-1}_{{\bm{\zeta^{i}}}}\nabla_{% \Sigma^{-1}}\mathcal{L}^{i}\stackrel{{\scriptstyle\eqref{eq:barfoot}}}{{=}}% \begin{bmatrix}\Sigma\nabla_{\bm{\mu}}\mathcal{L}^{i}\\\ 2\left(\Sigma\otimes\Sigma\right)^{-1}\nabla_{\Sigma}^{-1}\mathcal{L}^{i}\end{% bmatrix}\text{.}over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def. end_ARG end_RELOP caligraphic_I start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG italic_( italic_) end_ARG end_RELOP [ start_ARG start_ROW start_CELL roman_Î£ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL 2 ( roman_Î£ âŠ— roman_Î£ ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] . |   
---|---|---  
  
removing the vectorization from the term related to
Î£âˆ’1superscriptÎ£1\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1
end_POSTSUPERSCRIPT:

| âˆ‡~Î£âˆ’1â¢â„’i=2â¢Î£âˆ’1â¢âˆ‡Î£âˆ’1â„’iâ¢Î£âˆ’1=(â¢28â¢)âˆ’2â¢âˆ‡Î£â„’câ¢.subscript~âˆ‡superscriptÎ£1superscriptâ„’ğ‘–2superscriptÎ£1subscriptâˆ‡superscriptÎ£1superscriptâ„’ğ‘–superscriptÎ£1superscriptitalic-(28italic-)2subscriptâˆ‡Î£superscriptâ„’ğ‘.\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}^{i}=2\Sigma^{-1}\nabla_{\Sigma^{-1}}% \mathcal{L}^{i}\Sigma^{-1}\stackrel{{\scriptstyle\eqref{eq:riemann_grad_M}}}{{% =}}-2\nabla_{\Sigma}\mathcal{L}^{c}\text{.}over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = 2 roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG italic_( italic_) end_ARG end_RELOP - 2 âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT . |  | (29)  
---|---|---|---  
  
###  Derivation of the MGVBP update

For the precision matrix, the Riemann gradient from the manifold
â„³â„³\mathcal{M}caligraphic_M is
Î£âˆ’1â¢âˆ‡Î£âˆ’1Î£âˆ’1superscriptÎ£1subscriptâˆ‡superscriptÎ£1superscriptÎ£1\Sigma^{-1}\nabla_{\Sigma^{-1}}\Sigma^{-1}roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_Î£
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. Therefore, the following
equivalent representations hold:

| âˆ‡Â¯Î£âˆ’1â¢â„’=(â¢28â¢)Î£âˆ’1â¢âˆ‡Î£âˆ’1Î£âˆ’1=(â¢28â¢)âˆ’âˆ‡Î£â„’=(â¢29â¢)12â¢âˆ‡~Î£âˆ’1â¢â„’â¢.superscriptitalic-(28italic-)subscriptÂ¯âˆ‡superscriptÎ£1â„’superscriptÎ£1subscriptâˆ‡superscriptÎ£1superscriptÎ£1superscriptitalic-(28italic-)subscriptâˆ‡Î£â„’superscriptitalic-(29italic-)12subscript~âˆ‡superscriptÎ£1â„’.\bar{\nabla}_{\Sigma^{-1}}\mathcal{L}\stackrel{{\scriptstyle\eqref{eq:riemann_% grad_M}}}{{=}}\Sigma^{-1}\nabla_{\Sigma^{-1}}\Sigma^{-1}\stackrel{{% \scriptstyle\eqref{eq:riemann_grad_M}}}{{=}}-\nabla_{\Sigma}\mathcal{L}% \stackrel{{\scriptstyle\eqref{eq:equiv}}}{{=}}\frac{1}{2}\tilde{\nabla}_{% \Sigma^{-1}}\mathcal{L}\text{.}overÂ¯ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG italic_( italic_) end_ARG end_RELOP roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG italic_( italic_) end_ARG end_RELOP - âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG italic_( italic_) end_ARG end_RELOP divide start_ARG 1 end_ARG start_ARG 2 end_ARG over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L . |   
---|---|---  
  
Note that the last equality is not general; it holds because of the particular
(Î£âŠ—Î£)âˆ’1superscripttensor-
productÎ£Î£1\left(\Sigma\otimes\Sigma\right)^{-1}( roman_Î£ âŠ— roman_Î£ )
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT form of the second block of
â„ğœ»ğ’Šâˆ’1subscriptsuperscriptâ„1superscriptğœ»ğ’Š\mathcal{I}^{-1}_{{\bm{\zeta^{i}}}}caligraphic_I
start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT
bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT
end_POSTSUBSCRIPT, specific for the Gaussian loglikelihood. As a consequence,
the following three updates are equivalent:

| Î£âˆ’1superscriptÎ£1\displaystyle\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT | â†RÎ£âˆ’1â¢(Î²â¢Î£âˆ’1â¢âˆ‡Î£âˆ’1Î£âˆ’1)â¢,â†absentsubscriptğ‘…superscriptÎ£1ğ›½superscriptÎ£1subscriptâˆ‡superscriptÎ£1superscriptÎ£1,\displaystyle\leftarrow R_{\Sigma^{-1}}\left(\beta\Sigma^{-1}\nabla_{\Sigma^{-% 1}}\Sigma^{-1}\right)\text{,}â† italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_Î² roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT âˆ‡ start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) , |   
---|---|---|---  
| Î£âˆ’1superscriptÎ£1\displaystyle\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT | â†RÎ£âˆ’1â¢(âˆ’Î²â¢âˆ‡Î£â„’)â¢,â†absentsubscriptğ‘…superscriptÎ£1ğ›½subscriptâˆ‡Î£â„’,\displaystyle\leftarrow R_{\Sigma^{-1}}\left(-\beta\nabla_{\Sigma}\mathcal{L}% \right)\text{,}â† italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( - italic_Î² âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L ) , |  | (30)  
| Î£âˆ’1superscriptÎ£1\displaystyle\Sigma^{-1}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT | â†RÎ£âˆ’1â¢(Î²â¢12â¢âˆ‡~Î£âˆ’1â¢â„’)â¢.â†absentsubscriptğ‘…superscriptÎ£1ğ›½12subscript~âˆ‡superscriptÎ£1â„’.\displaystyle\leftarrow R_{\Sigma^{-1}}\left(\beta\frac{1}{2}\tilde{\nabla}_{% \Sigma^{-1}}\mathcal{L}\right)\text{.}â† italic_R start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_Î² divide start_ARG 1 end_ARG start_ARG 2 end_ARG over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ) . |  | (31)  
  
All three above correspond to an update of a Riemann gradient on the manifold
â„³â„³\mathcal{M}caligraphic_M based on the retraction form derived from the
manifold â„³â„³\mathcal{M}caligraphic_M (specific for the equipped Euclidean
metric). The caveat with (31) alone, without Proposition 5.1 is the following:
by removing the factor 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2
end_ARG the argument of the retraction form
(â¢31â¢)italic-(31italic-)\eqref{eq:equiv_3}italic_( italic_) derived from the
SPD manifold equipped with the Euclidean metric, is not longer the Riemann
gradient for this manifold, but the Riemann gradient of a different manifold
(the SPD manifold equipped with the Fisher-Rao metric
â„±â„±\mathcal{F}caligraphic_F). This point is central and discussed in the
main text.  
In support of our argument and derivation, by unfolding the retraction in
(31), the update

| Î£âˆ’1â†Î£âˆ’1+Î²2â¢[âˆ‡~Î£âˆ’1â¢â„’]+Î²22â¢[âˆ‡~Î£âˆ’1â¢â„’]â¢Î£â¢[âˆ‡~Î£âˆ’1â¢â„’]â¢,â†superscriptÎ£1superscriptÎ£1ğ›½2delimited-[]subscript~âˆ‡superscriptÎ£1â„’superscriptğ›½22delimited-[]subscript~âˆ‡superscriptÎ£1â„’Î£delimited-[]subscript~âˆ‡superscriptÎ£1â„’,\Sigma^{-1}\leftarrow\Sigma^{-1}+\frac{\beta}{2}\left[\tilde{\nabla}_{\Sigma^{% -1}}\mathcal{L}\right]+\frac{\beta^{2}}{2}\left[\tilde{\nabla}_{\Sigma^{-1}}% \mathcal{L}\right]\Sigma\left[\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}\right]% \text{,}roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT â† roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + divide start_ARG italic_Î² end_ARG start_ARG 2 end_ARG [ over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ] + divide start_ARG italic_Î² start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG [ over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ] roman_Î£ [ over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ] , |   
---|---|---  
  
aligns with the update form of (Lin etÂ al., 2020) as a special case of their
Bayesian learning rule, since the Gaussian FIM satisfies their block-
coordinate natural parametrization assumption.

Working on Î£Î£\Sigmaroman_Î£, one analogously obtains
RÎ£â¢(Î£â¢âˆ‡Î£â„’â¢Î£)=RÎ£â¢(12â¢âˆ‡~Î£â¢â„’)subscriptğ‘…Î£Î£subscriptâˆ‡Î£â„’Î£subscriptğ‘…Î£12subscript~âˆ‡Î£â„’R_{\Sigma}\left(\Sigma\nabla_{\Sigma}\mathcal{L}\Sigma\right)=R_{\Sigma}\left(%
\frac{1}{2}\tilde{\nabla}_{\Sigma}\mathcal{L}\right)italic_R
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT ( roman_Î£ âˆ‡
start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT caligraphic_L roman_Î£ ) =
italic_R start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT ( divide start_ARG 1
end_ARG start_ARG 2 end_ARG over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT
roman_Î£ end_POSTSUBSCRIPT caligraphic_L ); however, the latter does not
correspond to a simpler algebraic form leading to a computationally convenient
update like (30).

###  General form of the MGVBP update

For a prior pâˆ¼ğ’©â¢(ğ0,Î£0)similar-
toğ‘ğ’©subscriptğ0subscriptÎ£0p\sim\mathcal{N}\left({\bm{\mu}}_{0},\Sigma_{0}\right)italic_p
âˆ¼ caligraphic_N ( bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,
roman_Î£ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) and a variational posterior
qâˆ¼ğ’©â¢(ğ,Î£)similar-
toğ‘ğ’©ğÎ£q\sim\mathcal{N}\left({\bm{\mu}},\Sigma\right)italic_q âˆ¼
caligraphic_N ( bold_italic_Î¼ , roman_Î£ ), by rewriting the LB as

| ğ”¼qğœ»ğ’„â¢[hğœ»ğ’„â¢(ğœ½)]subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]subscriptâ„superscriptğœ»ğ’„ğœ½\displaystyle\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[h_{\bm{\zeta^{c}}}\left({\bm% {\theta}}\right)\right]blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] | =ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)+logâ¡pâ¢(ğ’š|ğœ½)]absentsubscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½ğ‘conditionalğ’šğœ½\displaystyle=\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left({\bm{\theta}}% \right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)+\log p\left({\bm{y}}% |{\bm{\theta}}\right)\right]= blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) + roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) ] |   
---|---|---|---  
|  | =ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)]+ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğ’š|ğœ½)]â¢,absentsubscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘conditionalğ’šğœ½,\displaystyle=\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left({\bm{\theta}}% \right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]+\mathbb{E}_{q% _{\bm{\zeta^{c}}}}\left[\log p\left({\bm{y}}|{\bm{\theta}}\right)\right]\text{,}= blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] + blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) ] , |   
  
we decompose âˆ‡ğœ»ğ’„â„’â¢(ğœ»ğ’„)subscriptâˆ‡superscriptğœ»ğ’„â„’superscriptğœ»ğ’„\nabla_{\bm{\zeta^{c}}}\mathcal{L}\left({\bm{\zeta^{c}}}\right)âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT ) as âˆ‡ğœ»ğ’„ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)]+âˆ‡ğœ»ğ’„ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğ’š|ğœ½)]subscriptâˆ‡superscriptğœ»ğ’„subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½subscriptâˆ‡superscriptğœ»ğ’„subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘conditionalğ’šğœ½\nabla_{\bm{\zeta^{c}}}\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left({\bm{% \theta}}\right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]+% \nabla_{\bm{\zeta^{c}}}\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left({\bm{y% }}|{\bm{\theta}}\right)\right]âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] + âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) ]. As in (14), we apply the log-derivative trick on the last term and write âˆ‡ğœ»ğ’„ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğ’š|ğœ½)]=ğ”¼qğœ»ğ’„â¢[âˆ‡ğœ»ğ’„[logâ¡qğœ»ğ’„â¢(ğœ½)]â¡logâ¡pâ¢(ğ’š|ğœ½)]subscriptâˆ‡superscriptğœ»ğ’„subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘conditionalğ’šğœ½subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]subscriptâˆ‡superscriptğœ»ğ’„subscriptğ‘superscriptğœ»ğ’„ğœ½ğ‘conditionalğ’šğœ½\nabla_{\bm{\zeta^{c}}}\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left({\bm{y% }}|{\bm{\theta}}\right)\right]=\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\nabla_{% \bm{\zeta^{c}}}\left[\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]% \log p\left({\bm{y}}|{\bm{\theta}}\right)\right]âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) ] = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) ]. On the other hand, it is easy to show that up to a constant that does not depend on ğğ{\bm{\mu}}bold_italic_Î¼ and Î£Î£\Sigmaroman_Î£

| ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)]=subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½absent\displaystyle\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left({\bm{\theta}}% \right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]=blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] = | âˆ’12â¢logâ¡|Î£0|+12â¢logâ¡|Î£|+12â¢d12subscriptÎ£012Î£12ğ‘‘\displaystyle-\frac{1}{2}\log|\Sigma_{0}|+\frac{1}{2}\log|\Sigma|+\frac{1}{2}d\- divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log | roman_Î£ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | + divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log | roman_Î£ | + divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_d |   
---|---|---|---  
|  | âˆ’12â¢trâ¢(Î£0âˆ’1â¢Î£)âˆ’12â¢(ğâˆ’ğ0)âŠ¤â¢Î£0âˆ’1â¢(ğâˆ’ğ0)â¢,12trsuperscriptsubscriptÎ£01Î£12superscriptğsubscriptğ0topsuperscriptsubscriptÎ£01ğsubscriptğ0,\displaystyle-\frac{1}{2}\text{tr}\left(\Sigma_{0}^{-1}\Sigma\right)-\frac{1}{% 2}\left({\bm{\mu}}-{\bm{\mu}}_{0}\right)^{\top}\Sigma_{0}^{-1}\left({\bm{\mu}}% -{\bm{\mu}}_{0}\right)\text{,}\- divide start_ARG 1 end_ARG start_ARG 2 end_ARG tr ( roman_Î£ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_Î£ ) - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( bold_italic_Î¼ - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT roman_Î£ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Î¼ - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , |   
  
so that

| âˆ‡Î£ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)]subscriptâˆ‡Î£subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½\displaystyle\nabla_{\Sigma}\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left({% \bm{\theta}}\right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] | =12â¢Î£âˆ’1âˆ’12â¢Î£0âˆ’1â¢,absent12superscriptÎ£112subscriptsuperscriptÎ£10,\displaystyle=\frac{1}{2}\Sigma^{-1}-\frac{1}{2}\Sigma^{-1}_{0}\text{,}= divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , |   
---|---|---|---  
| âˆ‡ğğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)]subscriptâˆ‡ğsubscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½\displaystyle\nabla_{\bm{\mu}}\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left% ({\bm{\theta}}\right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] | =âˆ’Î£0âˆ’1â¢(ğâˆ’ğ0)â¢.absentsubscriptsuperscriptÎ£10ğsubscriptğ0.\displaystyle=-\Sigma^{-1}_{0}\left({\bm{\mu}}-{\bm{\mu}}_{0}\right)\text{.}= - roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_Î¼ - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) . |   
  
For the natural gradients, we have

| âˆ‡~Î£âˆ’1â¢ğ”¼qğœ»ğ’Šâ¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’Šâ¢(ğœ½)]subscript~âˆ‡superscriptÎ£1subscriptğ”¼subscriptğ‘superscriptğœ»ğ’Šdelimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’Šğœ½\displaystyle\tilde{\nabla}_{\Sigma^{-1}}\mathbb{E}_{q_{\bm{\zeta^{i}}}}\left[% \log p\left({\bm{\theta}}\right)-\log q_{\bm{\zeta^{i}}}\left({\bm{\theta}}% \right)\right]over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] | =âˆ’âˆ‡Î£ğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)]absentsubscriptâˆ‡Î£subscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½\displaystyle=-\nabla_{\Sigma}\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p\left% ({\bm{\theta}}\right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]= - âˆ‡ start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] |   
---|---|---|---  
|  | =âˆ’12â¢Î£âˆ’1+12â¢Î£0âˆ’1â¢,absent12superscriptÎ£112subscriptsuperscriptÎ£10,\displaystyle=-\frac{1}{2}\Sigma^{-1}+\frac{1}{2}\Sigma^{-1}_{0}\text{,}= - divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , |   
| âˆ‡~ğâ¢ğ”¼qğœ»ğ’Šâ¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’Šâ¢(ğœ½)]subscript~âˆ‡ğsubscriptğ”¼subscriptğ‘superscriptğœ»ğ’Šdelimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’Šğœ½\displaystyle\tilde{\nabla}_{\bm{\mu}}\mathbb{E}_{q_{\bm{\zeta^{i}}}}\left[% \log p\left({\bm{\theta}}\right)-\log q_{\bm{\zeta^{i}}}\left({\bm{\theta}}% \right)\right]over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] | =Î£â¢âˆ‡ğğ”¼qğœ»ğ’„â¢[logâ¡pâ¢(ğœ½)âˆ’logâ¡qğœ»ğ’„â¢(ğœ½)]absentÎ£subscriptâˆ‡ğsubscriptğ”¼subscriptğ‘superscriptğœ»ğ’„delimited-[]ğ‘ğœ½subscriptğ‘superscriptğœ»ğ’„ğœ½\displaystyle=\Sigma\nabla_{\bm{\mu}}\mathbb{E}_{q_{\bm{\zeta^{c}}}}\left[\log p% \left({\bm{\theta}}\right)-\log q_{\bm{\zeta^{c}}}\left({\bm{\theta}}\right)\right]= roman_Î£ âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_Î¸ ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ ) ] |   
|  | =âˆ’Î£â¢Î£0âˆ’1â¢(ğâˆ’ğ0)â¢,absentÎ£subscriptsuperscriptÎ£10ğsubscriptğ0,\displaystyle=-\Sigma\Sigma^{-1}_{0}\left({\bm{\mu}}-{\bm{\mu}}_{0}\right)% \text{,}= - roman_Î£ roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_Î¼ - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , |   
  
while the naive estimators for âˆ‡~ğâ¢ğ”¼qğœ»ğ’Šâ¢[logâ¡pâ¢(ğ’š|ğœ½)]subscript~âˆ‡ğsubscriptğ”¼subscriptğ‘superscriptğœ»ğ’Šdelimited-[]ğ‘conditionalğ’šğœ½\tilde{\nabla}_{{\bm{\mu}}}\mathbb{E}_{q_{\bm{\zeta^{i}}}}\left[\log p\left({% \bm{y}}|{\bm{\theta}}\right)\right]over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) ] and âˆ‡~Î£âˆ’1â¢ğ”¼qğœ»ğ’Šâ¢[logâ¡pâ¢(ğ’š|ğœ½)]subscript~âˆ‡superscriptÎ£1subscriptğ”¼subscriptğ‘superscriptğœ»ğ’Šdelimited-[]ğ‘conditionalğ’šğœ½\tilde{\nabla}_{\Sigma^{-1}}\mathbb{E}_{q_{\bm{\zeta^{i}}}}\left[\log p\left({% \bm{y}}|{\bm{\theta}}\right)\right]over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ) ] turn analogous to feasible natural gradient estimators presented in Section 5.3, with hğœ»ğ’„subscriptâ„superscriptğœ»ğ’„h_{\bm{\zeta^{c}}}italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT replaced by logâ¡pâ¢(ğ’š|ğœ½)ğ‘conditionalğ’šğœ½\log p\left({\bm{y}}|{\bm{\theta}}\right)roman_log italic_p ( bold_italic_y | bold_italic_Î¸ ). This leads to the general form of the MGVBP update, based either on the hâ„hitalic_h-function gradient estimator (generally applicable) or the above decomposition (applicable under a Gaussian prior):

| âˆ‡~ğâ¢â„’tâ¢(ğœ»ğ’Š)subscript~âˆ‡ğsubscriptâ„’ğ‘¡superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{{\bm{\mu}}}\mathcal{L}_{t}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT bold_italic_Î¼ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆcğt+1Sâ¢âˆ‘s=1S[(ğœ½sâˆ’ğt)â¢logâ¡fâ¢(ğœ½s)]absentsubscriptğ‘subscriptğğ‘¡1ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptğœ½ğ‘ subscriptğğ‘¡ğ‘“subscriptğœ½ğ‘ \displaystyle\approx c_{{\bm{\mu}}_{t}}+\frac{1}{S}\sum_{s=1}^{S}\left[\left({% \bm{\theta}}_{s}-{\bm{\mu}}_{t}\right)\log f\left({\bm{\theta}}_{s}\right)\right]â‰ˆ italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] |   
---|---|---|---  
| âˆ‡~Î£âˆ’1â¢â„’tâ¢(ğœ»ğ’Š)subscript~âˆ‡superscriptÎ£1subscriptâ„’ğ‘¡superscriptğœ»ğ’Š\displaystyle\tilde{\nabla}_{\Sigma^{-1}}\mathcal{L}_{t}\left({\bm{\zeta^{i}}}\right)over~ start_ARG âˆ‡ end_ARG start_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_i end_POSTSUPERSCRIPT ) | â‰ˆCÎ£t+12â¢Sâ¢âˆ‘s=1S[(Î£tâˆ’1âˆ’Î£tâˆ’1â¢(ğœ½sâˆ’ğt)â¢(ğœ½sâˆ’ğt)âŠ¤â¢Î£tâˆ’1)â¢logâ¡fâ¢(ğœ½s)]absentsubscriptğ¶subscriptÎ£ğ‘¡12ğ‘†superscriptsubscriptğ‘ 1ğ‘†delimited-[]subscriptsuperscriptÎ£1ğ‘¡subscriptsuperscriptÎ£1ğ‘¡subscriptğœ½ğ‘ subscriptğğ‘¡superscriptsubscriptğœ½ğ‘ subscriptğğ‘¡topsubscriptsuperscriptÎ£1ğ‘¡ğ‘“subscriptğœ½ğ‘ \displaystyle\approx C_{\Sigma_{t}}+\frac{1}{2S}\sum_{s=1}^{S}\left[\left(% \Sigma^{-1}_{t}-\Sigma^{-1}_{t}\left({\bm{\theta}}_{s}-{\bm{\mu}}_{t}\right)% \left({\bm{\theta}}_{s}-{\bm{\mu}}_{t}\right)^{\top}\Sigma^{-1}_{t}\right)\log f% \left({\bm{\theta}}_{s}\right)\right]â‰ˆ italic_C start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT [ ( roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] |   
  
where

| {{CÎ£t=âˆ’12â¢Î£tâˆ’1+12â¢Î£0âˆ’1â¢,cğt=âˆ’Î£tâ¢Î£0âˆ’1â¢(ğtâˆ’ğ0)â¢,logâ¡fâ¢(ğœ½s)=logâ¡pâ¢(ğ’š|ğœ½s)â¢,ifÂ pÂ is Gaussian,{CÎ£t=0â¢,cğt=ğŸâ¢,logâ¡fâ¢(ğœ½s)=hğœ»ğ’„â¢(ğœ½s)â¢,in general, whetherÂ pÂ is Gaussian or not.casescasessubscriptğ¶subscriptÎ£ğ‘¡12subscriptsuperscriptÎ£1ğ‘¡12subscriptsuperscriptÎ£10,otherwisesubscriptğ‘subscriptğğ‘¡subscriptÎ£ğ‘¡subscriptsuperscriptÎ£10subscriptğğ‘¡subscriptğ0,otherwiseğ‘“subscriptğœ½ğ‘ ğ‘conditionalğ’šsubscriptğœ½ğ‘ ,otherwiseifÂ pÂ is Gaussian,otherwiseotherwisecasessubscriptğ¶subscriptÎ£ğ‘¡0,otherwisesubscriptğ‘subscriptğğ‘¡0,otherwiseğ‘“subscriptğœ½ğ‘ subscriptâ„superscriptğœ»ğ’„subscriptğœ½ğ‘ ,otherwisein general, whetherÂ pÂ is Gaussian or not.\begin{cases}\begin{cases}C_{\Sigma_{t}}=-\frac{1}{2}\Sigma^{-1}_{t}+\frac{1}{% 2}\Sigma^{-1}_{0}\text{,}\\\ c_{{\bm{\mu}}_{t}}=-\Sigma_{t}\Sigma^{-1}_{0}\left({\bm{\mu}}_{t}-{\bm{\mu}}_{% 0}\right)\text{,}\\\ \log f\left({\bm{\theta}}_{s}\right)=\log p\left({\bm{y}}|{\bm{\theta}}_{s}% \right)\text{,}\end{cases}&\text{if $p$ is Gaussian,}\\\ \\\ \begin{cases}C_{\Sigma_{t}}=0\text{,}\\\ c_{{\bm{\mu}}_{t}}=\bm{0}\text{,}\\\ \log f\left({\bm{\theta}}_{s}\right)=h_{{\bm{\zeta^{c}}}}\left({\bm{\theta}}_{% s}\right)\text{,}\end{cases}&\text{in general, whether $p$ is Gaussian or not.% }\end{cases}{ start_ROW start_CELL { start_ROW start_CELL italic_C start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = - divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = - roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_Î£ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_Î¼ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = roman_log italic_p ( bold_italic_y | bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) , end_CELL start_CELL end_CELL end_ROW end_CELL start_CELL if italic_p is Gaussian, end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL { start_ROW start_CELL italic_C start_POSTSUBSCRIPT roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 0 , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUBSCRIPT bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = bold_0 , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = italic_h start_POSTSUBSCRIPT bold_italic_Î¶ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) , end_CELL start_CELL end_CELL end_ROW end_CELL start_CELL in general, whether italic_p is Gaussian or not. end_CELL end_ROW |  | (32)  
---|---|---|---  
  
The short-hand notation logâ¡fâ¢(ğœ½s)ğ‘“subscriptğœ½ğ‘ \log f\left({\bm{\theta}}_{s}\right)roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) embeds the fact that the function is meant to be evaluated at the current value of the parameter, i.e., logâ¡fâ¢(ğœ½s)â‰¡logâ¡fâ¢(ğœ½s|ğ=ğt,Î£=Î£t)ğ‘“subscriptğœ½ğ‘ ğ‘“formulae-sequenceconditionalsubscriptğœ½ğ‘ ğsubscriptğğ‘¡Î£subscriptÎ£ğ‘¡\log f\left({\bm{\theta}}_{s}\right)\equiv\log f\left({\bm{\theta}}_{s}|{\bm{% \mu}}={\bm{\mu}}_{t},\Sigma=\Sigma_{t}\right)roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) â‰¡ roman_log italic_f ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | bold_italic_Î¼ = bold_italic_Î¼ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , roman_Î£ = roman_Î£ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ).
